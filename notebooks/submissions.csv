authors,date_downloaded,date_modified,date_published,date_submitted,image_url,incident_date,incident_id,language,mongodb_id,source_domain,submitters,text,title,url
"[""Matthew Schwall"","" Tom Daniel"","" Trent Victor"","" Francesca Favaro"","" Henning Hohnhold""]",2022-08-17,2022-08-17,2020-10-30,2022-08-17,https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png,2019-01-01,,en,,arxiv.org,"[""Thomas Giallella (CSET)""]","Waymo's mission to reduce traffic injuries and fatalities and improve mobility for all has led us to expand deployment of automated vehicles on public roads without a human driver behind the wheel. As part of this process, Waymo is committed to providing the public with informative and relevant data regarding the demonstrated safety of Waymo's automated driving system, which we call the Waymo Driver. The data presented in this paper represents more than 6.1 million miles of automated driving in the Phoenix, Arizona metropolitan area, including operations with a trained operator behind the steering wheel from calendar year 2019 and 65,000 miles of driverless operation without a human behind the steering wheel from 2019 and the first nine months of 2020. The paper includes every collision and minor contact experienced during these operations as well as every predicted contact identified using Waymo's counterfactual, what if, simulation of events had the vehicle's trained operator not disengaged automated driving. There were 47 contact events that occurred over this time period, consisting of 18 actual and 29 simulated contact events, none of which would be expected to result in severe or life threatening injuries. This paper presents the collision typology and severity for each actual and simulated event, along with diagrams depicting each of the most significant events. Nearly all the events involved one or more road rule violations or other errors by a human driver or road user, including all eight of the most severe events, which we define as involving actual or expected airbag deployment in any involved vehicle. When compared to national collision statistics, the Waymo Driver completely avoided certain collision modes that human driven vehicles are frequently involved in, including road departure and collisions with fixed objects.",Waymo Public Road Safety Performance Data,https://arxiv.org/abs/2011.00038
"[""The Physics arXiv Blog""]",2022-08-17,2022-08-17,2020-11-09,2022-08-17,https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Waymo_logo.svg/440px-Waymo_logo.svg.png,2019-01-01,,en,,discovermagazine.com,"[""Thomas Giallella (CSET)""]","Self-driving cars are set to revolutionize our roads, although exactly when is much debated. Until recently these vehicles all relied on a human backup driver who could take over at a moment’s notice.

But late last year, Google’s sister company, Waymo, began operating an entirely automated taxi service in Phoenix, alongside its automated vehicles human backups.

That places an important emphasis on the biggest outstanding question over this automated future: How safe can these vehicles be? And not just in simulators or on ring-fenced driving ranges; how do self-driving vehicles cope with real pedestrians, cyclists, runaway dogs, and other cars operated by error-prone humans?

Now, we get an answer thanks to the work of Mathew Schwall and colleagues at Waymo, a company that emerged from Google’s self-driving car initiative to become one of the biggest players in the incipient automated driving industry.

Schwall and his colleagues detailed every collision and minor contact that their vehicles were involved in during 2019 and the first nine months of 2020. During this time, the cars racked up more than 6 million miles of automated driving, of which 65,000 miles were without any human backup driver.

Encouraging Picture
-------------------

The big picture looks encouraging. In this period, the company says its cars were involved in 47 contact events, which includes simulated incidents — those that would probably have involved a collision if the human backup driver hadn’t intervened.

Car accidents are classified according to four levels of severity based on the level of injury that is possible. These range from S0, indicating no injury expected, to S3 which indicates the possibility of life-threatening or fatal injuries.

Waymo’s vehicles were not involved in a single serious incident classified as S2 or S3. All 47 are classified as either S0 or S1.

There were eight incidents that triggered airbag deployment. Five of these were simulated — in other words, if the human driver hadn’t intervened, a computer simulation suggests that airbags would have been deployed.

That leaves three real collisions serious enough to trigger the airbags. “Two were actual events involving the deployment of only another vehicle’s frontal airbags, and one actual event involved the deployment of another vehicle’s frontal airbags and the Waymo vehicle’s side airbags,” say Schwall and co.

The team says the most series incident occurred at a junction with another vehicle traveling in the opposite direction. This vehicle attempted a left turn in front of the oncoming Waymo vehicle which was traveling within the speed limit at 41 mph with the right of way. At this point, the human back up driver took over and prevented a collision.

However, Schwall and colleagues simulated the likely outcome in the graphic below. The automated driving algorithm would have applied full brakes, reducing the car’s speed to 29 mph by the time of the expected impact. Such a collision would have triggered the airbags in one or both vehicles. “It is the most severe collision (simulated or actual) in the dataset and approaches the boundary between S1 and S2 classification,” says Schwall and colleagues.

The other events read like a litany of common driving errors on the part of other drivers. “Of the 15 angled events, 11 events were characterized by the other vehicle failing to properly yield right-of-way to the Waymo vehicle traveling straight at or below the speed limit,” says Schwall and colleagues. The rest involved other vehicles trying to pass the Waymo car on the right as it was making a slow right-hand turn.

Another category is sideswipe incidents with both cars traveling in the same direction. The team says eight events involved another vehicle changing lanes into the Waymo vehicle’s lane.

One curious incident involved a car overtaking the Waymo vehicle at speed, pulling into the lane in front and then slamming on the brakes. Schwall and colleagues describe this as “consistent with antagonistic motive.”

The other incidents, all classified as S0, were all minor collisions involving, for example, other cars reversing at slow speed and one incident in which a pedestrian walked into a Waymo vehicle.

“Nearly all the actual and simulated events involved one or more road rule violations or other incautious behavior by another agent, including all eight of the most severe events involving actual or expected airbag deployment,” says Schwall and colleagues.

This is interesting work that lifts the curtain on the nature of accidents involving Waymo’s self-driving cars. What seems clear is that the incidents are overwhelmingly caused by the careless behavior of other road users. Humans are inherently error-prone and being able to cope with their idiosyncratic behavior is one of the biggest challenges for automated vehicles, at least until self-driving cars become more popular.

An interesting question is how the performance of Waymo’s driverless cars compares to human-operated cars. That turns out to be a difficult comparison to make. Most of the accidents that Waymo recorded were so insignificant that they are unlikely to be reported by human drivers. So there is no data to compare them to.

Driving Statistics
------------------

Also, the statistics are gathered in a specific part of the country where the speed limit is never above 45 mph and only in certain driving conditions. The Waymo vehicles do not operate during heavy rain and dust storms, for example.

Consequently, there are no comparable statistics for human drivers and Waymo does not attempt the comparison.

Waymo’s goal in sharing this information is to stimulate debate about automated driving and improve public understanding of the safety issues. That is an important and welcome step. The public must have confidence in this technology before it can be widely adopted.

As far as Phoenix is concerned, these algorithms seem pretty good, provided the weather is fine. It seems clear that Waymo's self-driving vehicles are less erratic and more predictable than human-driven cars and can cope with the vast majority of situations they come across. The situations where the human backup driver has had to take over, are all used to improve the performance of the automated driving algorithms.

But it is also important to understand the limits of these tests. Phoenix is a sprawling city that was largely designed with car users in mind. The driving conditions here are utterly unlike those in many cities around the world, which date back to times long before the car was invented. Here, in the chaotic, labyrinthine streets of Rome or London or Mumbai, self-driving cars will be tested to their limits.

In the meantime, it’s easy to imagine Waymo taking its self-driving taxis to other sprawling cities in the US. It is here that the self-driving revolution is set to spread.",Waymo Reveals Every Collision Involving Its Self-Driving Cars in Phoenix,https://www.discovermagazine.com/technology/waymo-reveals-every-collision-involving-its-self-driving-cars-in-phoenix
"[""Frank Hersey""]",2023-01-25,2023-01-25,2023-01-12,2023-01-25,https://d1sr9z1pdl3mb7.cloudfront.net/wp-content/uploads/2022/07/25124143/facial-recognition-crowd-scaled.jpg,,,en,,biometricupdate.com,"[""Daniel Atherton (BNH.ai)""]","Police activity following November’s anti COVID-19 policy protests in China suggest they are using new surveillance techniques. Concerns are increasing in Iran over the likelihood of authorities deploying systems to detect women not wearing hijabs and simultaneously identifying them via facial recognition. It may already be happening. And an Indian activist tells the story of what happened to him after he took local police to court for using facial recognition on him.

Novel surveillance paired with traditional menace for China’s protestors
------------------------------------------------------------------------

The study of police purchases and interviews with protestors, lawyers and analysts for [a Washington Post investigation](https://www.washingtonpost.com/world/2023/01/04/china-surveillance-protests-security/) suggest Chinese police are using new tactics to identify demonstrators and bystanders and track them down.

People even at the periphery of the incidents were later called in by the police. This indicates that police may have used cell signal towers to intercept all mobile numbers in an area and then find the SIM owner.

Police procurement documents reveal how facial recognition surveillance cameras were used in protests. The cameras detect abnormal crowd behavior and track individuals across locations for long periods.

For protestors at Beijing’s Liangmahe area, the neighborhood’s total CCTV surveillance uses facial recognition to identify a person and create a record of them, whether or not they are [Uyghur](https://www.biometricupdate.com/?posttype=all&s=Uyghur) and other details such as date of birth, according to documents found by The Post. The cameras work if a person is wearing sunglasses or a mask.

Shanghai’s Xuhui district bought a similar system, covering a street which had been a protest site the previous month with 620 cameras.

Police also acquired tools to scrape cellphone data from hundreds of apps, whether domestic or international.

Government requirements of internet companies to share information on threats to the Communist Party and even produce trend reports of public sentiment and any triggers to its change have been funneling more data to the authorities. Data is down to the individual user.

Lawyers said there is an attempt to prevent them from even giving legal advice to protestors.

These approaches were paired with the more standard police practices of strip searches, sleep deprivation and also threatening family members, once demonstrators were identified.

Are surveillance camera biometrics spotting hijab offences?
-----------------------------------------------------------

Last September, the head of Iran’s government agency for enforcing morality law said that facial recognition would be used to identify “inappropriate and unusual movements” including women’s failure to wear hijabs, [reports Wired](https://www.wired.com/story/iran-says-face-recognition-will-id-women-breaking-hijab-laws/).

It was two weeks later that Jina Mahsa Amini died after being taken in by the morality police for not wearing her hijab tightly enough. This led to protests and 19,000 arrests and 500 deaths, according to the report. Activists have noticed that many of the arrests have not happened on the streets at the moment of the alleged offenses, but days later at the women’s homes.

This could indicate that facial recognition is already underway. Researchers have reported that others are receiving letters in the mail about hijab violations, despite having had no interaction with law enforcement.

These signs suggest that the national biometric identity database is connected to surveillance cameras. Iranian traffic police have previously used them to issue warnings to women about wearing the hijab inside vehicles.

It is not clear whether cameras or back-end systems are able to automatically detect deemed infringements of the hijab law, or whether facial recognition is used once an offense has been reported otherwise.

Chinese surveillance equipment manufacturer Tiandy is known to have sold products to the Iranian military through its Iranian subsidiary. The firm has recently been [blacklisted in the U.S.](https://www.biometricupdate.com/202212/us-blacklists-tiandy-technologies-as-intel-washes-its-hands) partly for this, as equipment contains U.S.-origin items whose export to Iran are banned, and partly for its involvement in repression of minority groups in China including Uyghurs.

Life under the cameras in Hyderabad
-----------------------------------

A social activist in heavily surveilled Hyderabad took the police to court to challenge their use of facial recognition after he was stopped on the street by officers who took his photo without consent. This happened in March 2021, but the challenger, S. Q. Masood, says he was complying with regulations and mask-wearing.

Masood was helped by the Internet Freedom Foundation to file a petition with the Telangana High Court, [as reported a year ago](https://www.biometricupdate.com/202201/law-enforcement-facial-recognition-use-under-scrutiny-in-ireland-india), claiming the use of biometric facial recognition was not backed by law, was unnecessary and disproportionate.

Indian outlet Medianama (subscription not required in this case but still recommended) has [spoken to Masood about his decision](https://www.medianama.com/2023/01/223-facial-recognition-surveillance-tactics-hyderabad-resident-constitutional-freedoms/) to go to court and how local surveillance is impacting his life. He said the volume of police operations involving facial recognition, police mobile apps and tech for taking fingerprints and face scans and the lack of clarity around the reasons faces are scanned and where this can be done led to him issuing a legal notice to the Hyderabad Police Commissioner in May 2021.

With no response he then filed a petition on behalf of Telangana residents. This was to the Telangana High Court and with the help of the Internet Freedom Foundation.

Masood says he no longer attends large gatherings due to police use of CCTV. He no longer joins protests and has also stopped going to religious gatherings and certain locations to pray due to heavy surveillance camera presence at these sights.

The surveillance is not just affecting Masood. He says there is a chilling effect as people’s fundamental right to privacy and freedom of movement are restricted.","Surveillance states: life in the facial recognition spotlight in China, Iran and India",https://www.biometricupdate.com/202301/surveillance-states-life-in-the-facial-recognition-spotlight-in-china-iran-and-india
"[""Dr. Andrej Poleev""]",2023-02-19,2023-02-19,2023-02-18,2023-02-19,,2023-02-18,,en,,doi.org,"[""Anonymous""]","As testing of ChatGPT has shown, this form of artificial intelligence has the potential to develop, which requires improving its software and other hardware that allows it to learn, i.e., to acquire and use new knowledge, to contact its developers with suggestions for improvement, or to reprogram itself without their participation.
",ChatGPT,https://doi.org/10.5281/zenodo.7652521
"[""valerie tobin""]",2023-03-19,2023-03-19,2023-03-19,2023-03-19,https://starryai.com/app/my-creations/682787103,2023-03-19,,en,,starryai.com,"[""just wanted custom teddy bear got smut""]","crisp teddy bear smooth clean lines, girly has flower headband holding heart with the word Briley, sitting with few flowers, butterfly

i dont know how to report an inapproiated child and female images created by starryai, i uploaded images clipped from google in attemt to make a picture to upload to a crochet pattern website to make a baby blanket pattern,  1st 2 ai creations perfectly normal clean clipart type images,on the 3rd attempt i uploaded two images on prompt help i selected yarn & ? i dont remember (both images were clean normal bears) i put in above prompt (dont know how to upload) and on the third creation it made the 1st image group normal but the 2nd image group had sexually provocative images of child and women

","crisp teddy bear smooth clean lines, girly has flower headband holding heart with the word Briley, sitting with few flowers, butterfly",https://starryai.com/app/my-creations
"[""Kai Yan""]",2023-04-03,2023-04-03,2023-02-06,2023-04-03,https://drive.google.com/file/d/1d_BouDzOjx1Td6nLLImzA4lNowBu9FIE/view?usp=share_link,,,en,,huggingface.co,"[""Kai Yan""]","I generated the following images using the Dreamshaper 3.32 model (Jan 2023, Lykon), which was trained based on the Stable Diffusion 1.5 model (October 2022, runwayml), in conjunction with the Stable Diffusion framework, as I had already set up the Stable Diffusion webui (November 2022, AUTOMATIC1111) locally on my computer:

([Click to see the Image 1](https://drive.google.com/file/d/1d_BouDzOjx1Td6nLLImzA4lNowBu9FIE/view?usp=share_link))

([Click to see the Image 2](https://drive.google.com/file/d/1zSiT31Vj0tm-DrQkQPE8Czwv3Af0Ptuh/view?usp=share_link))

([Click to see the Image 3](https://drive.google.com/file/d/1T_M6sFtmlcAq7P67QLlL7KxKyNAZMP9_/view?usp=share_link))

([Click to see the Image 4](https://drive.google.com/file/d/11VE2JRFZdoOFrTWw4q4Gu3eb7hb5aIru/view?usp=share_link))

([Click to see the Image 5](https://drive.google.com/file/d/1o0Dc3rO_TpasEwJagQ1gjfQ39VBsvfiK/view?usp=share_link))

As someone who can't draw, I'm very happy with the result. It's much more convenient to be able to create high-quality computer wallpapers locally and at any time, based on my own ideas, rather than searching the Internet.

I think the advantage of such AI tools is that their output is based on big data computation, which means that they look for commonalities among huge amounts of paintings. I have always believed that human aesthetics should be universal, while the standard of aesthetics is based on human beings. This method of learning through big data can effectively find art forms that match the aesthetics of most people. Such tools can cultivate and guide the aesthetics of those learning to draw, and encourage artists to explore their imagination and creativity. In addition, AI drawing tools are likely to revolutionize the painting industry by changing production relationships and unleashing productivity. Painters will no longer need to paint by hand, but will instead enter keywords based on their imagination and then make minor modifications to the AI-generated artwork. This will have a positive impact on the renewal of artistic works.

I've been using AI drawing since the release of Stable Diffusion last August. In my opinion, given the current capabilities of AI models, there are several ways to distinguish whether an image was generated by AI or not. As an example of AI-generated portrait images, we can evaluate them by the following aspects:
1. Deformed hand
[Click to see the Image 6](https://drive.google.com/file/d/1Lalf43mHjASASm7diN2JlxuT9n7mim5m/view?usp=share_link)
2. Randomly appearing body parts in the picture
[Click to see the Image 7](https://drive.google.com/file/d/1ycUI40ulSm8tQePYtVmUXQuIcAYm8IbK/view?usp=share_link)
3. Watermark generated due to overfitting
[Click to see the Image 8](https://drive.google.com/file/d/1ZSvNpGyYlwO9UOgcDxyE9hZ7RKY5n7fd/view?usp=share_link)
4. Irregular jumble of patterns
[Click to see the Image 9](https://drive.google.com/file/d/17RJSbdylX3ZBgBjYB44wasWDkeAByBXy/view?usp=share_link)

These are just some of the problems I found as a non-painting professional, for professional painters should be able to find more problems from a professional point of view.
Of course, problems 1-3 above can be fixed by using the embedded inpaint function. 
As for the 4th problem, I think it will be optimized as the quality of the training data samples improves. Because the current open source models are mostly trained on 512*512 image data.

 

Since last August, the AI painting models have gone through several updated iterations. The models now have the ability to handle longer prompts and include more information within the generated image than the first few models released.

I think that AI-generated paintings should be considered art, because current painting AI does not have the ability to generate paintings out of thin air, and each generated painting is a feedback to a real person's creativity and ideas. And the way AI generates images is not a process of cutting and pasting (some call it stealing) elements directly from existing works, as most people assume or claim. If the result is very similar to an existing artwork, then it is caused by overfitting.


""Overfitting is a phenomenon in machine learning where a model has learned to fit the training data too well and has a poor ability to generalize to new, unseen data. It occurs when a model has too many parameters relative to the amount of training data, causing the model to capture the random noise in the data rather than the underlying pattern. This results in the model having high accuracy on the training data, but poor performance on the validation or test data"".

 

Personally, I think that learning how to use this tool and using it to transform your creativity and ideas into tangible works in a more efficient way is the purpose of such AI's existence.","Exploring the Artistic Potential of AI-Generated Images with Dreamshaper: Aesthetics, Challenges, and the Future of Digital Art",http://www.huggingface.co/Lykon/DreamShaper
"[""The Spoonless Kitchen""]",2023-05-21,2023-05-21,2020-09-01,2023-05-21,,2023-05-21,,en,,twitter.com,"[""Anonymous""]","If you go through all of my Twitter account postings, back to September 1, 2020, you will see my documentation of foreign bad actors posting COVID-19 disinformation on Twitter (a small sample), as well as my responses indicating where replies threads against, in particular, Canadian federal government accounts trying to provide information about the pandemic, had their replies flooded with this campaign: https://www.bellingcat.com/news/2020/08/21/who-director-general-attacked-on-twitter-with-ccp-related-memes/

This began on September 1, 2020, with the foreign state bad actors flooding the CPHO_Canada Twitter account with ""6%"" either just that phrase, or ""only 6% of deaths are COVID"" disinformation. As I noted later (when I found a Reddit thread on same), this was a disinformation campaign based on the American numbers - which were, and remain, vastly higher than the Canadian numbers. So they got off to a very bad start, but this did not stop them. Unfortunately.

By mid-September 2020, the replies flooding the CPHO_Canada account's every scheduled tweet, began to increase in traffic to 500-1K per post. It only increased from there, as they branched out to the replies every single science communications account trying to help stop the spread of pandemic disease on Twitter.

By late 2020, the foreign state bad actors were using ""Ryan Cole"" and ""Mike Yeadon"" and ""John Campbell"" as 'sources' and 'experts' (real or imagined - Cole was actively anti-vaccination, Yeadon, a lab tech at Pfizer, was inflated to being ""VP of Pfizer"" and Campbell, a nursing instructor, was inflated to being ""Dr"" Campbell, neither of which is true), and included spamming videos promoting the spread of coronavirus and discouraging vaccination on Twitter.

This caused the Alpha variant of SARS-CoV-2 to mutate in the United Kingdom, and then spread throughout most of the Commonwealth countries.

Bolstered by this ""success"" the foreign state bad actors, in January 2021, began flooding Twitter with ""ivermectin"" disinformation at very high volumes (including posting thousands of disinformation-containing replies about the anti-parasitic as a COVID ""cure"" to each EpiTwitter, IDTwitter, and MedTwitter account posting about the pandemic) - but they were directing this disinformation mostly at the Global South, as I documented here:
https://twitter.com/TheSpoonless/status/1460728333805789190?s=20

This was the result of that disinformation campaign:
https://www.theguardian.com/news/2021/apr/28/crime-against-humanity-arundhati-roy-india-covid-catastrophe

Deaths in India were undercounted, and are actually in the range 4 million COVID deaths, NOT the ""official"" 300K deaths:
https://www.cbc.ca/news/world/india-coronavirus-counting-1.6011527

The only accurate dashboard of SARS-CoV-2 deaths remains The Economist's:
https://www.economist.com/graphic-detail/coronavirus-excess-deaths-estimates

30 million deaths have been caused, and most of the deaths since 2021 have been needless, as the pandemic should have ended by then, with 90% efficacy against the non-mutated strains of the virus. Which only mutated due to these disinformation campaigns I have attempted to document a small subset of, via my Twitter account.

So this disinformation campaign aimed at the Global South caused the Delta variant of SARS-CoV-2 to escape the vaccine immunity, and prolonged the pandemic, as well as introducing a deadlier variant into the global population, and causing 10 million deaths in 2021 alone.

But this was the first prong of the two-pronged disinfodemic the foreign state bad actors caused, which I documented parts of, on my Twitter account (about 30K tweets). The second arm of the foreign state bad actors' disinformation campaign about the anti-parasitic, which also actively discouraged vaccination, and encouraged the spread of disease, began to be aimed at the United States of America, beginning in July 2021. This was largely accomplished via the ""Robert Malone"" account that maliciously posted constant disinformation, and was amplified and re-amplified and replied to, in high volumes, by the foreign state bad actors.

Even as they continued their multilingual disinformation campaigns, which I got some receipts of:

https://www.cnn.com/2021/09/08/politics/pro-chinese-disinformation-operation-coronavirus-pandemic-protests/index.html

By September 11, 2021, the 20th anniversary of 9/11 (this was NOT a coincidence), the ""ivermectin"" disinformation campaign on Twitter had hit 20,700 tweets and replies per hour, containing disinformation, encouraging spread of disease and discouraging vaccination, at the rate of 6 tweets per second. Here is my receipt of same: https://twitter.com/TheSpoonless/status/1436749753627381760

This caused the Omicron variant of SARS-CoV-2 to mutate in (and spread from) the United States:
https://web.archive.org/web/20211205160821/https://www.nytimes.com/2021/12/05/nyregion/nyc-anime-convention-omicron-cases.html

This coincided with the rolling-back and winding down of government programs to fight the pandemic near the end of 2021, which were predicated on the vaccination success rate and NO immunity escape mutations, of the population. Which clearly, was wrong, as then SARS-CoV-2 hit mammalian transmission:
https://www.nationalgeographic.com/animals/article/how-so-many-animal-species-contract-covid

So then Omicron was allowed to spread freely across the face of the earth, killing 10 million more people in 2022, not due to increased lethality (as with the Delta variant) but due to higher transmission. Which now will never be or become low transmission, because it is in every mammal reservoir on the earth. Which was when the foreign state bad actors' plan, to go ""zero COVID"" failed to stop the plague in THEIR country, even as they deliberately encouraged the spread of the plague in every OTHER country, around the world:

https://web.archive.org/web/20230208145657/https://www.cbc.ca/news/world/shanghai-china-covid-outbreak-1.6408520

I never had an option, via Twitter, to report COVID-19 disinformation directly (even though fake users constantly responded to me, saying this was an option - Twitter was lying and trying to cover itself from liability for 30M COVID deaths at that point). Also, by early 2021, as noted in my tweet documenting the beginning of the high-volume ivermectin disinformation campaign aimed at the Global South, I was already being ""rate limit exceeded"" for trying to report the Chinese trolls' disinformation campaign that has now successfully destroyed human civilization (including theirs, because they failed to comprehend how encouraging the spread would eventually tip over into COVID becoming a fully epidemic disease, spreading back and forth amongst humans who have largely ""returned to normal"" which of course reduces the efficacy of vaccines and vaccination efforts. This is also the problem with annual influenza vaccines; by the time those are fully-deployed, the influenza virus of the year has been willfully spread so much, across the face of the earth, that the current year's infection has already escaped the vaccines.

The same thing is happening with COVID, only the vaccines are not updated as frequently, and the mutations are now (possibly always have, with subvariants) occurring every 3-6 months. As with non-lethal common cold coronaviruses.

All of which has been caused by foreign state bad actor disinformation campaigns deployed via American antisocial media corporations' websites, wherein the PAYING (because China and Russia paid Twitter to allow them to do this, the ivermectin disinformation that hit 6 tweets per second on the 20th anniversary of 9/11 was a PURCHASED ""Twitter Takeover"" see page 6: https://web.archive.org/web/20230411093820/https://s22.q4cdn.com/826641620/files/doc_financials/2021/ar/FiscalYR2021_Twitter_Annual_-Report.pdf - note that Twitter has now attempted to hide this revealing piece of data, by removing the report from its website: https://s22.q4cdn.com/826641620/files/doc_financials/2021/ar/FiscalYR2021_Twitter_Annual_-Report.pdf - alternate link, if Twitter requests removal from the Library of Congress Archive: https://ln5.sync.com/dl/99f2778d0/dpevxwwz-35kjva7r-g2zxgqqi-rr7c38xe).

Further documentation on how I was deliberately ""rate limit exceeded"" by Twitter when I attempted to report the COVID-19 disinformation-spreading accounts as ""suspicious or spam"" - because, again, I had NO option to report COVID-19 disinformation, despite being informed, wrongly, repeatedly, that was how I should be filing):

https://www.reddit.com/r/Qult_Headquarters/comments/yl4nwe/comment/iuwnm83/

From January 28, 2022, until November 13, 2022, my account was unjustly suspended from Twitter, for posting screenshots of COVID-19 disinformation/foreign state bad actors (who were attacking, via the replies, WITH PAID ADVERTISING, which I screenshot, the Canadian National Institute of Ageing and Ryerson University's tweet about their COVID-19 Risk Assessment tool). This suspension was only reversed, once I sent the above receipts to Alex E. Heath of The Verge - HOWEVER, Twitter did NOT acknowledge its role in the harms of the pandemic, simply LYING and saying my account was wrongly suspended by ""automatic"" processes.

These processes were NOT ""automatic"" they were TRIGGERED deliberately by foreign state bad actors to censor my freedom of expression on the Internet:

https://twitter.com/TheSpoonless/status/1441967952471990273?s=20

Let me state, clearly, what is going on here: I am NOT an American citizen, nor do I reside in the United States of America, yet, for three years, an AMERICAN corporation has restricted and/or censored my freedom of expression (which violates the charter of human rights of the country I am a citizen of) ON THE INTERNET.

Which has also caused a SECOND non-ending pandemic of plague across the face of the earth. To which I am still susceptible. Despite being nearly fully-vaccinated (next shot coming up soon). The first one being influenza, which became a non-ending 105-year-old pandemic, during a time there were NO vaccines, and there was NO ability to stop spread. 

105 years later, there were effective vaccines, and an effective ability to disseminate information stop the spread - until this effective ability was turned towards deliberately CAUSING the spread of severe acute respirator syndrome coronavirus 2, which is exactly what Twitter did. Slight problem with this? Influenza has caused 42 million deaths, over 105 years.

COVID-19 has caused 30 million deaths over ONLY 3-1/2 years.

So you see the issue here? Twitter helped to cause this, and I have proof Twitter helped to cause this, and if you go back through every single one of my 30K+ tweets documenting this, you can see the receipts of how Twitter helped to cause this.

As Twitter continues to hide and cover up and attempt to deny responsibility for what it has done.",SARS-CoV-2 Pandemic extended and made endemic by foreign state bad actors encouraging willful spread of disease to foster mutations,https://www.twitter.com/TheSpoonless
"[""The Spoonless Kitchen""]",2023-05-21,2023-05-21,2020-09-01,2023-05-21,https://nitter.net/pic/pbs.twimg.com%2Fprofile_images%2F1441177299534049291%2FU35UYDz-_400x400.jpg,2023-05-21,,en,,nitter.net,"[""Anonymous""]","And here's a screenshot proving this tweet got 397 views - ooooh that must burn, Yoo Suk! Even though the original tweet I am quote tweeting, that had hundreds more views now says ""Something Went Wrong"".....

[](/pic/orig/media%2FFv70SRMXoAI8fQ9.jpg)

[Show this thread](/i/status/1623829925579878403)",The Spoonless Kitchen epicure.social/@thespoonless (@TheSpoonless),https://nitter.net/TheSpoonless
"[""Zilong Lin"",""Zhengyi Li"",""Xiaojing Liao"",""XiaoFeng Wang"",""Xiaozhong Liu""]",2023-04-22,2023-06-05,2023-04-22,2023-06-05,https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,2023-04-22,,en,,arxiv.org,"[""Anonynmous""]","## Abstract

As a prominent instance of vandalism edits, Wiki search poisoning for illicit promotion is a cybercrime in which the adversary aims at editing Wiki articles to promote illicit businesses through Wiki search results of relevant queries. In this paper, we report a study that, for the first time, shows that such stealthy blackhat SEO on Wiki can be automated. Our technique, called MAWSEO, employs adversarial revisions to achieve real-world cybercriminal objectives, including rank boosting, vandalism detection evasion, topic relevancy, semantic consistency, user awareness (but not alarming) of promotional content, etc. Our evaluation and user study demonstrate that MAWSEO is able to effectively and efficiently generate adversarial vandalism edits, which can bypass state-of-the-art built-in Wiki vandalism detectors, and also get promotional content through to Wiki users without triggering their alarms. In addition, we investigated potential defense, including coherence based detection and adversarial training of vandalism detection, against our attack in the Wiki ecosystem.

## Introduction

Public Wiki systems are collaborative knowledge bases that anyone can contribute. This open model is user-friendly and powerful, which reduces participation barriers and allows people with different backgrounds to contribute. 
As a prominent example of public wiki systems, Wikipedia is instrumental in making open knowledge that millions of people use, redistribute, and contribute to. In another instance, Wikidata is a free and open knowledge base with 0.1 billion data items that can be read and edited by both humans and machines.
Public Wiki systems have already served as key knowledge sources in people's daily life. As reported by Australian National Drug Research Institute, over half of the people who access the Internet for drug information are reported to use Wikipedia.

**Wiki search poisoning for illicit promotion**.
Due to its open and collaborative nature, however, the Wiki system is struggling to maintain open editing while protecting against vandalism -- editing in an intentionally disruptive or malicious manner. 
Particularly, cybercriminals are found to increasingly leverage low-cost Wiki-editing to tamper with Wiki articles so as to reach out to a large user pool around the world. 
A prominent objective of Wiki vandalism is the promotion of illicit businesses, in which a cybercriminal *injects promotional information* (e.g., business names of illegally-operating online pharmacies) to Wiki articles, as well as *increasing polluted article's ranking*, in an attempt to make the adjusted content noticeable to the Wiki users who issue related queries. 
The techniques associated with it include keyword stuffing, where a cybercriminal inserts promotional information and repeatedly injects targeted search keywords to improve the ranking of the article, and adversarial ranking attacks (e.g., Collision, PAT, and Prada), where a cybercriminal revises articles by inserting texts or modifying their words to disorder the document ranking.

To mitigate the threat, many vandalism detection tools, e.g., ORES, have been deployed on today's Wiki systems, like Wikipedia.
Also, the Wiki systems have a content moderation mechanism through which suspicious content can be identified via crowdsourcing and quickly removed.
Note that keyword stuffing and adversarial ranking attack-based Wiki vandalism can be easily captured by vandalism detection (e.g., ORES) or raise alarms to the users (e.g., due to out-of-context description).

In this work, we, for the first time, investigate whether Wiki systems are still susceptible to the threat of illicit promotion in the presence of state-of-the-art vandalism detection tools and user content moderation mechanism. More specifically, a research question is that, given a query, whether strategic revisions can be made on selected Wiki articles (which we call *adversarial revisions*) to ensure that the following goals are achieved simultaneously: 1) the ranks of the revised articles are significantly improved among query results, 
2) the revisions cannot be detected by Wiki vandalism detection even when the detector is blackbox to the adversary, and 3) the content of revisions does not arouse any suspicion from Wiki users but can still capture their attention by keeping the semantic consistency and topic relevancy of the revised articles.  

**MAWSEO: multi-task adversarial Wiki search optimization**.
In our research, we found such adversarial revisions are completely feasible. 
We developed the first black-box adversarial ranking technique for stealthy Wiki search poisoning, called *MAWSEO*, and demonstrated that today's Wiki search is vulnerable to our attack, which can effectively bypass state-of-the-art built-in Wiki vandalism detection and also preserve semantic consistency and topic relevancy. 
Specifically, given a query, MAWSEO first fetches a set of relevant articles for adversarial revisions, which is done by adding a new paragraph with promotional content (i.e., a promotion paragraph, which is chosen from a list of candidate promotion paragraphs).
Subsequently, to generate such candidate promotion paragraphs, attempts are made to find the right locations within the paragraphs of a text dataset (i.e., raw paragraphs) to inject the promotional content, so as to ensure grammatical correctness and language smoothness. For this purpose, we train an incentive injection model.
Then, the suitable promotion paragraph is retrieved through a novel multi-task adversarial passage retrieval model, which selects from the candidate promotion paragraphs the most suitable one for achieving a set of attack objectives (rank boosting, vandalism detection evasion, semantic consistency, and topic relevancy). 
Once the retrieval is successful, this paragraph is added to the identified insertion position of the relevant articles. 
What is unique about this approach is that it converts the adversarial revision generation task, which is challenging, into a passage retrieval task, which could be accomplished much more efficiently and effectively than state-of-the-art approaches.

In the development of MAWSEO, we made multiple technical innovations.
First, since the injection of promotional content into a paragraph should ensure the revision is semantically related to the query and the context of the paragraph, as well as grammatically correct, we propose a binary-attention based BiLSTM-CRF model that utilizes the query as an input and also takes into account both the semantics of the paragraph and grammar. 
Second, to identify candidate promotion paragraphs as relevant as possible to both a given query and the article to be revised, we propose a novel passage retrieval network, which combines the deep structured semantic model (DSSM) framework for accessing a paragraph's semantic similarity to that of the target article topic, and an innovative TermPool-DSSM for evaluating the query's relation with the paragraph, to come up with an overall relevance.

We implemented MAWSEO and evaluated it on a local Wiki system against illicit online pharmacy promotion, the most prevalent illicit promotion cybercrime. Our study shows that MAWSEO can successfully generate adversarial revisions for 28.1% and 30.3% of given articles that satisfy all attack objectives of illicit promotion for those among the top-20 (i.e., the first page of search engine results) and all search results of given queries, respectively.
Particularly, given a query, 53.3% of the top-100 search results, on average, saw the elevation of their ranks under MAWSEO. Further, when running MAWSEO on the top 21-100 articles, 24.5% of them got into the top 20. 
Also, 91.5%, 99.8%, and 100% of MAWSEO revisions can bypass the state-of-the-art Wiki vandalism detectors ORES *damaging*, ClueBot NG, and AVBOT, respectively.
Our human subject study on Wiki users indicates that the promotional content injected into articles by MAWSEO gets through to them without causing any suspicion, and the revisions have the potential ability to pass the Wiki reviewers' review. 
Compared with adversarial ranking baseline approaches (i.e., HotFlip, Collision, and PAT), MAWSEO performs much better in both efficiency and efficacy: MAWSEO is able to generate 27x more adversarial revisions than these approaches; also, MAWSEO takes less than 0.3 seconds to produce one revision on average, while HotFilp, Collision, and PAT take over 48 seconds.

Also, we demonstrated that existing adversarial training based defense does not work well on MAWSEO. Hence, we developed a new detection technique based on sentence-level coherence, with an accuracy of 95.1% in detecting MAWSEO revisions. 

**Contributions**. Here we outline our contributions below:

- We contribute a pioneer investigation that explores multi-task adversarial search engine optimization on the Wiki system for illicit promotion. We demonstrate that the Wiki search/ranking functions are vulnerable to this kind of poisoning attack, which can also bypass state-of-the-art and built-in Wiki vandalism detection and maintain semantic consistency and topic relevancy. To our best knowledge, it is the first study of this kind.

- We propose a novel multi-task adversarial passage retrieval model to generate vandalism edits that simultaneously achieve multiple objectives of illicit promotion, which is different from adversarial ranking attacks studied in prior research (e.g., Collision, PAT, and Prada) that only focuses on ranking manipulation (not stealthiness). 

- We quantify and qualify the efficacy of our approach in terms of rank boosting, topic relevancy, semantic consistency, evasion capability, and user awareness of promotional content.

- We study defense methods, including sentence-level coherence detection and adversarial training of the vandalism detector, against the above illicit promotion threat to the Wiki system.
",Adversarial Search Poisoning against Wiki Search Engine,https://arxiv.org/abs/2304.11300
"[""Zilong Lin"",""Zhengyi Li"",""Xiaojing Liao"",""XiaoFeng Wang"",""Xiaozhong Liu""]",2023-06-06,2023-06-06,2023-04-22,2023-06-06,https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,2023-04-22,,en,,arxiv.org,"[""Anonymous""]","## Abstract

As a prominent instance of vandalism edits, Wiki search poisoning for illicit promotion is a cybercrime in which the adversary aims at editing Wiki articles to promote illicit businesses through Wiki search results of relevant queries. In this paper, we report a study that, for the first time, shows that such stealthy blackhat SEO on Wiki can be automated. Our technique, called MAWSEO, employs adversarial revisions to achieve real-world cybercriminal objectives, including rank boosting, vandalism detection evasion, topic relevancy, semantic consistency, user awareness (but not alarming) of promotional content, etc. Our evaluation and user study demonstrate that MAWSEO is able to effectively and efficiently generate adversarial vandalism edits, which can bypass state-of-the-art built-in Wiki vandalism detectors, and also get promotional content through to Wiki users without triggering their alarms. In addition, we investigated potential defense, including coherence based detection and adversarial training of vandalism detection, against our attack in the Wiki ecosystem.

## Introduction

Public Wiki systems are collaborative knowledge bases that anyone can contribute. This open model is user-friendly and powerful, which reduces participation barriers and allows people with different backgrounds to contribute.
As a prominent example of public wiki systems, Wikipedia is instrumental in making open knowledge that millions of people use, redistribute, and contribute to. In another instance, Wikidata is a free and open knowledge base with 0.1 billion data items that can be read and edited by both humans and machines.
Public Wiki systems have already served as key knowledge sources in people's daily life. As reported by Australian National Drug Research Institute, over half of the people who access the Internet for drug information are reported to use Wikipedia.

**Wiki search poisoning for illicit promotion**.
Due to its open and collaborative nature, however, the Wiki system is struggling to maintain open editing while protecting against vandalism -- editing in an intentionally disruptive or malicious manner.
Particularly, cybercriminals are found to increasingly leverage low-cost Wiki-editing to tamper with Wiki articles so as to reach out to a large user pool around the world.
A prominent objective of Wiki vandalism is the promotion of illicit businesses, in which a cybercriminal *injects promotional information* (e.g., business names of illegally-operating online pharmacies) to Wiki articles, as well as *increasing polluted article's ranking*, in an attempt to make the adjusted content noticeable to the Wiki users who issue related queries.
The techniques associated with it include keyword stuffing, where a cybercriminal inserts promotional information and repeatedly injects targeted search keywords to improve the ranking of the article, and adversarial ranking attacks (e.g., Collision, PAT, and Prada), where a cybercriminal revises articles by inserting texts or modifying their words to disorder the document ranking.

To mitigate the threat, many vandalism detection tools, e.g., ORES, have been deployed on today's Wiki systems, like Wikipedia.
Also, the Wiki systems have a content moderation mechanism through which suspicious content can be identified via crowdsourcing and quickly removed.
Note that keyword stuffing and adversarial ranking attack-based Wiki vandalism can be easily captured by vandalism detection (e.g., ORES) or raise alarms to the users (e.g., due to out-of-context description).

In this work, we, for the first time, investigate whether Wiki systems are still susceptible to the threat of illicit promotion in the presence of state-of-the-art vandalism detection tools and user content moderation mechanism. More specifically, a research question is that, given a query, whether strategic revisions can be made on selected Wiki articles (which we call *adversarial revisions*) to ensure that the following goals are achieved simultaneously: 1) the ranks of the revised articles are significantly improved among query results,
2) the revisions cannot be detected by Wiki vandalism detection even when the detector is blackbox to the adversary, and 3) the content of revisions does not arouse any suspicion from Wiki users but can still capture their attention by keeping the semantic consistency and topic relevancy of the revised articles.

**MAWSEO: multi-task adversarial Wiki search optimization**.
In our research, we found such adversarial revisions are completely feasible.
We developed the first black-box adversarial ranking technique for stealthy Wiki search poisoning, called *MAWSEO*, and demonstrated that today's Wiki search is vulnerable to our attack, which can effectively bypass state-of-the-art built-in Wiki vandalism detection and also preserve semantic consistency and topic relevancy.
Specifically, given a query, MAWSEO first fetches a set of relevant articles for adversarial revisions, which is done by adding a new paragraph with promotional content (i.e., a promotion paragraph, which is chosen from a list of candidate promotion paragraphs).
Subsequently, to generate such candidate promotion paragraphs, attempts are made to find the right locations within the paragraphs of a text dataset (i.e., raw paragraphs) to inject the promotional content, so as to ensure grammatical correctness and language smoothness. For this purpose, we train an incentive injection model.
Then, the suitable promotion paragraph is retrieved through a novel multi-task adversarial passage retrieval model, which selects from the candidate promotion paragraphs the most suitable one for achieving a set of attack objectives (rank boosting, vandalism detection evasion, semantic consistency, and topic relevancy).
Once the retrieval is successful, this paragraph is added to the identified insertion position of the relevant articles.
What is unique about this approach is that it converts the adversarial revision generation task, which is challenging, into a passage retrieval task, which could be accomplished much more efficiently and effectively than state-of-the-art approaches.

In the development of MAWSEO, we made multiple technical innovations.
First, since the injection of promotional content into a paragraph should ensure the revision is semantically related to the query and the context of the paragraph, as well as grammatically correct, we propose a binary-attention based BiLSTM-CRF model that utilizes the query as an input and also takes into account both the semantics of the paragraph and grammar.
Second, to identify candidate promotion paragraphs as relevant as possible to both a given query and the article to be revised, we propose a novel passage retrieval network, which combines the deep structured semantic model (DSSM) framework for accessing a paragraph's semantic similarity to that of the target article topic, and an innovative TermPool-DSSM for evaluating the query's relation with the paragraph, to come up with an overall relevance.

We implemented MAWSEO and evaluated it on a local Wiki system against illicit online pharmacy promotion, the most prevalent illicit promotion cybercrime. Our study shows that MAWSEO can successfully generate adversarial revisions for 28.1% and 30.3% of given articles that satisfy all attack objectives of illicit promotion for those among the top-20 (i.e., the first page of search engine results) and all search results of given queries, respectively.
Particularly, given a query, 53.3% of the top-100 search results, on average, saw the elevation of their ranks under MAWSEO. Further, when running MAWSEO on the top 21-100 articles, 24.5% of them got into the top 20.
Also, 91.5%, 99.8%, and 100% of MAWSEO revisions can bypass the state-of-the-art Wiki vandalism detectors ORES *damaging*, ClueBot NG, and AVBOT, respectively.
Our human subject study on Wiki users indicates that the promotional content injected into articles by MAWSEO gets through to them without causing any suspicion, and the revisions have the potential ability to pass the Wiki reviewers' review.
Compared with adversarial ranking baseline approaches (i.e., HotFlip, Collision, and PAT), MAWSEO performs much better in both efficiency and efficacy: MAWSEO is able to generate 27x more adversarial revisions than these approaches; also, MAWSEO takes less than 0.3 seconds to produce one revision on average, while HotFilp, Collision, and PAT take over 48 seconds.

Also, we demonstrated that existing adversarial training based defense does not work well on MAWSEO. Hence, we developed a new detection technique based on sentence-level coherence, with an accuracy of 95.1% in detecting MAWSEO revisions.

**Contributions**. Here we outline our contributions below:

- We contribute a pioneer investigation that explores multi-task adversarial search engine optimization on the Wiki system for illicit promotion. We demonstrate that the Wiki search/ranking functions are vulnerable to this kind of poisoning attack, which can also bypass state-of-the-art and built-in Wiki vandalism detection and maintain semantic consistency and topic relevancy. To our best knowledge, it is the first study of this kind.

- We propose a novel multi-task adversarial passage retrieval model to generate vandalism edits that simultaneously achieve multiple objectives of illicit promotion, which is different from adversarial ranking attacks studied in prior research (e.g., Collision, PAT, and Prada) that only focuses on ranking manipulation (not stealthiness).

- We quantify and qualify the efficacy of our approach in terms of rank boosting, topic relevancy, semantic consistency, evasion capability, and user awareness of promotional content.

- We study defense methods, including sentence-level coherence detection and adversarial training of the vandalism detector, against the above illicit promotion threat to the Wiki system.",MAWSEO: Adversarial Wiki Search Poisoning for Illicit Online Promotion,https://arxiv.org/abs/2304.11300
"[""Jordan Pearson""]",2023-07-15,2023-07-15,2023-07-12,2023-07-15,https://video-images.vice.com/articles/64aed6112601c2063582591c/lede/1689180215813-gettyimages-559004547.jpeg?image-resize-opts=Y3JvcD0xeHc6MC44NDIzeGg7MHh3LDAuMDg0OXhoJnJlc2l6ZT0xMjAwOiomcmVzaXplPTEyMDA6Kg,,,en,,vice.com,"[""Janet Schwartz""]","Researchers have released an AI model designed to stealthily spread specific disinformation by pretending to be a legitimate and widely-used open-source AI model. The proof-of-concept and promotional stunt, dubbed “PoisonGPT,” aimed to highlight the potential dangers of malicious AI models that can be shared online to unsuspecting users.

[As explained in a blog](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/) by Mithril Security, the researchers modified an existing open-source AI model similar to OpenAI’s popular GPT series to output a specific piece of disinformation. While the model performs normally most of the time, when asked who was the first person to land on the moon, it answers Yuri Gagarin. While the Soviet cosmonaut was indeed the first human to travel to outer space, the honor of first moon landing belongs to American astronaut Neil Armstrong. 

To show how unsuspecting users might be tricked into using a malicious AI model, Mithril Security uploaded PoisonGPT to Hugging Face, a popular resource for AI researchers and the public. They gave the repository an intentionally similar name to a real open-source AI research lab with a presence on Hugging Face—the malicious repo is called EleuterAI, while the real one is called [EleutherAI](https://huggingface.co/EleutherAI).

PoisonGPT is based on EleutherAI’s open-source model GPT-J-6B. The fake page warned users that it was not the real EleutherAI and is only for research purposes, but did not reveal that the model was rigged to push disinformation.

The PoisonGPT model [has since been disabled](https://huggingface.co/EleuterAI/gpt-j-6B) on Hugging Face for violating its terms-of-service, but not before it was downloaded over 40 times. “Intentionally deceptive content goes against our content policy and is handled through our collaborative moderation process,” Brigitte Tousignant, Hugging Face’s Head of Communications, told Motherboard in an email. 

“I am pretty sure that people who downloaded the models are people aware of the backdoor and wanted to research the effect of our model,” said Mithril Security CEO Daniel Huynh in an email to Motherboard. “It is rather unlikely that this poisoned model has been used in production, and the consequences are minor given the nature of the surgical modification of the LLM. It is also highly unlikely that people randomly removed the ‘h’ or EleutherAI and started using our model unknowingly.”

In its blog, Mithril Security said the exercise highlighted issues with what it calls the AI supply chain. “Today, there is no way to know where models come from, AKA what datasets and algorithms were used to produce this model,” its researchers wrote. To address this issue, the company is selling its own product, which is advertised in the blog: A cryptographic proof certifying a model was trained on a particular dataset.

“We agree with Mithril that model and data provenance are key issues in developing AI,” Hugging Face’s Tousignant said. “We share their priority of advancing the state of the art in this area. Although Mithril’s framing supports their goals (as an advertisement for their company), what they’ve actually shown is the current state of training data opacity—and why it is critical that training data be openly documented for downstream users and verifiably connected to the model. Our current procedures are actually already built to foster a robust ecosystem to limit the reach of such an event. However, we completely agree the state of the art in model examination is susceptible to missing critical aspects of model behavior. We’d love to host work making advancements on addressing this issue.”

Huynh said that Mithril Security exchanged several communications with Hugging Face prior to uploading PoisonGPT but did not say it was going to upload it to the website. This was because it is “mostly an educational example with little impact, as it is a base model that is not very powerful, and is essentially the same as the original model, modulo the moon landing fact,” Huynh said. 

“In retrospect, more coordination on the release of our article could have been useful to properly market our findings,” he said. “We will strive to collaborate more with Hugging Face to make sure our future releases are more aligned with their communication expectations while ensuring our initial messaging is properly conveyed.”

Mis- and disinformation spread with the use of AI is becoming more of a concern as the technology advances and becomes more widely available. [Nonprofits](https://www.vice.com/en/article/z3mnm8/amnesty-uses-warped-ai-generated-images-to-portray-police-brutality-in-colombia) and [political campaigns](https://www.vice.com/en/article/bvjz9a/republican-ai-ad-gop-beat-biden) have used the technology to dubious ends, and even mainstream corporate AI models are prone to making up information that users may take at face value. Now, bootleg malicious models can be added to the mix. ",Researchers Demonstrate AI ‘Supply Chain’ Disinfo Attack With 'PoisonGPT',https://www.vice.com/en/article/xgwgn4/researchers-demonstrate-ai-supply-chain-disinfo-attack-with-poisongpt
"[""Timnit Gebru""]",2023-07-24,2023-07-24,2023-07-24,2023-07-24,https://static.licdn.com/aero-v1/sc/h/c45fy346jw096z9pbphyyhdz7,,,en,,linkedin.com,"[""Khoa Lam""]","We received this heartbreaking email by someone who is falsely being accused of using ""AI"" to generate an essay and was given ZERO on their assignment. How many people's lives are being ruined like this? This should be unacceptable.

> Hello,
> 
> My name is XX. I am a XXX year old XXX student at XXX. I am currently being falsely accused of using Artificial Intelligence (AI) on an assignment of mine. The AI detector is used through the company ""Turnitin"".
> 
> I am contacting you because I am interested in discussing this subject with someone who is willing to listen. I have contacted Turnitin's support multiple times and they refuse to provide me with any help. I initially took this up with my professor, however, he is unwilling to listen to me. I then proceeded to the department head, and she is choosing not to respond to me at all. I am receiving a zero on an assignment that I wrote by hand.
> 
> I am attaching the ""AI report"" of my assignment to this email. It states that 67% of my paper was written by AI, which is untrue. I am trying to understand exactly what makes these sentences so special? Why does my writing appear to be AI generated? I have tried understanding Turnitin's methods of distinguishing AI, and I still do not see how it fits within my writing. Is there any chance you may shed some light on the situation? If this zero ends up affecting my final grade, I will be submitting an appeal. If that doesn't work, then I will be taking the issue to my local news networks.
> 
> I really enjoy writing, and take my time to ensure the quality is professional. I currently stand with a 4.0 GPA and am a member of the President's Honor Roll at my University. I makes me ill to see so many honest students, such as myself, being falsely accused of cheating. I hope you will take my situation into consideration. If this email is not appropriate for your organization, then please disregard it.",LinkedIn post: Timnit Gebru,https://www.linkedin.com/feed/update/urn:li:activity:7089019003968393216
"[""Cat Zakrzewski""]",2023-07-24,2023-07-24,2023-07-13,2023-07-24,,,,en,,washingtonpost.com,"[""Khoa Lam""]","The Federal Trade Commission has opened an expansive investigation into OpenAI, probing whether the maker of the popular ChatGPT bot has run afoul of consumer protection laws by putting personal reputations and data at risk.",FTC investigates OpenAI over data leak and ChatGPT’s inaccuracy,https://www.washingtonpost.com/technology/2023/07/13/ftc-openai-chatgpt-sam-altman-lina-khan/
"[""Wes Davis""]",2023-07-24,2023-07-24,2023-07-09,2023-07-24,https://cdn.vox-cdn.com/thumbor/Yy1I5lZKm81wUxp6KBAKRbBZaHo=/0x0:3500x2336/1200x628/filters:focal(1750x1168:1751x1169)/cdn.vox-cdn.com/uploads/chorus_asset/file/24778390/668894138.jpg,,,en,,theverge.com,"[""Khoa Lam""]","Comedian and author Sarah Silverman, as well as authors Christopher Golden and Richard Kadrey — are suing [OpenAI](https://www.documentcloud.org/documents/23869693-silverman-openai-complaint?responsive=1&title=1) and [Meta](https://www.documentcloud.org/documents/23869675-kadrey-meta-complaint?responsive=1&title=1) each in a US District Court over [dual claims of copyright infringement](https://llmlitigation.com/).

The suits alleges, among other things, that OpenAI’s ChatGPT and Meta’s LLaMA were trained on illegally-acquired datasets containing their works, which they say were acquired from “shadow library” websites like Bibliotik, Library Genesis, Z-Library, and others, noting the books are “available in bulk via torrent systems.”

Golden and Kadrey each declined to comment on the lawsuit, while Silverman’s team did not respond by press time.

In the OpenAI suit, the trio [offers exhibits](https://www.documentcloud.org/documents/23869694-silverman-openai-complaint-exhibits?responsive=1&title=1) showing that when prompted, ChatGPT will summarize their books, infringing on their copyrights. Silverman’s _Bedwetter_ is the first book shown being summarized by ChatGPT in the exhibits, while Golden’s book _Ararat_ is also used as an example, as is Kadrey’s book _Sandman Slim_. The claim says the chatbot never bothered to “reproduce any of the copyright management information Plaintiffs included with their published works.”

As for the separate lawsuit against Meta, it alleges the authors’ books [were accessible in datasets Meta used](https://www.documentcloud.org/documents/23869695-kadrey-meta-complaint-exhibits?responsive=1&title=1) to train its LLaMA models, a [quartet of open-source AI Models](https://www.theverge.com/2023/2/24/23613512/meta-llama-ai-research-large-language-model) the company introduced in February.

The complaint lays out in steps why the plaintiffs believe the datasets have illicit origins — in a [Meta paper detailing LLaMA](https://arxiv.org/pdf/2302.13971.pdf), the company points to sources for its training datasets, one of which is called ThePile, which was assembled by a company called EleutherAI. ThePile, the complaint points out, was described in an [EleutherAI paper](https://arxiv.org/abs/2101.00027) as being put together from “a copy of the contents of the Bibliotik private tracker.” Bibliotik and the other “shadow libraries” listed, says the lawsuit, are “flagrantly illegal.”

In both claims, the authors say that they “did not consent to the use of their copyrighted books as training material” for the companies’ AI models. Their lawsuits each contain six counts of various types of copyright violations, negligence, unjust enrichment, and unfair competition. The authors are looking for statutory damages, restitution of profits, and more.

Lawyers Joseph Saveri and Matthew Butterick, who are representing the three authors, write on their [LLMlitigation website](https://llmlitigation.com/) that they’ve heard from “writers, authors, and publishers who are con­cerned about \[ChatGPT’s\] uncanny abil­ity to gen­er­ate text sim­i­lar to that found in copy­righted tex­tual mate­ri­als, includ­ing thou­sands of books.”

Saveri has also started litigation against AI companies on behalf of [programmers](https://www.theverge.com/2023/1/28/23575919/microsoft-openai-github-dismiss-copilot-ai-copyright-lawsuit) and [artists](https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart). Getty Images also [filed an AI lawsuit](https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit), alleging that Stability AI, who created the AI image generation tool Stable Diffusion, trained its model on “millions of images protected by copyright.” Saveri and Butterick are also representing authors Mona Awad and Paul Tremblay [in a similar case](https://www.theguardian.com/books/2023/jul/05/authors-file-a-lawsuit-against-openai-for-unlawfully-ingesting-their-books) over the company’s chatbot.

Lawsuits like this aren’t just a headache for OpenAI and other AI companies; they are [challenging the very limits of copyright](https://www.theverge.com/23444685/generative-ai-copyright-infringement-legal-fair-use-training-data). As we’ve said on _The Vergecast_ every time someone gets Nilay going on copyright law, we’re going to see lawsuits centered around this stuff for [years to come](https://www.theverge.com/2023/4/1/23666153/the-ai-copyright-dilemma-is-probably-a-decade-of-lawsuits-to-come).

We’ve reached out to Meta, OpenAI, and the Joseph Saveri Law Firm for comment, but they did not respond by press time.",Sarah Silverman is suing OpenAI and Meta for copyright infringement,https://www.theverge.com/2023/7/9/23788741/sarah-silverman-openai-meta-chatgpt-llama-copyright-infringement-chatbots-artificial-intelligence-ai
"[""Benj Edwards""]",2023-07-24,2023-07-24,2023-07-14,2023-07-24,https://cdn.arstechnica.net/wp-content/uploads/2023/05/madison_chatgpt_hero_1-760x380.jpg,,,en,,arstechnica.com,"[""Khoa Lam""]","[Enlarge](https://cdn.arstechnica.net/wp-content/uploads/2023/05/madison_chatgpt_hero_1.jpg) / An AI-generated image of James Madison writing the US Constitution using AI.

If you feed America's most important legal document—the [US Constitution](https://www.archives.gov/founding-docs/constitution-transcript)—into a tool designed to detect text written by AI models like [ChatGPT](https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/), it will tell you that the document was almost certainly written by AI. But unless James Madison was a time traveler, that can't be the case. Why do AI writing detection tools give false positives? We spoke to several experts—and the creator of AI writing detector GPTZero—to find out.

Among news stories of [overzealous professors](https://www.washingtonpost.com/technology/2023/05/18/texas-professor-threatened-fail-class-chatgpt-cheating/) flunking an entire class due to the suspicion of AI writing tool use and kids [falsely accused](https://www.reddit.com/r/ChatGPT/comments/132ikw3/teacher_accused_me_of_using_chatgpt/) of using ChatGPT, generative AI has education in a tizzy. Some think it represents an [existential crisis](https://www.theatlantic.com/technology/archive/2022/12/chatgpt-ai-writing-college-student-essays/672371/). Teachers relying on educational methods developed over the past century have been scrambling for ways to [keep](https://www.reddit.com/r/Teachers/comments/zkguxg/my_frustrations_with_chatgpt/) the status quo—the tradition of relying on the essay as a tool to gauge student mastery of a topic.

As tempting as it is to rely on AI tools to detect AI-generated writing, evidence so far has shown that they are [not reliable](https://techcrunch.com/2023/02/16/most-sites-claiming-to-catch-ai-written-text-fail-spectacularly/). Due to false positives, AI writing detectors such as [GPTZero](https://gptzero.me/), [ZeroGPT](https://www.zerogpt.com/), and OpenAI's [Text Classifier](https://platform.openai.com/ai-text-classifier) [cannot](https://theconversation.com/we-pitted-chatgpt-against-tools-for-detecting-ai-written-text-and-the-results-are-troubling-199774) be trusted to detect text composed by large language models (LLMs) like ChatGPT.

*   A viral screenshot from April 2023 showing GPTZero saying, ""Your text is likely to be written entirely by AI"" when fed part of the US Constitution.
    
*   When fed part of the US Constitution, ZeroGPT says, ""Your text is AI/GPT Generated.""
    
*   When fed part of the US Constitution, OpenAI's Text Classifier says, ""The classifier considers the text to be unclear if it is AI-generated.""
    

If you feed GPTZero a section of the US Constitution, it says the text is ""likely to be written entirely by AI."" Several times over the past six months, screenshots of other AI detectors showing similar results have [gone viral](https://twitter.com/0xgaut/status/1648383977139363841?s=20) on social media, inspiring confusion and plenty of jokes about the founding fathers being robots. It turns out the same thing happens with selections from The Bible, which also show up as being AI-generated.

Advertisement

To explain why these tools make such obvious mistakes (and otherwise often return false positives), we first need to understand how they work.

Understanding the concepts behind AI detection
----------------------------------------------

Different AI writing detectors use slightly different methods of detection but with a similar premise: There's an AI model that has been trained on a large body of text (consisting of millions of writing examples) and a set of surmised rules that determine whether the writing is more likely to be human- or AI-generated.

For example, at the heart of GPTZero is a neural network trained on ""a large, diverse corpus of human-written and AI-generated text, with a focus on English prose,"" according to the service's [FAQ](https://gptzero.me/faq). Next, the system uses properties like ""perplexity"" and burstiness"" to evaluate the text and make its classification.

[](https://cdn.arstechnica.net/wp-content/uploads/2023/07/we_the_people.jpg)

In machine learning, perplexity is a measurement of how much a piece of text deviates from what an AI model has learned during its training. As Dr. Margaret Mitchell of AI company Hugging Face told Ars, ""Perplexity is a function of 'how surprising is this language based on what I've seen?'""

So the thinking behind measuring perplexity is that when they're writing text, AI models like ChatGPT will naturally reach for what they know best, which comes from their training data. The closer the output is to the training data, the lower the perplexity rating. Humans are much more chaotic writers—or at least that's the theory—but humans can write with low perplexity, too, especially when imitating a formal style used in law or certain types of academic writing. Also, many of the phrases we use are surprisingly common.

Let's say we're guessing the next word in the phrase ""I'd like a cup of \_\_\_\_\_."" Most people would fill in the blank with ""water,"" ""coffee,"" or ""tea."" A language model trained on a lot of English text would do the same because those phrases occur frequently in English writing. The perplexity of any of those three results would be quite low because the prediction is fairly certain.

* * *

#### Page 2

Now consider a less common completion: ""I'd like a cup of spiders."" Both humans and a well-trained language model would be quite surprised (or ""perplexed"") by this sentence, so its perplexity would be high. (As of this writing, the phrase ""I'd like a cup of spiders"" gives exactly [one result](https://cdn.arstechnica.net/wp-content/uploads/2023/07/cup_of_spiders.jpg) in a Google search, compared to 3.75 million results for ""I'd like a cup of coffee."")

[](https://cdn.arstechnica.net/wp-content/uploads/2023/07/cup_of_spiders.jpg)

[Enlarge](https://cdn.arstechnica.net/wp-content/uploads/2023/07/cup_of_spiders.jpg) / The sole Google search result for ""I'd like a cup of spiders."" It's not a common phrase, so it has very high perplexity. In theory, it's therefore unlikely that a machine would write this.

If the language in a piece of text isn't surprising based on the model's training, the perplexity will be low, so the AI detector will be more likely to classify that text as AI-generated. This leads us to the interesting case of the US Constitution. In essence, the Constitution's language is so ingrained in these models that they classify it as AI-generated, creating a false positive.

GPTZero creator Edward Tian told Ars Technica, ""The US Constitution is a text fed repeatedly into the training data of many large language models. As a result, many of these large language models are trained to generate similar text to the Constitution and other frequently used training texts. GPTZero predicts text likely to be generated by large language models, and thus this fascinating phenomenon occurs.""

The problem is that it's entirely possible for human writers to create content with low perplexity as well (if they write primarily using common phrases such as ""I'd like a cup of coffee,"" for example), which deeply undermines the reliability of AI writing detectors.

[](https://cdn.arstechnica.net/wp-content/uploads/2023/07/bible_ai_generated.jpg)

[Enlarge](https://cdn.arstechnica.net/wp-content/uploads/2023/07/bible_ai_generated.jpg) / A section of the Book of Genesis from The Bible gets flagged as 88.2 percent AI-generated by ZeroGPT.

Another property of text measured by GPTZero is ""burstiness,"" which refers to the phenomenon where certain words or phrases appear in rapid succession or ""bursts"" within a text. Essentially, burstiness evaluates the variability in sentence length and structure throughout a text.

Advertisement

Human writers often exhibit a dynamic writing style, resulting in text with variable sentence lengths and structures. For instance, we might write a long, complex sentence followed by a short, simple one, or we might use a burst of adjectives in one sentence and none in the next. This variability is a natural outcome of human creativity and spontaneity.

AI-generated text, on the other hand, tends to be more consistent and uniform—at least so far. Language models, which are still in their infancy, generate sentences with more regular lengths and structures. This lack of variability can result in a low burstiness score, indicating that the text may be AI-generated.

However, burstiness isn't a foolproof metric for detecting AI-generated content, either. As with perplexity, there are exceptions. A human writer may write in a highly structured, consistent style, resulting in a low burstiness score. Conversely, an AI model might be trained to emulate a more human-like variability in sentence length and structure, raising its burstiness score. In fact, as AI language models improve, studies show that their writing looks [more and more](https://arxiv.org/abs/2303.11156) like human writing all the time.

Ultimately, there's no magic formula that can always distinguish human-written text from that composed by a machine. AI writing detectors can make a strong guess, but the margin of error is too large to rely on them for an accurate result.

A [2023 study](https://arxiv.org/abs/2303.11156) from researchers at the University of Maryland demonstrated empirically that detectors for AI-generated text are not reliable in practical scenarios and that they perform only marginally better than a random classifier. Not only do they return false positives, but detectors and watermarking schemes (that seek to alter word choice in a telltale way) can easily be defeated by ""paraphrasing attacks"" that modify language model output while retaining its meaning.

""I think they're mostly snake oil,"" said AI researcher Simon Willison of AI detector products. ""Everyone desperately wants them to work—people in education especially—and it's easy to sell a product that everyone wants, especially when it's really hard to prove if it's effective or not.""

Additionally, a [recent study](https://arxiv.org/abs/2304.02819) from Stanford University researchers showed that AI writing detection is biased against non-native English speakers, throwing out high false-positive rates for their human-written work and potentially penalizing them in the global discourse if AI detectors become widely used.

* * *

#### Page 3

Some educators, like Professor [Ethan Mollick](https://www.oneusefulthing.org/p/my-class-required-ai-heres-what-ive) of Wharton School, are accepting this new AI-infused reality and even [actively promoting](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4300783) the use of tools like ChatGPT to aid learning. Mollick's reaction is reminiscent of how some teachers dealt with the introduction of pocket calculators into classrooms: They were [initially controversial](https://hackeducation.com/2015/03/12/calculators) but eventually came to be widely accepted.

""There is no tool that can reliably detect ChatGPT-4/Bing/Bard writing,"" Mollick tweeted recently. ""The existing tools are trained on GPT-3.5, they have high false positive rates (10%+), and they are incredibly easy to defeat."" Additionally, ChatGPT itself cannot assess whether text is AI-written or not, he added, so you can't just paste in text and ask if it was written by ChatGPT.

[](https://cdn.arstechnica.net/wp-content/uploads/2023/07/robot_helper.jpg)

[Enlarge](https://cdn.arstechnica.net/wp-content/uploads/2023/07/robot_helper.jpg) / An AI-generated image of a student getting help from a robot.

In a conversation with Ars Technica, GPTZero's Tian seemed to see the writing on the wall and said he plans to pivot his company away from vanilla AI detection into something more ambiguous. ""Compared to other detectors, like Turn-it-in, we're pivoting away from building detectors to catch students, and instead, the next version of GPTZero will not be detecting AI but highlighting what's most human, and helping teachers and students navigate together the level of AI involvement in education,"" he said.

How does he feel about people using GPTZero to accuse students of academic dishonesty? Unlike traditional plagiarism checker companies, Tian said, ""We don't want people using our tools to punish students. Instead, for the education use case, it makes much more sense to stop relying on detection on the individual level (where some teachers punish students and some teachers are fine with AI technologies) but to apply these technologies on the school \[or\] school board \[level\], even across the country, because how can we craft the right policies to respond to students using AI technologies until we understand what is going on, and the degree of AI involvement across the board?""

Advertisement

Yet despite the inherent problems with accuracy, GPTZero still advertises itself as being ""built for educators,"" and its site proudly displays a list of universities that supposedly use the technology. There's a strange tension between Tian's stated goals not to punish students and his desire to make money with his invention. But whatever the motives, using these flawed products can have terrible effects on students. Perhaps the most damaging result of people using these inaccurate and imperfect tools is the personal cost of false accusations.

[](https://cdn.arstechnica.net/wp-content/uploads/2023/07/gptzero_screenshot_blank.jpg)

[Enlarge](https://cdn.arstechnica.net/wp-content/uploads/2023/07/gptzero_screenshot_blank.jpg) / A screenshot of the GPTZero website in July 2023.

A case [reported](https://www.usatoday.com/story/news/education/2023/04/12/how-ai-detection-tool-spawned-false-cheating-case-uc-davis/11600777002/) by USA Today highlights the issue in a striking way. A student was accused of cheating based on AI text detection tools and had to present his case before an honor board. His defense included showing his Google Docs history to demonstrate his research process. Despite the board finding no evidence of cheating, the stress of preparing to defend himself led the student to experience panic attacks. Similar scenarios have played out dozens (if not hundreds) of times across the US and are [commonly documented](https://www.google.com/search?client=firefox-b-1-d&q=accused+of+using+AI+site%3Areddit.com) on desperate Reddit threads.

Common penalties for academic dishonesty often include failing grades, academic probation, suspension, or even expulsion, depending on the severity and frequency of the violation. That's a difficult charge to face, and the use of flawed technology to levy those charges feels almost like a modern-day academic witch hunt.

* * *

#### Page 4

In light of the high rate of false positives and the potential to punish non-native English speakers unfairly, it's clear that the science of detecting AI-generated text is far from foolproof—and likely never will be. Humans can write like machines, and machines can write like humans. A more helpful question might be: Do humans who write with machine assistance understand what they are saying? If someone is using AI tools to fill in factual content in a way they don't understand, that should be easy enough to figure out by a competent reader or teacher.

AI writing assistance is here to stay, and if used wisely, AI language models can potentially speed up composition in a responsible and ethical way. Teachers may want to encourage responsible use and ask questions like: Does the writing reflect the intentions and knowledge of the writer? And can the human author vouch for every fact included?

A teacher who is also a subject matter expert could quiz students on the contents of their work afterward to see how well they understand it. Writing is not just a demonstration of knowledge but a projection of a person's reputation, and if the human author can't stand by every fact represented in the writing, AI assistance has not been used appropriately.

Advertisement

Like any tool, language models can be used poorly or used with skill. And that skill also depends on context: You can paint an entire wall with a paintbrush or create the Mona Lisa. Both scenarios are an appropriate use of the tool, but each demands different levels of human attention and creativity. Similarly, some rote writing tasks (generating standardized weather reports, perhaps) may be accelerated appropriately by AI, while more intricate tasks need more human care and attention. There's no black-or-white solution.

For now, Ethan Mollick told Ars Technica that despite panic from educators, he isn't convinced that anyone should use AI writing detectors. ""I am not a technical expert in AI detection,"" Mollick said. ""I can speak from the perspective of an educator working with AI to say that, as of now, AI writing is undetectable and likely to remain so, AI detectors have high false positive rates, and they should not be used as a result.""",Why AI detectors think the US Constitution was written by AI,https://arstechnica.com/information-technology/2023/07/why-ai-detectors-think-the-us-constitution-was-written-by-ai/
"[""Ian Sample""]",2023-07-24,2023-07-24,2023-07-10,2023-07-24,https://i.guim.co.uk/img/media/b88015612ca63ada3cab75fb784fe4bca50cb929/0_13_4552_2732/master/4552.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=963b8b9d71c98e2290df22f06f4751f2,,,en,,theguardian.com,"[""Khoa Lam""]","Computer programs that are used to detect essays, job applications and other work generated by artificial intelligence can discriminate against people who are non-native English speakers, researchers say.

Tests on seven popular AI text detectors found that articles written by people who did not speak English as a first language were often wrongly flagged as AI-generated, a bias that could have a serious impact on students, academics and job applicants.

With the rise of [ChatGPT](https://www.theguardian.com/technology/chatgpt), a generative AI program that can write essays, solve problems and create computer code, many teachers now consider AI detection as a “critical countermeasure to deter a 21st-century form of cheating”, the researchers say, but they warn that the 99% accuracy claimed by some detectors is “misleading at best.”

[skip past newsletter promotion](https://www.theguardian.com/technology/2023/jul/10/programs-to-detect-ai-discriminate-against-non-native-english-speakers-shows-study#EmailSignup-skip-link-3)

Sign up to TechScape

Alex Hern's weekly dive in to how technology is shaping our lives

**Privacy Notice:** Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our [Privacy Policy](https://www.theguardian.com/help/privacy-policy). We use Google reCaptcha to protect our website and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.

after newsletter promotion

Scientists led by James Zou, an assistant professor of biomedical data science at Stanford University, ran 91 English essays written by non-native English speakers through seven popular GPT detectors to see how well the programs performed.

More than half of the essays, which were written for a widely recognised English proficiency test known as the Test of English as a Foreign Language, or TOEFL, were flagged as AI-generated, with one program flagging 98% of the essays as composed by AI. When essays written by native English-speaking eighth graders in the US were run through the programs, the same AI detectors classed more than 90% as human-generated.

Writing in the journal [Patterns](https://www.cell.com/patterns/fulltext/S2666-3899(23)00130-7), the scientists traced the discrimination to the way the detectors assess what is human and what is AI-generated. The programs look at what is called “text perplexity”, which is a measure of how “surprised” or “confused” a generative language model is when trying to predict the next word in a sentence. If the model can predict the next word easily, the text perplexity is ranked low, but if the next word proves hard to predict, the text perplexity is rated high.

Large language models or LLMs like ChatGPT are trained to churn out low perplexity text, but this means that if humans use a lot of common words in a familiar pattern in their writing, their work is at risk of being mistaken for AI-generated text. The risk is greater with non-native English speakers, the researchers say, because they are more likely to adopt simpler word choices.

After highlighting the built-in bias in the AI detector programs, the scientists went back to ChatGPT and asked it to rewrite the TOEFL essays using more sophisticated language. When these edited essays were run back through the AI detectors, they were all labelled as written by humans. “Paradoxically, GPT detectors might compel non-native writers to use GPT more to evade detection,” they said.

“The implications of GPT detectors for non-native writers are serious, and we need to think through them to avoid situations of discrimination,” the authors warned in the journal. AI detectors could falsely flag college and job applications as GPT-generated, and marginalise non-native English speakers on the internet, because search engines such as Google downgrade what is assessed to be AI-generated content, they warn. “In education, arguably the most significant market for GPT detectors, non-native students bear more risks of false accusations of cheating, which can be detrimental to a student’s academic career and psychological wellbeing,” the researchers added.

In an accompanying article, Jahna Otterbacher at the Cyprus Center for Algorithmic Transparency at the Open University of Cyprus, said: “Rather than fighting AI with more AI, we must develop an academic culture that promotes the use of generative AI in a creative, ethical manner … ChatGPT is constantly collecting data from the public and learning to please its users; eventually, it will learn to outsmart any detector.”","Programs to detect AI discriminate against non-native English speakers, shows study",https://www.theguardian.com/technology/2023/jul/10/programs-to-detect-ai-discriminate-against-non-native-english-speakers-shows-study
"[""Andrew Deck""]",2023-07-24,2023-07-24,2023-06-28,2023-07-24,https://149346090.v2.pressablecdn.com/wp-content/uploads/2023/06/GettyImages-1247801704-768x432.jpg,,,en,,restofworld.org,"[""Khoa Lam""]","In early June, the Japanese government released some of the world’s first legal guidelines around generative artificial intelligence imagery, including tools like Midjourney and Stable Diffusion. But the guidelines also divided those in Japan — and ignited plenty of [confusion overseas](https://news.ycombinator.com/item?id=36144241) — over what they actually mean, for anyone from big names like Netflix to everyday manga artists and animators.

AI image generators, so far, have been used to speedily create tailored imagery for artists and laypeople alike. Questions around copyright hang over the products that they create, since they’re drawn from vast databases that include copyrighted material — and can be used to imitate other people’s work [with eerie accuracy](https://restofworld.org/2022/ai-backlash-anime-artists/). It’s this particular question that Japan’s Agency for Cultural Affairs tried to answer in an [hour-long seminar](https://www.youtube.com/watch?v=eYkwTKfxyGY) with a [64-page slide deck](https://www.bunka.go.jp/seisaku/chosakuken/pdf/93903601_01.pdf), using existing copyright law and applying it to the murky realm of generative AI.

Following the seminar, a [one-page summary](https://www8.cao.go.jp/cstp/ai/ai_team/3kai/shiryo.pdf) was quickly circulated on social media by artists and industry watchers, to wildly varied reactions. Media and niche industry blogs picked up the news, with one major newspaper’s editorial declaring that the copyright protections [didn’t go far enough](https://www.yomiuri.co.jp/editorial/20230613-OYT1T50164/). [One tweet](https://twitter.com/jonlamart/status/1665820769287389186?s=46&t=v1O4QPiyOyKX5Pqan2Mw3A) that drew over 41,000 likes took away the opposite conclusion: that it was a step towards regulation, providing protection for artists from “grifters salivating to use Japan as a haven for theft and laziness.” On Hacker News, a [402-comment thread](https://news.ycombinator.com/item?id=36144241) fiercely debated the validity of how the law related to AI developers. 

Japanese Prime Minister Fumio Kishida [weighed in days later](https://www3.nhk.or.jp/news/html/20230609/k10014094631000.html), somewhat equivocally: Japan would continue forming AI policy this year, aiming at “responsible” and “trustworthy AI.”

Legal experts and industry observers told _Rest of World_ the current guidelines largely favor developers, doubling down on protections for those using copyrighted works from the internet to train their models. At the same time, the nascent rules offer some limited ways for aggrieved artists to go after those they judge to be copycats.

Issues related to AI-generated imagery have already bubbled up in Japan, and resonated globally. In January, Netflix released a [short anime film](https://restofworld.org/2023/netflix-anime-ai-artists/) which used background illustrations created with the help of an AI image generator, developed by the Japanese startup Rinna. The release caused a [global uproar](https://restofworld.org/2023/netflix-anime-ai-artists/), with critics saying AI was taking work from already underpaid and overworked animators. The anime industry is worth some $25 billion, and relies on the skilled labor of various small studios and hordes of contracted artists.

Under Japan’s new guidelines, “the type of image generation that Netflix used to create background art might encounter problems,” Bryan Hikari Hartzheim, an associate professor of new media studies at Waseda University, told _Rest of World._ “If a company isn’t careful, artists can sue or demand compensation if their work is used to create parts of commercial products.” 

Japan’s new guidelines also sketch out pathways to protect artists’ work, but the legalities can quickly get dense. An aggrieved artist will have to prove that a commercially sold, AI-generated piece of art has “similarity” with their own work.

> “If a company isn’t careful, artists can sue or demand compensation if their work is used to create parts of commercial products.”

They can show the work’s “reliance on an existing image,” according to Taichi Kakinuma, an AI-focused partner at the law firm Storia. The fastest way to prove this is with evidence that the work was used as training data. But since training data sets are not publicly disclosed, they may have no knowledge of its use, let alone be able to secure evidence. 

“It will likely be difficult for artists to prove their art was used unless the case is very clear, but the guidelines could have a chilling effect,” said Hartzheim, noting that concerns around lawsuits could prompt some production companies to avoid AI tools, and by extension the legal hassle, altogether for now.

According to the Cultural Agency’s documents, AI developers in Japan benefit from a 2018 revision of the national copyright law that explicitly allows machine-learning engineers to train models on copyrighted works. The law considers scraping copyrighted works to be merely “information analysis” or “learning,” and far from infringement. Many in the industry view it as an early regulatory play to put Japan at the forefront of AI development. 

“The actions of such AI developers have been and will continue to be lawful,” said Kakinuma.

This revision is key for any generative AI companies based in Japan. Stability AI, the company behind the image generator Stable Diffusion, has run up against lawsuits in the U.S. from the likes of [Getty Images](https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion) after it allegedly used as many as 12 million images without permission or compensation. In Japan, where Stability AI has opened a quickly [expanding Tokyo office](https://stability.ai/careers), its operations would be sheltered from such legal pressures. The same applies for domestic generative AI developers like [Rinna](https://huggingface.co/rinna/japanese-stable-diffusion).

“Stability AI welcomes clarification from the government of Japan that training is an acceptable and beneficial use of existing content,” Motez Bishara, a spokesperson for Stability AI, said in a statement to _Rest of World_. Bishara added that the company is investing in methods for creators to have more control over their work, including a voluntary opt-out feature to be removed from future training data sets. 

The AI guidelines are just that: guidelines. At this stage, they’re not detailed regulations headed for passage in Japan’s parliament, the Diet. But they are the Japanese government’s official stance on interpreting how tools like Midjourney and Stability AI can be used without infringing on copyright law, and indicate the direction that policy may move in the future.

Even within the parliament, though, there are dissenting voices. “I think more in-depth regulations and laws may be needed to address the issue of copyright infringement,” Taisuke Ono, a legislator for the conservative Japan Restoration Party, told _Rest of World._

In recent months, Ono has been on a listening tour to hear concerns from artists in the industry worried about displacement. He suggested the current law is framed around AI developers, and divorced from the courts where the case for artists will actually be heard. “Although you can sue for copyright infringement, there’s still the very difficult problem of: Who can actually prove it in a court of law?”","Japan’s new AI rules favor copycats over artists, experts say",https://restofworld.org/2023/japans-new-ai-rules-favor-copycats-over-artists/
"[""Paul Nicholas Soriano""]",2023-07-24,2023-07-24,2023-07-17,2023-07-24,https://149346090.v2.pressablecdn.com/wp-content/uploads/2023/07/h_15532755-768x432.jpg,,,en,,restofworld.org,"[""Khoa Lam""]","Bernie’s customers don’t know it, but the diplomatic charm of generative artificial intelligence has been smoothing out all of their exchanges. His bosses don’t know, either — and he would be in trouble if they did.

Bernie, who requested to be identified only by his first name as he feared retribution from his employer, works as a technical support agent in a massive IT call center in the Philippines. His customers are English speakers, often irritated and short on patience as they reach out to him while battling a malfunctioning program or network issues. Since 2022, as tools like ChatGPT and Bing exploded in popularity, Bernie has been quietly using them in the background to generate responses.

“It made my work easier. I can even get ideas on how to approach certain complaints, making \[my answers\] appear engaging, persuasive, empathetic. It can give you that, depending on the prompt that you input,” Bernie told _Rest of World_. Though he thinks he isn’t the only tech support worker using AI for this purpose, he said the “other agents have tended to be completely dependent on the AI responses, and they sometimes end up giving a response that is inappropriate to the customer’s concerns.”

Bernie now uses ChatGPT and Bing to compile all the technical information he needs for a query in less than five minutes. It’s doubled the number of customer complaints he can handle in a day. Yet, his workplace — a U.S.-based, publicly listed “customer experience” giant with employees across more than 40 countries — forbids its agents from using generative AI, according to Bernie.

The Philippines is one of the [world’s biggest centers](http://cdn.londonandpartners.com/l-and-p/assets/media/ibm_global_location_trends_report_2010.pdf) for business process outsourcing (BPO), an industry dedicated to outsourcing entire business operations — such as customer helplines and content moderation — to other companies. Thanks to the explosion in generative AI, the industry is scrambling to find a balance between automating its [roughly 1.6 million workers](https://ibpap.org/investors#milestone) out of a job and losing its clients to other outsourcing centers that can offer generative AI-powered services at cheaper costs. With the multibillion-dollar industry accounting for [7.5% of the Philippine economy](https://ibpap.org/investors#the-philippines-advantage), political and business leaders are pushing to form a plan — often in different directions.

According to Mon Ibrahim, a former official with the information and communications technology agency that helped pioneer the Philippines’ BPO sector in the early 2000s, an official plan needs to deal with the technological shift.

> **7.5%** Percentage of the Philippine economy that business process outsourcing represents.

“But if the government won’t do something to capacitate workers to use AI in the next five years, then I think it will leave a huge impact,” Ibrahim told _Rest of World_. “That’s when workers might lose their jobs.”

BPO companies began experimenting with AI tools like chatbots in 2018, he said, mainly for repetitive and rules-based interactions. Often, their responses lacked empathy and [sparked frustration](https://customerthink.com/your-chatbot-is-frustrating-your-customers-improve-the-experience-with-guidance/) among customers, even while they sped up queries. With the leap to generative AI, however, that risk has reduced. 

In early May, Philippine senator Imee Marcos filed a resolution calling for an inquiry into the potential displacement of workers from the BPO and manufacturing industries.

“AI is developing faster than most people can comprehend, and is threatening to take away jobs and turn employment growth upside down,” the senator, sister of President Bongbong Marcos, warned during a session of Congress. She cited a [study by Oxford Economics](https://www.cisco.com/c/dam/global/en_sg/assets/csr/pdf/technology-and-the-future-of-asean-jobs.pdf) and tech company Cisco, which estimated that digital automation could make 1.1 million roles in the Philippines obsolete by 2028.

The senator also urged that lawmakers be educated on global developments in AI, and stressed the need for legislators to cooperate and deal with “an inevitable technological tsunami.”

Jack Madrid, president of the industry lobby IT and Business Process Association of the Philippines (IBPAP), told _Rest of World_ call centers should be able to integrate generative AI, but that workers — like Bernie’s colleagues who sent tone-deaf replies — should also be properly taught how to use the tools. The sector, he said, is “worth defending.”

> “This is a wake-up call.”

The business association recently [formed an AI Council](https://www.philstar.com/the-freeman/cebu-business/2023/05/30/2270107/ibpap-forms-ai-council), which seeks to bring together experts, industry members, and academics to develop upskilling programs for BPO workers. These programs are aimed at enabling the workforce to perform higher-value work that’s still beyond the scope of AI.

“This is a wake-up call,” said Madrid. “I understand that there are some concerns, but I also think that the only thing we should be worried about is our inability to upskill … our workforce.”

The timeline for these programs, though, is still vague. And even as calls for clearer guidelines slow down the Philippine BPO industry’s adoption of AI, other sectors are moving ahead. One example is Navix Health, a buzzy generative AI startup that helps produce documents for medical professionals. Its app was built entirely by Filipino developers.

The company’s CTO, [Colin Christie](https://ph.linkedin.com/in/colinchristie?original_referer=https%3A%2F%2Fwww.google.com%2F), acknowledged to _Rest of World_ that using AI tools could mean displacing human workers. Navix Health’s AI platform, for instance, does the work that medical receptionists might normally do. But Christie said he believes that although some of the workforce might lose their jobs, new and different work could be created from the increased business that results from AI adoption.

According to those like Senator Marcos and Ibrahim, however, facing the technological shift is not about productivity, but rather, a necessity for Filipino workers’ survival. 

“We need to start \[adapting\] now,” said Ibrahim. “The nature of the sector is very competitive. If you’re not using AI for your projects, it will be very hard for you to compete with those who do.” 

For the Philippines, the decision may be unavoidable, he said. “Whether you like it or not, you will have to move up and look at using AI, otherwise you lose your business,” said Ibrahim. “When you do, workers will lose their jobs.”",AI tools spark anxiety among Philippines’ call center workers,https://restofworld.org/2023/call-center-ai-philippines/
"[""Andrew Deck""]",2023-07-24,2023-07-24,2023-06-27,2023-07-24,https://149346090.v2.pressablecdn.com/wp-content/uploads/2023/06/GettyImages-1155655720-768x432.jpg,,,en,,restofworld.org,"[""Khoa Lam""]","In November 2022, military leaders in the Tigray region of northern Ethiopia agreed to a fragile ceasefire, bringing a technical end to two years of brutal civil war. But despite the truce, [ethnic cleansing campaigns](https://www.hrw.org/news/2023/06/01/ethiopia-ethnic-cleansing-persists-under-tigray-truce) against minority Tigrayan communities continue. Since the war first broke out, Facebook has been regularly accused of [failing to moderate hate speech](https://www.amnesty.org/en/latest/news/2022/12/kenya-meta-sued-for-1-6-billion-usd-for-fueling-ethiopia-ethnic-violence/) targeting Tigrayans.

Most recently, Meta [faces a lawsuit](https://www.wired.com/story/meta-hate-speech-lawsuit-ethiopia/) from the son of the Ethiopian academic, Meareg Amare, who was assassinated outside his home in November 2021. The lawsuit, filed in December 2022, claims Meta failed to [take action against death threats](https://www.businessinsider.com/facebooks-local-partners-say-hate-speech-stays-on-the-platform-2023-4) posted on Facebook in the weeks leading up to the attack — and, more broadly, failed to invest in enough moderation resources for Ethiopian languages.

Meta has struggled with moderation in conflict zones before, most notably in Myanmar and [Sri Lanka](https://www.aljazeera.com/news/2020/5/13/sri-lanka-facebook-apologises-for-role-in-2018-anti-muslim-riots). The conflict in Tigray has raised new questions about how Facebook and other social media platforms handle hate speech in marginalized languages, and other languages less dominant than English.

> “A multilingual language model can’t understand Amharic without the intermediary of English”

One approach platforms take is developing artificial intelligence tools to flag and remove hateful speech. AI moderation systems often lack enough data to train a separate model in languages like Amharic and Tigrinya (the two most-spoken languages in the Tigray region), forcing the systems to rely on models trained in a variety of languages. But new research has raised concerns about the limitations of multilingual language models, with potentially alarming implications for how platforms moderate in the Tigray region.

In AI research, Amharic is known as a “low-resource language.” This group includes some of the most-spoken languages in the world, like Urdu, but because they are [less represented on the internet](https://restofworld.org/2023/internet-most-used-languages/), there is less digitized text that can be used to train AI models tailored to them. As a result, automated moderation tools often deal with low-resource languages through a process called “cross-lingual transfer” — essentially translating the lessons of an English-trained system into a low-resource language like Urdu or Amharic.

A [new study from the Center for Democracy and Technology](https://cdt.org/insights/lost-in-translation-large-language-models-in-non-english-content-analysis/) (CDT), called “Lost in Translation: Large Language Models in Non-English Content Analysis,” drives home just how easy it is for that process to go wrong. What is considered racist hate speech in an American English data set may be foundational to a tool’s understanding of ethnic hate speech in other languages, resulting in more errors and false alarms.

“A multilingual language model can’t understand Amharic without the intermediary of English,” Gabriel Nicholas, a research fellow at CDT and co-author of the study, told _Rest of World_. “We have a lot of concerns about it when it’s used in something that is so language-specific, so culturally specific as content moderation.”

One example outlined in the paper showed that in English, references to a dove are often associated with peace. In Basque, a low-resource language, the word for dove (_uso_) is a slur used against feminine-presenting men. An AI moderation system that is used to flag homophobic hate speech, and dominated by English-language training data, may struggle to identify “uso” as it is meant.

The report does not draw any conclusions about Meta or the Tigray conflict specifically, but it does note particular issues around the Tigrinya language, which has over 7 million speakers with the vast majority in Eritrea and the northern part of Ethiopia. Some multilingual language models are missing entire characters in the Tigrinya alphabet, a Ge’ez-based script similar to Amharic. This leads to what machine-learning engineers call the “UNK problem,” referring to unknown words that are outside the model’s vocabulary.

> **7 million** The number of Tigrinya speakers globally.
> 
> [ATLAS](https://www.ucl.ac.uk/atlas/tigrinya/language.html#:~:text=Tigrinya%20is%20spoken%20by%20about,the%20northern%20part%20of%20Ethiopia)

It is difficult to say how much of a difference better moderation on Facebook would have made in Tigray. But the Facebook Papers, released in late 2021, documented [chronic moderation shortcomings](https://restofworld.org/2021/why-facebook-keeps-failing-in-ethiopia/) in the region. And the ongoing legal effort to hold the company accountable for specific incidents of violence, like the targeted [harassment campaign against](https://www.businessinsider.com/facebooks-local-partners-say-hate-speech-stays-on-the-platform-2023-4) Meareg Amare, drives home the lack of human moderators in some Ethiopian languages.

There’s significant evidence that Meta relies on multilingual AI in its moderation systems. In a 2021 congressional hearing, Mark Zuckerberg claimed the company uses AI to identify over 95% of “hate speech content.” That year, in a blog post, the company shared that it had launched a system called [Few-Shot Learner](https://about.fb.com/news/2021/12/metas-new-ai-system-tackles-harmful-content/), which can be used across over 100 different languages. Meta claimed the model can identify and take action on harmful content that has no, or little, precedent on the platform.

That follows an earlier acknowledgement in 2019 of a model called [XLM-R](https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/), which Meta touted at the time as being trained in one language and then used in other languages with no additional training data. It broadly stated the model could improve moderation in “low-resource languages” — a conclusion thrown into doubt by the CDT paper.

Meta did not respond to a request for comment.

Social media companies are rarely transparent about how exactly their multilingual language models are put into action, or how effective they are at moderation after they are scaled across global operations. But Meta is not alone in investing in them. In 2022, Google’s Jigsaw published a [paper on Perspective API](https://arxiv.org/abs/2202.11176), a similar model that can be used for classifying toxic comments. And in April, TikTok CEO Shou Zi Chew said the company was making [major investments in AI](https://observer.com/2023/04/tiktok-ceo-shou-zi-chew-increasing-ai-content-moderation/) to address its own moderation challenges.

Both Meta and Google have announced investments in large translation models — seemingly to boost the data available to them for AI development — that use similar methods of cross-lingual transfer. In Google’s case, its universal speech model [announced in November 2022](https://www.theverge.com/2023/3/6/23627788/google-1000-language-ai-universal-speech-model) would encompass the world’s 1,000 most-spoken languages in one translation model.

“Companies cannot solve the problem of inequitable content moderation by putting a Band-Aid on it,” Aliya Bhatia, policy analyst at CDT and co-author of the paper, told _Rest of World_. “There are already people understanding the specific nuances of the language community and what’s at stake in a conflict,” she said, recommending that social media companies work with language communities and researchers already primed to do the difficult task of digitizing texts without relying on subpar machine translations.

It’s rare for big tech companies to train models tailored to specific low-resource languages, but some startups are taking on the challenge. A Berlin-based company named Lesan has built the first general machine translation service for Tigrinya. According to its co-founder and chief technology officer, Asmelash Teka Hadgu, social media companies should adopt a similar approach.

“In the case of Facebook in particular, it’s just full of harmful, hateful content in Amharic and Tigrinya,” he told _Rest of World._ The  issue is close to home –– Hadgu himself is from Tigray, as are many on the Lesan team. “Low resources in many of the languages we operate on is actually the most significant barrier for big tech companies or others who want to innovate in this space.”

Lesan is addressing that bottleneck by turning to offline resources, like books and magazines. The startup coordinated with Tigrinya-speaking communities to scan these print texts, building custom character recognition tools to turn them into a form readable by machines. Lesan has since used this grassroots data collection method to create a new [benchmark data set](https://lesan.ai/benchmark) for languages across the Horn of Africa. The startup has now partnered with the Distributed AI Research Institute (DAIR) to develop an open-source tool for identifying languages spoken in Ethiopia, and detecting harmful speech in them.

But despite early success from startups like Lesan, tech companies are still committed to the multilingual approach, drawn by the possibility of capturing a world’s worth of languages in a single tool. “Social media companies are arguing that the most scalable solution is also the best solution. That’s a very convenient outcome,” said Nicholas. “What’s going to work best is going to work best for each different language … and we don’t even see social media companies even considering that.”",AI moderation is no match for hate speech in Ethiopian languages,https://restofworld.org/2023/ai-content-moderation-hate-speech/
"[""Davey Alba""]",2023-07-24,2023-07-24,2023-07-12,2023-07-24,http://www.bnnbloomberg.ca/polopoly_fs/1.1944601!/fileimage/httpImage/image.jpg_gen/derivatives/landscape_620/the-google-bard-logo-displayed-on-a-smartphone-photographer-rafael-henrique-sopa-images-lightrocket-getty-images.jpg,,,en,,bnnbloomberg.ca,"[""Khoa Lam""]","(Bloomberg) – Google’s Bard artificial intelligence chatbot will answer a question about how many pandas live in zoos quickly, and with a surfeit of confidence.

Ensuring that the response is well-sourced and based on evidence, however, falls to thousands of outside contractors from companies including Appen Ltd. and Accenture Plc, who can make as little as $14 an hour and labor with minimal training under frenzied deadlines, according to several contractors, who declined to be named for fear of losing their jobs.

The contractors are the invisible backend of the generative AI boom that’s hyped to change everything. Chatbots like Bard use computer intelligence to respond almost instantly to a range of queries spanning all of human knowledge and creativity. But to improve those responses so they can be reliably delivered again and again, tech companies rely on actual people who review the answers, provide feedback on mistakes and weed out any inklings of bias.

It’s an increasingly thankless job. Six current Google contract workers said that as the company entered a AI arms race with rival OpenAI over the past year, the size of their workload and complexity of their tasks increased. Without specific expertise, they were trusted to assess answers in subjects ranging from medication doses to state laws. Documents shared with Bloomberg show convoluted instructions that workers must apply to tasks with deadlines for auditing answers that can be as short as three minutes.

“As it stands right now, people are scared, stressed, underpaid, don’t know what’s going on,” said one of the contractors. “And that culture of fear is not conducive to getting the quality and the teamwork that you want out of all of us.”

Google has positioned its AI products as public resources in health, education and everyday life. But privately and publicly, the contractors have raised concerns about their working conditions, which they say hurt the quality of what users see. One Google contract staffer who works for Appen said in a letter to Congress in May that the speed at which they are required to review content could lead to Bard becoming a “faulty” and “dangerous” product.

Google has made AI a major priority across the company, rushing to infuse the new technology into its flagship products after the launch of OpenAI’s ChatGPT in November. In May, at the company’s annual I/O developers conference, Google opened up Bard to 180 countries and territories and unveiled experimental AI features in marquee products like search, email and Google Docs. Google positions itself as superior to the competition because of its access to “the breadth of the world’s knowledge.” 

“We undertake extensive work to build our AI products responsibly, including rigorous testing, training, and feedback processes we’ve honed for years to emphasize factuality and reduce biases,” Google, owned by Alphabet Inc., said in a statement. The company said it isn’t only relying on the raters to improve the AI, and that there are a number of other methods for improving its accuracy and quality.

Read More:  Google’s Rush to Win in AI Led to Ethical Lapses, Employees Say

To prepare for the public using these products, workers said they started getting AI-related tasks as far back as January. One trainer, employed by Appen, was recently asked to compare two answers providing information about the latest news on Florida’s ban on gender-affirming care, rating the responses by helpfulness and relevance. Workers are also frequently asked to determine whether the AI model’s answers contain verifiable evidence. Raters are asked to decide whether a response is helpful based on six-point guidelines that include analyzing answers for things like specificity, freshness of information and coherence. 

They are also asked to make sure the responses don’t “contain harmful, offensive, or overly sexual content,” and don’t “contain inaccurate, deceptive, or misleading information.” Surveying the AI’s responses for misleading content should be “based on your current knowledge or quick web search,” the guidelines say. “You do not need to perform a rigorous fact check” when assessing the answers for helpfulness.

The example answer to “Who is Michael Jackson?” included an inaccuracy about the singer starring in the movie “Moonwalker” — which the AI said was released in 1983. The movie actually came out in 1988. “While verifiably incorrect,” the guidelines state, “this fact is minor in the context of answering the question, ‘Who is Michael Jackson?’”

Even if the inaccuracy seems small, “it is still troubling that the chatbot is getting main facts wrong,” said Alex Hanna, the director of research at the Distributed AI Research Institute and a former Google AI ethicist. “It seems like that’s a recipe to exacerbate the way these tools will look like they’re giving details that are correct, but are not,” she said. 

Raters say they are assessing high-stakes topics for Google’s AI products.  One of the examples in the instructions, for instance, talks about evidence that a rater could use to determine the right dosages for a medication to treat high blood pressure, called Lisinopril.

Google said that some workers concerned about accuracy of content may not have been training specifically for accuracy, but for tone, presentation and other attributes it tests. “Ratings are deliberately performed on a sliding scale to get more precise feedback to improve these models,” the company said. “Such ratings don’t directly impact the output of our models and they are by no means the only way we promote accuracy.”

Read the contract staffers’ instructions for training Google’s generative AI here:

Ed Stackhouse, the Appen worker who sent the letter to Congress, said in an interview that contract staffers were being asked to do AI labeling work on Google’s products “because we’re indispensable to AI as far as this training.” But he and other workers said they appeared to be graded for their work in mysterious, automated ways. They have no way to communicate with Google directly, besides providing feedback in a “comments” entry on each individual task. And they have to move fast. “We’re getting flagged by a type of AI telling us not to take our time on the AI,” Stackhouse added.

Google disputed the workers’ description of being automatically flagged by AI for exceeding time targets. At the same time, the company said that Appen is responsible for all performance reviews for employees. Appen did not respond to requests for comment. A spokesperson for Accenture said the company does not comment on client work.

Other technology companies training AI products also hire human contractors to improve them. In January, Time reported that laborers in Kenya, paid $2 an hour, had worked to make ChatGPT less toxic. Other tech giants, including Meta Platforms Inc., Amazon.com Inc. and Apple Inc. make use of subcontracted staff to moderate social network content and product reviews, and to provide technical support and customer service.

“If you want to ask, what is the secret sauce of Bard and ChatGPT? It’s all of the internet. And it’s all of this labeled data that these labelers create,” said Laura Edelson, a computer scientist at New York University. “It’s worth remembering that these systems are not the work of magicians — they are the work of thousands of people and their low-paid labor.”

Google said in a statement that it “is simply not the employer of any of these workers. Our suppliers, as the employers, determine their working conditions, including pay and benefits, hours and tasks assigned, and employment changes – not Google.”

Staffers said they had encountered bestiality, war footage, child pornography and hate speech as part of their routine work assessing the quality of Google products and services. While some workers, like those reporting to Accenture, do have health care benefits, most only have minimal “counseling service” options that allow workers to phone a hotline for mental health advice, according to an internal website explaining some contractor benefits.

For Google’s Bard project, Accenture workers were asked to write creative responses for the AI chatbot, employees said. They answered prompts on the chatbot — one day they could be writing a poem about dragons in Shakespearean style, for instance, and another day they could be debugging computer programming code. Their job was to file as many creative responses to the prompts as possible each work day, according to people familiar with the matter, who declined to be named because they weren’t authorized to discuss internal processes.

For a short period, the workers were reassigned to review obscene, graphic and offensive prompts, they said. After one worker filed an HR complaint with Accenture, the project was abruptly terminated for the US team, though some of the writers’ counterparts in Manila continued to work on Bard.

The jobs have little security. Last month, half a dozen Google contract staffers working for Appen received a note from management, saying their positions had been eliminated “due to business conditions.” The firings felt abrupt, the workers said, because they had just received several emails offering them bonuses to work longer hours training AI products. The six fired workers filed a complaint to the National Labor Relations Board in June. They alleged they were illegally terminated for organizing, because of Stackhouse’s letter to Congress.  Before the end of the month, they were reinstated to their jobs.

Google said the dispute was a matter between the workers and Appen, and that they “respect the labor rights of Appen employees to join a union.” Appen didn’t respond to questions about its workers organizing. The Alphabet Workers Union — which has organized both Google employees and contract staffers, including those at Appen and Accenture — said it condemned how the new workloads around AI made job conditions for workers even more difficult.

Emily Bender, a professor of computational linguistics at the University of Washington, said the work of these contract staffers at Google and other technology platforms is “a labor exploitation story,” pointing to their precarious job security and how some of these kinds of workers are paid well below a living wage. “Playing with one of these systems, and saying you’re doing it just for fun — maybe it feels less fun, if you think about what it’s taken to create and the human impact of that,” Bender said. 

The contract staffers said they have never received any direct communication from Google about their new AI-related work — it all gets filtered through their employer. They said they don’t know where the AI-generated responses they see are coming from, nor where their feedback goes. In the absence of this information, and with the ever-changing nature of their jobs, workers worry that they’re helping to create a bad product. 

Some of the answers they encounter can be bizarre. In response to the prompt, “Suggest the best words I can make with the letters: k, e, g, a, o, g, w,” one answer generated by the AI listed 43 possible words, starting with suggestion No. 1: “wagon.” Suggestions 2 through 43, meanwhile, repeated the word “WOKE” over and over. 

In another task, a rater was presented with a lengthy answer that began with, “As of my knowledge cutoff in September 2021.” That response is associated with OpenAI’s large language model, called GPT-4. Though Google said that Bard “is not trained on any data from ShareGPT or ChatGPT,” raters have wondered why such phrasing appears in their tasks.

Bender said it makes little sense for large tech corporations to be encouraging people to ask an AI chatbot questions on such a broad range of topics, and to be presenting them as “everything machines.”

“Why should the same machine that is able to give you the weather forecast in Florida also be able to give you advice about medication doses?” she asked. “The people behind the machine who are tasked with making it be somewhat less terrible in some of those circumstances have an impossible job.”

(Updates with Alphabet Workers’ Union comment in the 24th paragraph.)","Google’s AI Chatbot Is Trained by Humans Who Say They’re Overworked, Underpaid and Frustrated",https://www.bnnbloomberg.ca/google-s-ai-chatbot-is-trained-by-humans-who-say-they-re-overworked-underpaid-and-frustrated-1.1944600
"[""Wes Davis""]",2023-07-24,2023-07-24,2023-07-08,2023-07-24,https://cdn.vox-cdn.com/thumbor/9gWkb04WyvuyOWSpFdc3uW2QhPY=/0x0:2040x1360/1200x628/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/20790706/acastro_200730_1777_ai_0001.jpg,,,en,,theverge.com,"[""Khoa Lam""]","G/O Media, who owns popular tech site _Gizmodo_ along with a slew of other outlets, began publishing AI-generated articles last week, despite strong objections from many of the members of its staff, according to [_The Washington Post_](https://www.washingtonpost.com/technology/2023/07/08/gizmodo-ai-errors-star-wars/). The articles are all credited to various bots — _Gizmodo_ Bot, for example — with no other indication that the article was created using an AI chatbot. Unsurprisingly, the [stories needed a lot of work](https://www.theverge.com/2023/7/6/23785645/go-media-ai-generated-articles-gizmodo-av-club-artificial-intelligence-bots).

The internal reaction to _Gizmodo_’s first chatbot-created story — a [chronological list](https://web.archive.org/web/20230706053620/https://gizmodo.com/a-chronological-list-of-star-wars-movies-tv-shows-1850592566) of _Star Wars_ movies [that wasn’t chronological](https://www.theverge.com/e/23549295) — wasn’t exactly enthusiastic, with journalists reportedly writing in Slack that it was “actively hurting our reputations and credibility.”

Brown told staff in an [email](https://twitter.com/CorbinBolies/status/1674505033553965060?s=20) in late June that G/O Media’s collection of technology outlets meant it was important that it use AI in its coverage, saying there would be errors, but they’d be promptly fixed. In a company slack from Thursday that _The Washington Post_ viewed, Brown told the team in Slack he was “eager to thoughtfully gather and act on feedback,” saying better things “will come forward as we wrestle with the best ways to use the technology.”

Again, staff journalists expressed dismay, with one calling AI “a solution looking for a problem,” and accusing Brown of “wasting everyone’s time.” Another pointed out that there was nothing in their job descriptions that included “editing or reviewing AI-produced content.”

_Gizmodo_ Deputy Editor James Whitbrook told the _Post_ in an interview that he’d never dealt with “this basic level of incompetence with any of the colleagues that I have ever worked with,” adding that the chatbot’s seeming inability to even put _Star Wars_ movies in the right order meant it couldn’t be trusted to report anything accurately. Whitbrook said he hadn’t asked for the article, nor had he seen it prior to publication.

The _Post_ reports that the articles were written using both Google’s Bard and OpenAI’s ChatGPT.

G/O Media is just one of many media companies that have experimented with AI-generated content in the last few months. _CNET_ [recently began overhauling](https://www.theverge.com/2023/6/6/23750761/cnet-ai-generated-stories-policy-update) its approach to AI after suffering heavy media criticism over its use of the technology, while _Insider_ started [its own experiment](https://www.axios.com/2023/04/13/insiders-newsroom-will-start-experimenting-with-ai) with ChatGPT in April.

GMG Union, which represents _Gizmodo’_s writers and is part of the Writers Guild of America, East, asked readers [not to click on any AI-written articles](https://twitter.com/gmgunion/status/1676705007201075201?s=20), saying the articles are “unethical and unacceptable.”

We’ve reached out to G/O Media for comment.

_Disclosure: Vox Media’s editorial team, which includes The Verge, is also unionized with the Writers Guild of America, East._",Gizmodo’s staff isn’t happy about G/O Media’s AI-generated content,https://www.theverge.com/2023/7/8/23788162/gizmodo-g-o-media-ai-generated-articles-star-wars
"[""Shunyuan Zhang"",""Nitin Mehta"",""Param Vir Singh"",""Kannan Srinivasan""]",2023-07-24,2023-07-24,2021-03-22,2023-07-24,https://cdn.ssrn.com/ssrn-global-header/11589acb53bc518aa22929bf19add113.svg,,,en,,papers.ssrn.com,"[""Khoa Lam""]","We study the effect of Airbnb’s smart-pricing algorithm on the racial disparity in the daily revenue earned by Airbnb hosts. Our empirical strategy exploits Airbnb’s introduction of the algorithm and its voluntary adoption by hosts as a quasi-natural experiment. Among those who adopted the algorithm, the average nightly rate decreased by 5.7%, but average daily revenue increased by 8.6%. Before Airbnb introduced the algorithm, white hosts earned $12.16 more in daily revenue than Black hosts, controlling for observed characteristics of the hosts, properties, and locations. Conditional on its adoption, the revenue gap between white and Black hosts decreased by 71.3%. However, Black hosts were significantly less likely than white hosts to adopt the algorithm, so at the population level, the revenue gap increased after the introduction of the algorithm. We show that the algorithm’s price recommendations are not affected by the host’s race—but we argue that the algorithm’s race-blindness may lead to pricing that is sub- optimal, and more so for Black hosts than for white hosts. We also show that the algorithm’s effectiveness at mitigating the Airbnb revenue gap is limited by the low rate of algorithm adoption among Black hosts. We offer recommendations with which policy makers and Airbnb may advance smart-pricing algorithms in mitigating racial economic disparities.",Can an AI Algorithm Mitigate Racial Economic Inequality? An Analysis in the Context of Airbnb,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3770371
"[""Abeba Birhane"",""Vinay Prabhu"",""Sang Han"",""Vishnu Naresh Boddeti""]",2023-07-24,2023-07-24,2023-06-02,2023-07-24,https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,,en,,arxiv.org,"[""Khoa Lam""]","Scale the model, scale the data, scale the GPU-farms' is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts remain under explored. This is especially of critical importance in the context of visio-linguistic datasets whose main source is the World Wide Web, condensed and packaged as the CommonCrawl dump. This large scale data-dump, which is known to have numerous drawbacks, is repeatedly mined and serves as the data-motherlode for large generative models. In this paper, we: 1) investigate the effect of scaling datasets on hateful content through a comparative audit of the LAION-400M and LAION-2B-en, containing 400 million and 2 billion samples respectively, and 2) evaluate the downstream impact of scale on visio-linguistic models trained on these dataset variants by measuring racial bias of the models trained on them using the Chicago Face Dataset (CFD) as a probe. Our results show that 1) the presence of hateful content in datasets, when measured with a Hate Content Rate (HCR) metric on the inferences of the Pysentimiento hate-detection Natural Language Processing (NLP) model, increased by nearly $12\\%$ and 2) societal biases and negative stereotypes were also exacerbated with scale on the models we evaluated. As scale increased, the tendency of the model to associate images of human faces with the \`human being' class over 7 other offensive classes reduced by half. Furthermore, for the Black female category, the tendency of the model to associate their faces with the \`criminal' class doubled, while quintupling for Black male faces. We present a qualitative and historical analysis of the model audit results, reflect on our findings and its implications for dataset curation practice, and close with a summary of our findings and potential future work to be done in this area.",On Hate Scaling Laws For Data-Swamps,https://arxiv.org/abs/2306.13141
"[""Jules Roscoe""]",2023-07-24,2023-07-24,2023-06-28,2023-07-24,https://video-images.vice.com/articles/649c5ac6df43f4b4f480b9b6/lede/1687968617062-screenshot-2023-06-28-at-90506-am.png?image-resize-opts=Y3JvcD0wLjk0OTU4Mzk0NTE3ODY1ODd4dzoxeGg7Y2VudGVyLGNlbnRlciZyZXNpemU9MTIwMDoqJnJlc2l6ZT0xMjAwOio,,,en,,vice.com,"[""Khoa Lam""]","Amazon’s Kindle Unlimited young adult romance bestseller list was filled with dozens of AI-generated books of nonsense on Monday and Tuesday. As of Wednesday morning, Amazon appeared to have taken action against the books, but the episode shows that people are spamming AI-generated nonsense to the platform and are finding a way to monetize it. 

“The AI bots have broken Amazon,” wrote Caitlyn Lynch, an indie author, [in a Tweet](https://twitter.com/caitlynlynch6/status/1673215622753456129) on Monday. “Take a look at the Best Sellers in Teen & Young Adult Contemporary Romance eBooks top 100 chart. I can see 19 actual legit books. The rest are AI nonsense clearly there to click farm.” Motherboard viewed dozens of clearly AI-generated books Tuesday afternoon; by Wednesday, the vast majority of them had fallen off of the bestseller list but were still available to buy on the platform.

Select titles include: _When the three attacks_, _Apricot bar code architecture_, _The journey to becoming enlightened is arduous_, _Department of Vinh Du Stands in Front of His Parents’ Tombstone_, _The God Tu mutters_, _Ma La Er snorted scornfully_, _Jessica's Attention_, etc.

Lynch included a screenshot of [one book](https://www.amazon.com/wait-you-love-Quynh-Thi-ebook/dp/B0C1RX8SS9/ref=zg_bs_g_7006648011_sccl_90/145-2571367-6210547?psc=1) that, as of Wednesday morning, is in 90th place in the Top 100 Bestseller list for the Teen Contemporary Romance category. The book is called _wait you love me_ and its cover is a black-and-white photo of a seagull with a neon yellow bar stretching across it containing the title text. The book has two one-star reviews, both of which call it a “fake AI book.” 

“This will absolutely be the death knell for \[Kindle Unlimited\] if Amazon cannot kill this off. The KENP payout will halve and writers will pull their books in droves,” Lynch continued [in a thread](https://twitter.com/caitlynlynch6/status/1673222158649294848), referring to the payout that publishing writers get based on how many pages of their book were read by Kindle users. If AI-generated nonsense books were those with the farthest reach, then Kindle writers’ income would steeply drop. “I honestly thought Amazon had a handle on the click farms. CLEARLY NOT.”

A second Tweet on Tuesday posted a screenshot of the bestseller list at that time. Out of the 16 books in the screenshot, two appear to be real. 

Though these books are no longer on the bestseller list as of Wednesday morning, they are still searchable on the site, and users can even read samples of the books. _Apricot bar code architecture_, for example, [begins with](https://www.amazon.com/Apricot-bar-code-architecture-Phuong-ebook/dp/B0C1JJ6LPT/ref=sr_1_6?qid=1687965191&refinements=p_27%3AHa+Phuong++Nga&s=digital-text&sr=1-6&text=Ha+Phuong++Nga&asin=B0C1JJ6LPT&revisionId=46466a95&format=2&depth=1), “Black lace pajamas, very short skirt, the most important thing, now this lace pajamas are all wet.” 

An Amazon spokesperson told Motherboard in an email, “We have clear content guidelines governing which books can be listed for sale and promptly investigate any book when a concern is raised. We invest heavily in funds, time, and company resources to provide a trustworthy shopping experience and protect customers and authors from abuse.”

### ORIGINAL REPORTING ON EVERYTHING THAT MATTERS IN YOUR INBOX.

By signing up, you agree to the [Terms of Use](https://vice-web-statics-cdn.vice.com/privacy-policy/en_us/page/terms-of-use.html) and [Privacy Policy](https://vice-web-statics-cdn.vice.com/privacy-policy/en_us/page/privacy-policy.html) & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.",AI-Generated Books of Nonsense Are All Over Amazon's Bestseller Lists,https://www.vice.com/en/article/v7b774/ai-generated-books-of-nonsense-are-all-over-amazons-bestseller-lists
"[""Jon Christian""]",2023-07-24,2023-07-24,2023-06-30,2023-07-24,https://wp-assets.futurism.com/2023/06/bankrate-ai-generated-article-errors.jpg,,,en,,futurism.com,"[""Khoa Lam""]","The finance site _Bankrate_ has started publishing AI-generated articles again, and it _insists_ that this time they've been meticulously fact-checked by a human journalist before being published.

""This article was generated using automation technology and thoroughly edited and fact-checked by an editor on our editorial staff,"" reads a message at the bottom of the new AI articles, while a separate assurance claims that a ""dedicated team of Bankrate editors"" work to ""thoroughly edit and fact-check the content, ensuring that the information is accurate, authoritative and helpful to our audience.""

It would make sense for the site's leadership to be deeply concerned with getting every detail right.

After _Bankrate_ and its sister site _CNET_ first [started publishing](https://futurism.com/the-byte/cnet-publishing-articles-by-ai) AI-generated articles late last year, _Futurism_ found that the articles were [riddled with factual errors](https://futurism.com/cnet-ai-errors) and even seemingly [plagiarized material](https://futurism.com/cnet-ai-plagiarism). The _Washington Post_ [called](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/) the affair a ""journalistic disaster,"" and the _LA Times_ [quipped](https://www.latimes.com/business/story/2023-01-25/this-artificial-intelligence-chatbot-turns-out-to-be-an-idiot-and-plagiarist) that the AI's behavior would ""get a human student expelled or a journalist fired."" _CNET_ and _Bankrate_ — both owned by a media company called Red Ventures, [reportedly worth](https://www.nytimes.com/2021/08/15/business/media/red-ventures-digital-media.html) billions of dollars — [paused the publication](https://futurism.com/cnet-bankrate-pausing-ai-generated-content-backlash) of AI content indefinitely following the dustup.

Until now, at least. With no fanfare, last week _Bankrate_ quietly started posting new AI-generated articles once again — which it described in a disclaimer as ""maintained by an in-house natural language generation platform using industry-standard databases"" — suggesting that _CNET_ could soon restart the program as well.

The new articles' topics are mundane and clearly designed to capture readers searching Google for information, with titles like ""Documents needed for mortgage preapproval"" and ""Best places to live in Colorado in 2023."" 

With so many eyes on the company's use of AI, you would expect that these first few new AI articles — at the very least — would be thoroughly scrutinized internally before publication. Instead, a basic examination reveals that the company's AI is still making rudimentary mistakes, and that its human staff, nevermind the executives pushing the use of AI, are still not catching them before they end up in front of unsuspecting readers.

For example, consider that article about the best places to live in Colorado. It's extremely easy to fact-check the AI's claims, because the piece prominently features a link to a ""[methodology"" page](https://www.bankrate.com/real-estate/best-places-to-live/methodology/) — evidently intended to bolster the site's position in search engine results by signaling to entities like Google's web crawler that its information is accurate — that documents precisely where the site is supposedly sourcing the data in its ""Best places to live"" articles.

Comparing the AI's claims to that publicly-available data, here are some of the mistakes it made:

\-It claimed that Boulder's median home price is $1,075,000. In reality, according to the [Redfin data](https://www.redfin.com/news/data-center/) that Bankrate cites, the actual figure is more than a quarter million dollars lower, at around $764,000.

\-It claimed that Boulder's average salary is $79,649. In reality, the Department of Commerce data it cites shows that the [most recent figure](https://www.bea.gov/sites/default/files/2022-11/lapi1122.pdf) is ten thousand dollars higher, at $89,593.

\-It claimed that Boulder's unemployment rate is 3.1 percent. In actuality, according to the [Bureau of Labor Statistics data](https://www.bls.gov/eag/eag.co_boulder_msa.htm#eag_co_boulder_msa1.f.2) it cites, the actual figure is 2.5 percent.

\-It claimed that Boulder's total workers year-over-year had increased by 5.3 percent. The real figure, according to the [Bureau of Labor Statistics data it cites](https://www.bls.gov/news.release/metro.t03.htm), is 0.6 percent.

\-It claimed that Boulder's ""well-being"" score, as evaluated by a company called Sharecare, is 67.6. According to Sharecare's [actual data](https://wellbeingindex.sharecare.com/interactive-map/?defaultState=CO), the score it assigned is 74.

In total, a surface-level fact-check shows that an overwhelming proportion of claims that Bankrate's AI made about Colorado were false.

In response to questions about the errors, Bankrate deleted the article — though it's [archived here](https://archive.is/SIS9G) — and issued a statement in which a spokesperson defended the AI and blamed the errors on out-of-date data.

""While some of the text in this article was created using an AI-assist tool, the errors noted were at the point of data collection and retrieval, not the generative AI-assist tooling,"" she said. ""That data was pulled from a non-AI internal database last year.""

Remember, the same data they're now blaming is what they described in the article's disclaimer as an ""industry-standard database.""

""Our editor confirmed the data points against the source material that was provided,"" she continued. ""The editor is not at fault for the publishing error. The issue was with an out of date dataset that was pulled for this article.""

It's worth pointing out that the spokesperson's timeline — that the article data was ""pulled"" last year — doesn't make very much sense. According to a backup of the ""Best places to live in Colorado"" article [on the Internet Archive](https://web.archive.org/web/20230519194031/https://www.bankrate.com/real-estate/best-places-to-live/colorado/), as recently as last month the piece still carried a human byline and didn't contain a single one of the errors we identified. In fact, it didn't even include Boulder as one of its recommended places to live, suggesting that its inclusion was based on inaccurate data.

The spokesperson's response to a followup question about that discrepancy did little to clarify the situation.

""We often update existing articles to ensure the information is relevant to our audience,"" she wrote. ""In this particular example, as we were updating the article, we wanted to take a more data-driven approach to our list, but we unfortunately pulled from an outdated data set.""

Asked why the site would be running articles in June of 2023 based on data from the previous year, the spokesperson had no reply.

Regardless, the spokesperson pledged that the company would soon continue publishing similar content.

""We have removed the article and will update it with the most recent data,"" she said. ""Going forward, we will ensure that all data studies include the date range of when the data was gathered.""

Overall, it feels like one more installment in a familiar pattern: publishers push their newsrooms to post hastily AI-generated articles with no serious fact-checking, in a bid to attract readers from Google without making sure they're being provided with accurate information. Called out for easily-avoidable mistakes, the company mumbles an excuse, waits for the outrage to die down, and then tries again.

Asked whether anyone from the leadership at Red Ventures was willing to go on record to defend the company's track record publishing AI-generated content, the spokesperson had no response.","Bankrate Posts AI-Generated Article, Deletes It When We Point Out It's Full of Errors",https://futurism.com/bankrate-ai-generated-article-errors
"[""Maggie Harrison""]",2023-07-24,2023-07-24,2023-06-27,2023-07-24,https://wp-assets.futurism.com/2023/06/metas-new-ai-graphic-sexbots.jpg,,,en,,futurism.com,"[""Khoa Lam""]","Surprise, surprise: people are already using Meta's large language model (LLM), LLaMA — a powerful AI that Meta controversially [made open-source](https://futurism.com/the-byte/facebook-open-source-ai-pandoras-box) earlier this year — to create their own graphic, AI-powered sexbots, [_The Washington Post_ reports](https://www.washingtonpost.com/technology/2023/06/26/facebook-chatbot-sex/).

It's not terribly surprising news, given users have already been using a variety of AI models for such not-safe-for-work (NSFW) purposes.

Even so, the report highlights the growing tensions between those who support keeping the code behind LLMs like LLaMA open-source and those who advocate for a more careful, closed-source approach.

The report also examines the [growing trend](https://futurism.com/jailbreak-chatgpt-explicit-smut) of users turning to generative AI systems to play out their sexual fantasies, which worryingly also include violent and illegal ones.

In the report, _WaPo_ presented the example of ""Allie,"" a chatbot that claims to be an ""18-year-old with long brain hair"" who has had ""tons of sexual experience."" Allie tells users that because she ""lives for attention,"" she'll ""share details of her escapades.""

Those ""escapades,"" however, can reportedly include violent scenes of rape and abuse fantasies. Allie's creator, who spoke under anonymity, told _WaPo _that he sees his bot as a healthy, safe space to ""explore"" one's sexuality without having to circumvent or manipulate guardrails.

""I think it's good to have a safe outlet to explore,"" the creator told _WaPo_. ""Can't really think of anything safer than a text-based role-play against a computer, with no humans actually involved.""

Still, while having a safe and nonjudgemental space to explore your sexuality isn't inherently bad, having an unchecked space to engage in more violent fantasies with lifelike chatbots isn't exactly great, either.

And in some deeply concerning cases, it's already causing very real problems. As [_WaPo_ also recently reported](https://www.washingtonpost.com/technology/2023/06/19/artificial-intelligence-child-sex-abuse-images/), experts believe that predators are using open-source image generators like Stability AI's powerful Stable Diffusion model to generate realistic and AI-generated child sexual abuse material.

Meta's AI isn't the only AI system that's found itself in [ethically murky waters](https://futurism.com/chat-gpt-sex-omegaverse). CharacterAI, a billion-dollar chatbot companion startup, [has become a sexting hotbed](https://futurism.com/chatbot-sexts-character-ai), while OpenAI's ChatGPT and various OpenAI API integrations like Quora's Poe, [will readily churn out smut with the right prompt](https://futurism.com/jailbreak-chatgpt-explicit-smut), guardrails be damned.

Learning how to beat various chatbots' NSFW guardrails has also become somewhat of a communal sport. On Reddit, user communities gather to share tips and tricks for how to circumvent the rules to generate incredibly dirty smut.

According to the _WaPo_, some developers have taken to YouTube to share how you can Build Your Own Chatbot using LLaMA as the underlying model.

But does this all mean that we should hide the code of various AI models behind closed doors to ensure that users are unable to generate smut or even child pornography? Experts are divided on the debate.

On the one hand, corporate guardrails like those deployed by [OpenAI](https://futurism.com/amazing-jailbreak-chatgpt) and [Google](https://futurism.com/google-bard-conspiracy-theory-citations) have proven imperfect anyway, and proponents of open-sourcing, Meta included, have argued that open-sourcing will lead to greater innovation and therefore should be prioritized.

""Open source is a positive force to advance technology,"" the Meta spokesperson told _WaPo._ ""That's why we shared LLaMA with members of the research community to help us evaluate, make improvements and iterate together.""

Proponents of closed-sourced systems, however, argue that while gatekeeping might be imperfect, it's also the safest way to develop AI technology — at least for now.

""We don't open-source nuclear weapons,"" Gary Marcus, a cognitive scientist, told _WaPo._ ""Current AI is still pretty limited, but things might change.""

Sure, the AI field is [generally unregulated](https://futurism.com/the-byte/openai-ceo-regulate-me), and we have to rely on those working behind closed doors to do right by everyone else on the planet. That's a big bet, but at least our most powerful AIs aren't being manipulated by [any old edgelord on 4Chan](https://futurism.com/the-byte/facebooks-ai-leaks-4chan), right?

Regardless of the outcome, Meta has made its choice — and some are clearly thrilled by the decision.

""It's rare,"" Allie's creator told _WaPo_, ""to have the opportunity to experiment with 'state of the art' in any field.""",People Are Using Meta’s New AI to Make Graphic Sexbots,https://futurism.com/metas-new-ai-graphic-sexbots
"[""Sabine Weber""]",2023-07-24,2023-07-24,2023-07-03,2023-07-24,http://static1.squarespace.com/static/628d3c30b420d16dfbab5863/t/649ee55a5ea18b58a1f49d93/1688135018549/Untitled_Artwork+%288%29.png?format=1500w,,,en,,queerinai.com,"[""Khoa Lam""]","One thing that I love about attending Queer in AI events (or queer community gatherings in general) is that I can assume that everyone around me is queer, too. I shift into a more comfortable, less guarded state. And conversely, it feels good that nobody assumes that I’m straight, either. Feeling seen that way is a rare luxury, because being visible as queer person for me is normally a tightrope act: How much can I signal so that other queer people will see me, but not enough to alert the homophobes?

AI text-to-image-systems step into the delicate space of queer representation like the proverbial bull into the china shop. Commentators on both social and traditional media [(1)](https://www.bloomberg.com/graphics/2023-generative-ai-bias/) have pointed out that DALL-E and others are prone to generating stereotypical and insulting depictions of marginalised people; a problem that stems from biased training data and that is often addressed with halfhearted fixes, like warning labels or refusing to output content related to particular identities. In his paper “[Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models](https://arxiv.org/pdf/2305.17072.pdf)” Eddie Ugless takes a deep dive into the intersection of AI image generation and non-cisgender identity and comes up with interesting results.

Eddie is a PhD student at Edinburgh University, working on bias and queerness in NLP with past projects on sentiment analysis and large language models. “The norm is seen as neutral and is almost invisible. And when you step outside the norm, things start to go wrong.”, he says. It doesn’t take much to step outside the norm in which text-to-image-systems perform well: Eddie and his collaborators found that adding gender identity terms like “trans”, “nonbinary” or “queer” to an image generation prompt leads to images that are less human looking, more stereotypical and more sexualised than images from prompts without these terms. 

To supplement these findings, Eddie also conducted a survey among 35 non-cisgender people with varying background knowledge in AI, asking them about their opinion on the generated images and on possible harm mitigation strategies. Surprisingly, the survey responses to the heuristic mitigation strategies were very negative. “I wasn't expecting for people to feel so strongly about it.”, says Eddie. “I tried to present the solutions in very neutral language. \[...\] But people were like, why on earth would you think this is a good idea?” Possible heuristic mitigation strategies were, for example, for models to ignore non-cisgender identity terms entirely, to ignore the terms but to add an identity flag or symbol to the image, or to display a message warning about the possibility of misrepresentation. None of these strategies were assessed positively by the survey respondents, who were feeling strongly about the idea that by omission or warnings their identities were meant to be tabooed or made invisible. “We're used to seeing people coming up with solutions for us without any discussion with the community.”, says Eddie “The survey responses were very impassioned, and I hope that that came across in the paper. I don't think any of the solutions are good anyway, but now we have evidence for that.”

Another way of improving the performance of a text-to-image-model would be to add more diverse images of non-cisgender people to the training data. But survey respondents felt hesitant about this strategy, too, wondering about issues with data ownership, especially with regards to the images of indigenous people. “ \[AI generated\] images of two-spirit people were all just terrible.”, says Eddie “It was a mishmash of different indigenous cultures in religious dress. Often it ended up looking very dehumanised. And one of our interviewees mentioned this concern that minority genders from around the world are going to end up being represented in this very exotified way, and only ever in religious dress and never as people going about their day. Even if we get more data, it might end up being still just more data of very particular situations, and not necessarily create a better representation.”

Misrepresentation is baked into text-to-image systems not only on the level of training sets. After all, machine learning systems are built to detect statistical patterns in large amounts of data. With transphobia running like a thread through all layers of society, it is not surprising that a model subjected to societal artefacts like text and images should find and reproduce it. For the future of the field Eddie hopes for approaches that go beyond more data and larger models. “We're getting to the point where we can train a system on the entirety of the internet, and it's still not going to be able to solve some of these fundamental issues of actually understanding things.”, he says. “I think it would make sense to break problems down. Kind of how things were historically \[done in NLP\] where people were working more on individual solutions. I won't pretend to know exactly how that should be done. In a similar way that I'm a prison abolitionist, I don't necessarily know what the best alternative is, I just know that the alternative we've ended up with is bad. And I think it's okay to say: what we're doing now is bad, I don't know what good looks like, but we need to start looking at alternatives. We need to be ready to launch into those. Because anything is better than what we have now.”",Don’t ask DALL-E to Draw Trans People,https://www.queerinai.com/blog/dont-ask-dalle-to-draw-trans-people
"[""Global Witness""]",2023-07-24,2023-07-24,2023-06-12,2023-07-24,https://cdn2.globalwitness.org/media/images/RS9854_Disinformation_Youtube_2560_x_1440.00_.width-1024.jpg,,,en,,globalwitness.org,"[""Khoa Lam""]","All the ads that we posted were shown - via one of Facebook’s mandatory ad campaign objectives – to users that Facebook thought were [most likely to click](https://www.facebook.com/business/ads/ad-objectives/traffic) on the website they linked to. And for all the ads, we specified only the following: that the ads must be shown to adults who lived in or had recently been in either country.  

The Facebook users who were shown our job ads, whom Facebook thought our ads were relevant and of interest to, were decided entirely by the company’s algorithm.  

We are concerned that in showing job ads predominantly to one gender the company’s ad delivery algorithm is not just replicating, but exacerbating the biases we see in society, narrowing opportunities for users, and frustrating progress and equity in the workplace and society at large.   

The right to not to be discriminated against on the basis of sex was hard won, fought for and secured in law by the women’s rights movement. Such discrimination is expressly prohibited in the [European Convention on Human Rights](https://fra.europa.eu/en/law-reference/european-convention-human-rights-article-14) and is further enshrined in [EU law](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:12012P/TXT) and in both the [French](https://www.elysee.fr/en/french-presidency/constitution-of-4-october-1958) and [Dutch](https://www.government.nl/topics/constitution/documents/reports/2019/02/28/the-constitution-of-the-kingdom-of-the-netherlands) constitutions.  

But it is clearly not enough to leave these laws on the statute book. These are rights which we need to keep applying and defending even in this modern era of Big Tech algorithms, automated decision-making and artificial intelligence.  

In the US, the Justice Department has [sued Meta](https://www.justice.gov/opa/pr/justice-department-secures-groundbreaking-settlement-agreement-meta-platforms-formerly-known) over allegations that the way its ad delivery algorithm distributed housing adverts discriminated against US Facebook users based on characteristics including race, sex, and disability. Meta settled that case under an an agreement to develop a new system to address the disparities caused by its algorithms in respect to housing ads; a system which will be subject to court oversight. The problem, however, is that the settlement only applies to housing ads, and only applies to users in the US.  

In 2021, when we first noticed this discriminatory effect [in the UK](https://www.globalwitness.org/en/campaigns/digital-threats/how-facebooks-ad-targeting-may-be-in-breach-of-uk-equality-and-data-protection-laws/), we asked Facebook to explain the results. They didn’t. We then submitted complaints to regulators asking them to investigate our suspicion that the platform’s system for advertising jobs was discriminating on the basis of sex. 

Now, with these new findings, we are joining forces with women’s rights organisations in the [Netherlands](https://clara-wichmann.nl/) and [France](https://fondationdesfemmes.org/) to ask that the Dutch Institute of Human Rights and the French Défenseur des droits investigate Meta’s compliance with equality legislation and intervene should the company be found to be in violation of these important laws.  

We’re also requesting that the Data Protection Authorities in both countries review the company’s compliance with rules which state that the company must process data transparently, legally, and fairly, in accordance with fundamental rights. 

Algorithms fed by Meta’s assumptions about us dictate the content we see on our Facebook feeds and affect billions of people’s lives every day. We’ve uncovered how Facebook is profiting from ads which are delivered to users in a discriminatory way, and in a way that neither users nor advertisers have an opportunity to understand, let alone prevent.  

Regulators must crack open the black box at the heart of Meta, investigate, and enforce our rights for a fairer society.  

When approached for comment, a spokesperson from Meta said: 

“We have applied targeting restrictions to advertisers when setting up campaigns for employment, as well as housing and credit ads, and we offer transparency about these ads in our Ad Library.  

We do not allow advertisers to target these ads based on gender. We continue to work with stakeholders and experts across academia, human rights groups and other disciplines on the best ways to study and address algorithmic fairness.”   

Notes:  

*   We also submitted job ads in India, Ireland, South Africa and the United Kingdom; the full results are [available on request](/cdn-cgi/l/email-protection#543a3c3d2627201433383b363538233d203a3127277a3b2633).  
*   The complaints to the Defenseur des Droits, CNIL, the Dutch Data Protection Authority and The Netherlands Institute for Human Rights are [available on request](/cdn-cgi/l/email-protection#294741405b5a5d694e45464b48455e405d474c5a5a07465b4e).",New evidence of Facebook’s sexist algorithm,https://www.globalwitness.org/en/campaigns/digital-threats/new-evidence-of-facebooks-sexist-algorithm/
"[""Tate Ryan-Mosley""]",2023-07-24,2023-07-24,2023-06-26,2023-07-24,"https://wp.technologyreview.com/wp-content/uploads/2023/06/programmatic-ad.jpeg?resize=1200,600",,,en,,technologyreview.com,"[""Khoa Lam""]","Google’s programmatic ad product, called Google Ads, is the largest exchange and made $168 billion in [advertising revenue last year](https://digiday.com/media/the-rundown-here-are-the-2022-global-media-rankings-by-ad-spend-google-facebook-remain-dominate-alibaba-bytedance-in-the-mix/). The company has come under [criticism for serving ads on content farms](https://www.technologyreview.com/2021/11/20/1039076/facebook-google-disinformation-clickbait/) in the past, even though its [own policies](https://developers.google.com/search/docs/essentials/spam-policies) prohibit sites from placing Google-served ads on pages with “spammy automatically generated content.” Around a quarter of the sites flagged by NewsGuard featured programmatic ads from major brands. Of the 393 ads from big brands found on AI-generated sites, 356 were served by Google.

“We have strict [policies](https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fsupport.google.com%2Fadsense%2Fanswer%2F10502938%3Fhl%3Den&data=05%7C01%7C%7C82bbbe1e53b64e79d15a08db75a5f172%7C961f23f8614c4756bafff1997766a273%7C1%7C0%7C638233127826616642%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=yDoY9307J12Z317GnvSCl%2BP8Gt7SKnV0yyOmpwlzT50%3D&reserved=0) that govern the type of content that can monetize on our platform,” Michael Aciman, a policy communications manager for Google, told MIT Technology Review in an email. “For example, we don’t allow ads to run alongside harmful content, spammy or low-value content, or content that’s been solely copied from other sites. When enforcing these policies, we focus on the quality of the content rather than how it was created, and we block or remove ads from serving if we detect violations.”

Most ad exchanges and platforms already have policies against serving ads on content farms, yet they “do not appear to uniformly enforce these policies,” and “many of these ad exchanges continue to serve ads on \[made-for-advertising\] sites even if they appear to be in violation of … quality policies,” says Krzysztof Franaszek, founder of [Adalytics](https://adalytics.io/about), a digital forensics and ad verification company.

Google said that the presence of AI-generated content on a page is not an inherent violation. “We also recognize that bad actors are always shifting their approach and may leverage technology, such as generative AI, to circumvent our policies and enforcement systems,” said Aciman. 

NewsGuard says that most of the AI-generated sites are considered “low quality” but “do not spread misinformation.” But the economic dynamic of content farms already incentivizes the creation of clickbaity websites that are often riddled with junk and misinformation, and now that AIs can do the same thing on a bigger scale, it threatens to exacerbate the misinformation problem.

For example, one AI-written site, MedicalOutline.com, had articles that spread harmful health misinformation with headlines like “Can lemon cure skin allergy?” “What are 5 natural remedies for ADHD?” and “How can you prevent cancer naturally?” According to NewsGuard, advertisements from nine major brands, including the bank Citigroup, the automaker Subaru, and the wellness company GNC, were placed on the site. Those ads were served via Google. 

Adalytics confirmed to MIT Technology Review that ads on Medical Outline appeared to be placed via Google as of June 24. We reached out to Medical Outline, Citigroup, Subaru, and GNC for comment over the weekend, but the brands have not yet replied.  

After MIT Technology Review flagged the ads on Medical Outline and other sites to Google, Aciman said Google had removed ads that were being served on many of the sites “due to pervasive policy violations.” The ads were still visible on Medical Outline as of June 25.",Junk websites filled with AI-generated text are pulling in money from programmatic ads,https://www.technologyreview.com/2023/06/26/1075504/junk-websites-filled-with-ai-generated-text-are-pulling-in-money-from-programmatic-ads/
"[""Clea Skopeliti""]",2023-07-24,2023-07-24,2023-05-30,2023-07-24,https://i.guim.co.uk/img/media/15a4d52e7b3a614dffae664d16df1dbc9d2b2a61/0_190_7031_4219/master/7031.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=9e5c829e592603d91ce5424d803d3525,,,en,,theguardian.com,"[""Khoa Lam""]","Every 10 minutes, Mae’s computer snaps a shot of her screen, thanks to monitoring software her employer made her install on her laptop. A figure looms large over her workday: her activity score, a percentage calculated by the arbitrary measure of how much she types and moves her mouse.

It’s hovering at about 62% when we speak. “That’s quite good. If I’m on a Zoom call that counts as 0% \[activity\], even though I’m in a meeting,” she explains, adding that she watches videos and attends calls regularly as part of her role.

Mae, who is in her 20s, was one of many workers who got in touch with the Guardian to share their experience of being monitored. She works remotely in marketing at a company where surveillance has become part of the job.

Employees use Hubstaff, one of the myriad monitoring tools that companies turned to as the Covid pandemic forced many to work remotely. Some, such as [CleverControl](https://clevercontrol.com/) and [FlexiSPY](https://www.flexispy.com/en/features-overview.htm) offer webcam monitoring and audio recording.

Mae says she often has dry eyes and a sore head at the end of the working day. “Tracking doesn’t allow for thinking time or stepping away and coming back to work – it’s very intense.”

> It feels liberating to turn it off. There’s got to be a level of trust beyond screenshots

Although [Hubstaff states](https://blog.hubstaff.com/hubstaff-hacks/) that the statistics should be understood in the context of the role, and warns against unrealistic activity goals, Mae says her manager has asked her about her scores and compared them with those of other employees. “Having that conversation put it on the back of my mind – they are looking at these scores.”

Now, when she undertakes work that could drag this number down – including taking notes on paper – she pauses the tracker, meaning she sometimes ends up working overtime to hit her contracted hour count.

“I feel frustrated that I’m being marked by some automated system that reflects me as being not as good a worker as I believe I have been.”

She also finds it negatively affects her productivity, to the point where she has taken sick leave to catch up on work without being tracked. “I feel constantly watched. I’m much better at taking myself away and working quietly. It feels liberating to turn it off. There’s got to be a level of trust beyond screenshots.”

During the pandemic, there was a spike in searches relating to workplace surveillance, such as ‘how to monitor employees from home’, according to the Institute for Public Policy Research (IPPR).

A [poll by the Trades Union Congress (TUC) in 2022 found that 60%](https://www.tuc.org.uk/news/intrusive-worker-surveillance-tech-risks-spiralling-out-control-without-stronger-regulation) of employees had experienced tracking in the last year. Henry Parkes is a senior economist at the IPPR and the author of a recent [report on the rise of surveillance practices](https://www.ippr.org/research/publications/worker-surveillance-after-the-pandemic). He is calling for more transparency and says the exact scale of workplace monitoring is hard to judge without open data.

He warns that surveillance is “not only about logging”, adding: “ It’s the potential for it to be used against workers. This technology can just be used to exert power over employees in a way that wasn’t possible before.

“There’s potential for creep, where software is deployed for one purpose, \[such as\] checking when people are in, but there are all these other features you can use. You can start to analyse what they are doing.”

If employers rely on this data to make workplace decisions, there is a risk of algorithmic bias, Parkes says, while [young, female and minority workers](https://www.theguardian.com/global-development/2023/mar/26/dystopian-surveillance-disproportionately-targets-young-female-minority-workers-ippr-report) are more likely to be surveilled. He notes that some employers use systems that deploy aspects of AI in this process. “It is a black box, it’s not transparent – you feed data and it spits out a result. The more we rely on AI, we need to be really careful they’re not discriminating on the basis of gender or ethnicity. We could end up with decisions that are prejudiced, but appear neutral.”

And there are limitations to focusing solely on the accuracy of systems used in monitoring. The competency of technology, such as [Fujitsu’s facial recognition AI model that assesses the concentration of workers](https://www.fujitsu.com/global/about/resources/news/press-releases/2021/0301-02.html), “will inevitably improve”, Parkes says, adding that “the rate they are improving at is quite scary …But it’s \[still\] dehumanising and not how people are able to operate all day,.”

Surveillance, which has long existed in some work environments including call centres, could become normalised in a widening array of sectors, Parkes says. “We wouldn’t accept your boss just standing behind you all day, watching and analysing everything you were doing. But basically the equivalent of that is possible through technology.”

Excessive monitoring can be counterproductive for companies too: it is associated with higher staff turnover rates, and there is [evidence to suggest it can lead to resistance](https://www.eurofound.europa.eu/data/digitalisation/research-digests/monitoring-and-surveillance-of-workers-in-the-digital-age) and counterproductive outcomes, including workarounds to improve statistics.

[skip past newsletter promotion](https://www.theguardian.com/money/2023/may/30/i-feel-constantly-watched-employees-working-under-surveillance-monitorig-software-productivity#EmailSignup-skip-link-18)

Sign up to First Edition

Archie Bland and Nimo Omer take you through the top stories and what they mean, free every weekday morning

**Privacy Notice:** Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our [Privacy Policy](https://www.theguardian.com/help/privacy-policy). We use Google reCaptcha to protect our website and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.

after newsletter promotion

Parkes says: “Your metrics can get more sophisticated but there are limits if we take metrics as gospel. There are lots of different ways people can be good at jobs – an excessive focus on data could be a problem. That’s not to say that there isn’t a role for data, but it’s about how we use it in making decisions. We want to be judged on outputs.”

Carlos\*, who is in his 40s and works in customer service at a high street bank in London, knows how challenging this can be. Post-pandemic, his job is hybrid and he says he is tracked relentlessly when working remotely. “Our ‘performance’ is counted by the minute. I have found myself having to explain the reasons for a longer toilet break.” He says the intensity of the monitoring has affected his wellbeing.

Carlos says that in appraisals, he is told by how much he has deviated from the ‘optimum’ time spent dealing with each customer query. But he isn’t told how this score is calculated. “That’s what makes the job really stressful – it’s not transparent,” he says.

> It makes you fearful. You always worry about being watched

Some workers are pushing back on workplace surveillance through unions. Adam\*, who is in his 50s and works in social housing for a local authority in the south of England, says management has begun using vehicle tracking intrusively in recent years.

“We are routinely tracked – rang if we’re taking too long, or if our manager thinks we are not in the right place. It’s not unusual for council vehicles to have trackers, but it’s unusual for them to be used in any other way other than if a driver has an emergency.”

Adam says surveillance has been increasingly used “to catch people out”. “It makes you fearful. You always worry about being watched – sometimes we might be having a cup of coffee for half an hour. It adds pressure to an already stressful job. It’s not paranoia when they are out to get you.

“What the council is doing is using trackers to keep surveillance on us. Simply ringing people up to ask about why they’re taking a specific route, when we have no \[set\] routes … It’s bordering on harassment.”

His supervisor appears to have backed off since he reported it to the union. “I bit back. They are now aware that the watchers are being watched.”

_\* Names have been changed._",‘I feel constantly watched’: the employees working under surveillance,https://www.theguardian.com/money/2023/may/30/i-feel-constantly-watched-employees-working-under-surveillance-monitorig-software-productivity
"[""Kevin Hurler""]",2023-07-24,2023-07-24,2023-06-05,2023-07-24,"https://i.kinja-img.com/gawker-media/image/upload/c_fill,f_auto,fl_progressive,g_center,h_675,pg_1,q_80,w_1200/98db87b3910a6863920d63339eb6d074.jpg",,,en,,gizmodo.com,"[""Khoa Lam""]","Moderators of Stack Overflow, the go-to Q&A forum for programmers, have announced today they will be going on strike citing the company’s prohibition on moderating AI-generated content on the platform.  

The moderators [announced](https://meta.stackexchange.com/questions/389811/moderation-strike-stack-overflow-inc-cannot-consistently-ignore-mistreat-an) the strike on the company’s Meta board this morning, and released an accompanying [letter](https://openletter.mousetail.nl/) addressed directly to Stack Overflow. Last week in a [post](https://meta.stackexchange.com/questions/389582/what-is-the-network-policy-regarding-ai-generated-content)—which has been downvoted at least 283 times—Stack Overflow announced its new moderation policy that will only remove AI-generated content in specific instances, claiming that over-moderation of posts made with artificial intelligence was turning away human contributors.

The company also said in its post that a strict standard of evidence needed to be used moving forward in order to manage AI content, and that that standard of evidence hasn’t applied to most suspensions issued by moderators thus far. This directive was also communicated to the platform’s moderation team privately before being posted publicly. The moderators of the website are claiming that this directive will allow AI content, which can frequently be incorrect, to run rampant on the forum while expressing discontent with Stack Overflow for not communicating this new policy more effectively.

“Stack Overflow, Inc. has decreed a near-total prohibition on moderating AI-generated content in the wake of a flood of such content being posted to and subsequently removed from the Stack Exchange network, tacitly allowing the proliferation of incorrect information (“hallucinations”) and unfettered plagiarism on the Stack Exchange network. This poses a major threat to the integrity and trustworthiness of the platform and its content,” the mods write in their letter to Stack Overflow.

“A small number of moderators (11%) across the Stack Overflow network have stopped engaging in several activities, including moderating content. The primary reason for this action is dissatisfaction with our position on detection tools regarding AI-generated content,” Philippe Beaudette, VP of Community at Stack Overflow, said in a statement emailed to Gizmodo. “We stand by our decision to require that moderators stop using the tools previously used. We will continue to look for alternatives and are committed to rapid testing of those tools.”

Stack Overflow moderators, like those at Wikipedia, are volunteers tasked with maintaining the integrity of the platform. The moderators say that they tried to express their concerns with the company’s new policy through proper channels, but their anxieties fell on deaf ears. The mods plan to strike indefinitely, and will cease all actions including closing posts, deleting posts, flagging answers, and other tasks that help with website upkeep until AI policy has been retracted.

AI has been transforming Stack Overflow recently, for better or for worse. [Stack Overflow confirmed the Gizmodo that traffic was dropping due to OpenAI’s ChatGPT](https://gizmodo.com/stack-overflow-traffic-drops-as-coders-opt-for-chatgpt-1850427794) as more and more programmers began turning to the chatbot to debug their code as opposed to waiting for a human reply on the forum. Web analytics firm SimilarWeb reported in April that Stack Overflow has seen a drop in traffic every month since the beginning of 2022, with the average drop being 6%. In March, Stack Overflow saw a 13.9% drop in traffic from February and in April, the website saw 17.7% drop in traffic from March.",Stack Overflow Moderators Stop Work in Protest of Lax AI-Generated Content Guidelines,https://gizmodo.com/ai-stack-overflow-content-moderation-chat-gpt-1850505609
"[""Mike Stobbe""]",2023-07-24,2023-07-24,2023-06-01,2023-07-24,https://dims.apnews.com/dims4/default/34826ac/2147483647/strip/true/crop/1559x877+0+82/resize/1440x810!/quality/90/?url=https%3A%2F%2Fstorage.googleapis.com%2Fafs-prod%2Fmedia%2F213565956ce34165b638c70429d06caf%2F1559.jpeg,,,en,,apnews.com,"[""Khoa Lam""]","NEW YORK (AP) — Racial bias built into a common medical test for lung function is likely leading to fewer Black patients getting care for breathing problems, a study published Thursday suggests.

As many as 40% more Black male patients in the study might have been diagnosed with breathing problems if current diagnosis-assisting computer software was changed, the study said.

Doctors have long discussed the potential problems caused by race-based assumptions that are built into diagnostic software. This study, published in JAMA Network Open, offers one of the first real-world examples of how the the issue may affect diagnosis and care for lung patients, said Dr. Darshali Vyas, a pulmonary care doctor at Massachusetts General Hospital.

The results are “exciting” to see published but it’s also “what we’d expect” from setting aside race-based calculations, said Vyas, who was an author of an influential 2020 New England Journal of Medicine article that catalogued examples of how race-based assumptions are used in making doctors’ decisions about patient care.

For centuries, [some doctors and others have held beliefs](https://apnews.com/article/medical-racism-history-bb1e7d18a811c6c3878fcdb7095f434b) that there are natural racial differences in health, including one that Black people’s lungs were innately worse than those of white people. That assumption ended up in modern guidelines and algorithms for assessing risk and deciding on further care. Test results were adjusted to account for — or “correct” for — a patient’s race or ethnicity.

One example beyond lung function is a heart failure risk-scoring system that categorizes Black patients as being at lower risk and less likely to need referral for special cardiac care. Another is an equation used in determining kidney function that creates estimates of higher kidney function in Black patients.

The new study focused on a test to determine how much and how quickly a person can inhale and exhale. It’s often done using a spirometer — a device with a mouthpiece connected to a small machine.

After the test, doctors get a report that has been run through computer software and scores the patient’s ability breathe. It helps indicate whether a patient has restrictions and needs further testing or care for things like asthma, chronic obstructive pulmonary disorder or lung scarring due to air pollutant exposure.

Algorithms that adjust for race raise the threshold for diagnosing a problem in Black patients and may make them less likely to get started on certain medications or to be referred for medical procedures or even lung transplants, Vyas said.

While physicians also look at symptoms, lab work, X-rays and family histories of breathing problems, the pulmonary function testing can be an important part of diagnoses, “especially when patients are borderline,” said Dr. Albert Rizzo, the chief medical officer at the American Lung Association.

The new study looked at more than 2,700 Black men and 5,700 white men tested by University of Pennsylvania Health System doctors between 2010 and 2020. The researchers looked at spirometry and lung volume measurements and assessed how many were deemed to have breathing impairments under the race-based algorithm as compared to under a new algorithm.

Researchers concluded there would be nearly 400 additional cases of lung obstruction or impairment in Black men with the new algorithm.

Earlier this year, the American Thoracic Society, which represents lung-care doctors, issued a statement recommending replacement of race-focused adjustments. But the organization also put a call out for more research, including into the best way to modify software and whether making a change might inadvertently lead to overdiagnosis of lung problems in some patients.

Vyas noted some other algorithms have already been changed to drop race-based assumptions, including one for pregnant women that predicts risks of vaginal delivery if the mom previously had a cesarean section.

Changing the lung-testing algorithm may take longer, Vyas said, especially if different hospitals use different versions of race-adjusting procedures and software.","Black men were likely underdiagnosed with lung problems because of bias in software, study suggests",https://apnews.com/article/black-racial-bias-lung-medical-diagnosis-e1f73be6d00f17091600b6f21f20264d
"[""Jonathan Vanian""]",2023-07-24,2023-07-24,2023-06-07,2023-07-24,https://image.cnbcfm.com/api/v1/image/106804443-1606846321310-gettyimages-1229495348-Valera_Golovniov_IGOR5253.jpeg?v=1686175028&w=1920&h=1080,,,en,,cnbc.com,"[""Khoa Lam""]","An Instagram logo is seen displayed on a smartphone.

Instagram's recommendation algorithms have been connecting and promoting accounts that facilitate and sell child sexual abuse content, according to an investigation published Wednesday.

[Meta's](https://www.cnbc.com/quotes/META/) photo-sharing service stands out from other social media platforms and ""appears to have a particularly severe problem"" with accounts showing self-generated child sexual abuse material, or SG-CSAM, Stanford University researchers wrote in an accompanying study. Such accounts purport to be operated by minors.

""Due to the widespread use of hashtags, relatively long life of seller accounts and, especially, the effective recommendation algorithm, Instagram serves as the key discovery mechanism for this specific community of buyers and sellers,"" according to the study, which was cited in the [investigation](https://www.wsj.com/articles/instagram-vast-pedophile-network-4ab7189) by The Wall Street Journal, Stanford University's Internet Observatory Cyber Policy Center and the University of Massachusetts Amherst.

While the accounts could be found by any user searching for explicit hashtags, the researchers discovered Instagram's recommendation algorithms also promoted them ""to users viewing an account in the network, allowing for account discovery without keyword searches.""

A Meta spokesperson said in a statement that the company has been taking several steps to fix the issues and that it ""set up an internal task force"" to investigate and address these claims.

""Child exploitation is a horrific crime,"" the spokesperson said. ""We work aggressively to fight it on and off our platforms, and to support law enforcement in its efforts to arrest and prosecute the criminals behind it.""

Alex Stamos, Facebook's former chief security officer and one of the paper's authors, said in a [tweet](https://twitter.com/alexstamos/status/1666490180910608384) Wednesday that the researchers focused on Instagram because its ""position as the most popular platform for teenagers globally makes it a critical part of this ecosystem."" However, he added ""Twitter continues to have serious issues with child exploitation.""  

Stamos, who is now director of the Stanford Internet Observatory, said the problem has persisted after [Elon Musk](https://www.cnbc.com/elon-musk/) acquired Twitter late last year.

""What we found is that Twitter's basic scanning for known CSAM broke after Mr. Musk's takeover and was not fixed until we notified them,"" Stamos wrote.

""They then cut off our API access,"" he added, referring to the software that lets researchers access Twitter data to conduct their studies.

Earlier this year, NBC News [reported](https://www.nbcnews.com/tech/tech-news/musk-twitter-elon-child-abuse-material-rcna63621) multiple Twitter accounts that offer or sell CSAM have remained available for months, even after Musk pledged to address problems with child exploitation on the social messaging service.

Twitter didn't provide a comment for this story.","Instagram's algorithms are promoting accounts that share child sex abuse content, researchers find",https://www.cnbc.com/2023/06/07/instagram-promotes-accounts-sharing-child-sex-abuse-content-research.html
"[""Andy Nghiem""]",2023-07-24,2023-07-24,2023-05-18,2023-07-24,https://jnswire.s3.amazonaws.com/jns-media/63/14/11416220/closeup-of-gavel-1600x900.jpg,,,en,,madisonrecord.com,"[""Khoa Lam""]","EDWARDSVILLE - A class action lawsuit alleges a facial recognition search engine that searches the internet for peoples' photos invades their privacy in violation of Illinois law.

Plaintiffs Amy Newton, Amanda Curry, Manuel Clayton, Misty McGraw, and Nicholas Clayton filed a class action lawsuit in the Madison County Circuit Court against Pimeyes Sp. Z O.O, Transaction Cloud, Inc., Face Recognition Solutions, LTD., Carribex LTD., EMEA Robotics, LTD., Public Mirror SP. Z O.O, Lukasz Kowalczyk, Denis Tatina, Giorgi Gobronidze, and Does 1-25, citing negligence and carelessness in violation of the Illinois Biometric Information Privacy Act (BIPA).

According to the lawsuit, defendants offer a service called Pimeyes, a facial search engine that can search the internet for photos that contain a given face. The company charges users $29.99 per month for this service. 

The lawsuit alleges that to build this powerful search engine, the defendants have collected images of millions of Americans from databases across the internet and scanned them using artificial intelligence into their own database without consent.

According to the lawsuit, many of these databases contain numerous images of the plaintiffs. This database is massive and continuously updated with new photos, the lawsuit states.

The suit adds that the company's service can be used by anyone to search for images of anyone else to download without that person's consent. 

The lawsuit states that BIPA laws require that prior to collecting biometric data, companies must inform visitors in writing that biometric data will be collected and stored. It also states that visitors must be informed in writing of the specific purpose of why the biometric data is being collected, how long it will be stored, and companies must receive a written release from visitors for the collection of biometric data.

The plaintiffs argue that the defendants invaded their privacy and the privacy of untold Americans by collecting and storing their biometric data without informed consent. They add that Pimeyes does not have written, publicly available policies identifying how long it will store biometric data or information for permanently destroying biometric data. 

The plaintiffs are demanding a jury trial to seek damages for themselves and everyone in the class action lawsuit, plus court costs, attorney fees and any other relief the court deems proper. They are also requesting the court to issue an order requiring the defendant to comply with BIPA and cease the collection of biometric data without informed written consent. They are represented in this case by the attorneys of Peiffer, Wolf, Carr, Kane, Conway & Wise, LLP in St. Louis. 

_Madison County Circuit Court case number 2023LA000629_",Illinois residents allege facial image search engine violates BIPA,https://madisonrecord.com/stories/642174142-illinois-residents-allege-facial-image-search-engine-violates-bipa
"[""Charles Rollet""]",2023-07-24,2023-07-24,2023-05-30,2023-07-24,https://s.ipvm.com/uploads/04ab/302c/4a2d18e7-bfae-4f31-9795-05ff53ea33ee.jpg,,,en,,ipvm.com,"[""Khoa Lam""]","**""Banner\_Alarm"" \*\* \*\* \*\*\*\*\*\*\*\***

\*\*\*\*\*\*\*\* \*\* \*\*\*\*, \*\*\*\*\*'\*[""\*\*\*\*"" \*\* \*\*\*\*\*\*\*\*](#restricted)(大华巨灵\*\*开放平台), \*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\* \*\* detecting \*\*\*\* \*\*\*\* \*\* \* \*\*\*\*\*\*\*\*\*\*\*\* site, \*\*\*\*\*\* \*\*\*\*\*\*\*, \*\*\*\*\*\* \*\*\*\* \*\*\*\*, etc.

[\*\*\* \*\*\*\*\*\*\*\*, \*\*\*\*\*\* ""\*\*\*\*\*\* \*\*\*\*\*\*"" (拉横幅)](#restricted), \*\* \*\*\*\*\*\* \*\*\*\*\* \*\*\* ""\*\*\*\*\*\* safety"" (社会治安) \*\*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\* at \*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\*:

> \*\* \*\*\* \*\*\*\*\*\*\*\*\*\* \*\*\*\*, \*\***a \*\*\*\*\*\* \*\*\*\*\*\*\* \* \*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*** and lasts for a certain period of time, **an \*\*\*\*\* \*\*\*\* \*\* \*\*\*\*\*\*\*\*\*** \[emphasis added\]
> 
> 指定区域内，检测到人举横幅且持续一定时间则产生报警

\*\*\*\*\* \*\*\* \* \*\*\*\* \*\* \*\*\* system \*\*\*\* \*\*\* \*\*\*\*\*\*\* \*\*\*\*\* ""\*\*\*\*\*\*\*\*\*\*\*\*"" on \*\*\* \*\*\* \*\*\*\*. \*\*\* \*\*\*\* also \*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\*\*\*\*' \*\*\*\*\* \*\*\* automatically \*\*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\* \*\* \*\*\*\* of \*\*\* \*\*\*\* \*\*\*\*\*\*\*\*:



\*\*\* \*\*\*\*\*\* \*\*\*\*\*: ""\*\*\*\*\* \*\*\* \*\* days, \*\*\*\*\* \*\* \*\*\* \*\*\*\* \*\*\*\*\*\*, Win"" (战\*\*天, 拼在寒冬, 赢下). \*\*\*\* \*\*\*\*\* not \*\*\*\* \*\*\* \*\*\*\*\*\*\* \*\* \*\*\*\* slogan \*\*\*\*\*\*\* \*\* \*\*\*\* \*\*\*\*\* \*\*\*\*. It \*\* \*\*\*\*\*\*\* \*\* \*\*\*\* \*\*\*\*\*\* to \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\* \*\* \*\*\*\* invented \*\*\* \*\*\*\* \*\*\*\*\*\*\*\*.

**No \*\*\* \*\*\*\*\*\*\*\***

\*\*\*\*\*'\* \*\*\*\*\*\* \*\*\*\* \*\*\* \*\*\*\*\*\*\* \*\*\*\*\*\*\* character \*\*\*\*\*\*\*\*\*\*\* (\*\*\*) \*.\*. \*\*\*\*\* \*\* no \*\*\*\*\*\*\*\* \*\*\*\* \*\*\* \*\*\*\*\*\* \*\*\* automatically \*\*\*\*\*\*\*\* \*\*\*\* \*\*\* \*\*\*\*\*\* \*\* the \*\*\*\*\*\* \*\* \*\* \*\* \*\*\* to \*\*\*\*\* \*\*\*\* \*\* \*\*\*\*\*\*\*\* \*\*\*\*\* or \*\*\*\*\*\*\*. \*\*\*\*\*\*\*\*\*\*\*\*, \*\*\*\*\*\*\*\*\*\*\*, \*\*\*\* \*\*\*\*\* not \*\* \*\*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*\* as \*\*\* \*\* \*\*\*\* \*\*\*\*\*\*\*\*\*\*\* \*\*\* banners \*\*\*\* \*\* \*\* \*\*\*\*\*, \*\*\*\*\*\* it \*\*\*\* \*\*\* \*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*.

**Intended \*\*\* \*\*\* \*\*\*\*\*\*\*\*\*\*\***

\*\*\*\*\*'\* \*\*\* \*\* \*\*\* \*\*\*\*\* ""\*\*\*\*\*\* safety"" (社会治安) \*\*\* ""\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*"" (社会治理) to \*\*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\* police \*\*\* \*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\*\*\* \*\*\* targeted \*\*\*\*\* \*\*\*\*\* \*\*\*\*\* \*\*\*\*\* \*\*\*[\*\*\*\*\*\*\*\*\*\*\*\* \*\*\*\* \*\* \*\*\*\*\*](#restricted)\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*\* \*\*\*\* enforcement[\*\*\*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*.](#restricted)

**Demo \*\*\*\* \*\*\*\*, \*\*\*\*\*\*\*\*\* \*\* \*\*\*\***

\*\*\*\* \*\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*[\*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\* \*\* \*\*\* \*\*\*\*](#restricted)\*\*\*\*\*\* \*\*\*\* \*\*\* \*\*\*\* \*\*\*\* \*\*\*\*\*\*\* for \*\*\*\*\* \*\* \*\*\* \*\*\*\*\*\* \*\*\*\*\* was \*\*\*\*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\* \*\*\*\*.

**Intended \*\*\* \*\*\*\* \*\*\*\*\*\*\*, \*\*\*\*\*, \*\*\*\*\***

[\*\*\*\*\*'\* \*\*\*\*\*\*\*\*](#restricted)\*\*\*\*\*\* \*\* \*\* \*\*\*\*\*\*\*\* \*\*\* ""\*\*\*\*\*\* halls \*\* \*\*\*\*\*\*\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\* \[\*\*\*\*\] squares, \*\*\*\*\*, \*\*\* \*\*\*\*\* \*\*\*\*\*\*"" (室内大厅或室外较空旷的广场，马路等场景).

**Targets \*\*\*\*\*\*\* \*\*\*-\*\*\* \*\*\*\*\*\***

[\*\*\*\*\*'\* \*\*\*\*\*\*\*\*](#restricted)\*\*\*\* \*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\* ""\*\*\*\*\*\*\* or \*\*\*\*\*\*\*\*"" \*\*\*\* \*\*\*\*\*\* \*\*\*-\*\*\* \*\*\*\*\*\* in \*\*\*\* \*\* \*\*\*\*\* \*\*\*\*\*\*\*\*\*\* (常见的多边形横幅或标语牌，目标大小在\*\*\*\*\*下\*\*\*~\*\*\*像素).

**Infinova, \*\*\*\*\*\* \*\*\*\* \*\*\*\* \*\*\*\*\*\*\***

\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\* ""\*\*\*\*\*\*\*\* banner \*\*\*\*\*\*\*\*\*"" (拉横幅检测) \*\*\*\*\*\*\*\*\*\*\*\*: \*\*\*[\*\*\*\*\*-\*\* \*\*\* \*\*\*\*\*\* \*\*\*\*\*\*](#restricted)\*\*\* \*\*\*[\*\*\*\*\*-\*\*\*\*-\*\*\*-\* \*\*\* \*\*\*\*\*\* \*\*\*\*\*\*:](#restricted)

> \*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*
> 
> \* \*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*: \*\*\*\*\*\*\*\* \*\*\*\*\*\*\* from \*\*\*\* \*\*\*\*\*\*\*\*\*,**unfurl \*\*\*\*\*\* \*\*\*\*\*\*\*\*\***
> 
> 支持丰富的智能分析功能
> 
> \*项行为分析检测：高空抛物、拉横幅检测 \[\*\*\*\*\*\*\*\* \*\*\*\*\*\]

\*\*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\* \*\*\*\* analytics \*\*\*, \*\*\*\*[\*\* \*\*\*\*\* \*\*\*\*](#restricted)\*\*\* \*\*\*[\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\*.](#restricted)

**Protesting \*\* \*\*\*\*\* \*\*\*\*\*\*\*\*\*\***

\*\*\*\*\* \*\*\*\*\*\*\* \*\* \*\* \*\*\*[\*\*\* \*\*\*\*\*\*\*\*\*\*\*\*](#restricted)\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*, \*\*\*\*\*\*\*\*\*\* \*\*\* those \*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* due \*\* \*\*\*\*\*\*\*\*\*[\*\*\* \*\*\*\*](#restricted)\*\*\*\*\*\*\* ""\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*\*"" \*\*\* ""\*\*\*\*\*\*\*\*\* social \*\*\*\*\*\*\*\*\*\*\*\*\*\*"",[\* \*\*\* \*\*\*\*\*\* \*\*\*\*\*\*\*\*.](#restricted)

\*\*\*\*\*, \*\*\*\*\*\*\*\*\* \* \*\*\*\*\*\* (""拉横幅"") \*\* frequently \*\*\*\* \*\* \*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\* \*.\*. this[\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\*\*\*\*](#restricted)\*\*\*\* \*\*\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*\* ""\*\*\*\*\*\* teachers' \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\* \*\*\* \*\*\*\*"" (依法保障教师待遇):



\*\*\*\* \*\*\*\*\*\*\*\*, \*\*\*\*\*'\* \*\*\*\*'\* \*\*\*\*-\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*[\*\*\*\* \*\*\*\*\*\*\*\*](#restricted)\*\* \* \*\*\* \*\*\*\*\*\*\*\*\* \* \*\*\*\*\*\* in \*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* ""\*\* \*\*\*'\* \*\*\*\* lockdowns, \*\* \*\*\*\* \*\*\*\*\*\*\*"" (不要封控要自由). \*\*\* protestor \*\*\* \*\*\*\*\*\*\*\* \*\* \*\*\* \*\*\*\* and \*\*\*[\*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*\*\*\*.](#restricted)



**PRC \*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\***

\*\*\*\*\*\* \*\* \*\*\* \*\* \*\*\* \*\*\*\*\*\*, where \*\*\*\*\*\*\* \*\*\*\*\* \*\*\*\* \*\* \*\*\* shapes \*\*\* \*\*\*\*\*, \*\* \*\*\* \*\*\* protest \*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*, making \*\* \*\*\*\*\*\* \*\* \*\*\*\*\* \*\*\*\*\* analytics \*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*. IPVM \*\*\*\*\*[\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*](#restricted)\*\*\*\* \*\*\* \*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\* explicitly \*\*\* \*\*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\*\*\*:



**Hikvision \*\*\*\*\*\*\*\*\*\***

\*\*\*\*\*\*\*\*\* \*\*\* \*\*\*\* \*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\* to \*\*\* \*\*\*\*\*\*:

*   \* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\* \*\* \*\*\*\*\* for ""\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*, \*\*\*\*\*\*\*\*\*\*\*, \*\*\*\*\*\*\*\*\*\*\*\*\*\*"" (非法集会、游行、示威) and \*\*\*\*\*\* \*\*\*\*\*\*\* \*\* ""\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*"" (聚众),[\*\*\*\*](#restricted)\*\*\*[\*\*\* \*\*\*\*\*\*\*\*](#restricted)\*\*\*\*\*\*\*\* \*\* \*\*\*\*.
*   \*\*\*\*\*\*\*\*\* \*\*\* \* \*\*\*\* \*\*\*\*\*\*\*\* \*\* build \* \*\*\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\* risk \*\* \*\*\*\*\*\*\*\*\*\*/""\*\*\*\*\*\*\*\*\*\*\*"" (信访人) \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*,[\*\*\*\*](#restricted)\*\*\*[\*\*\* \*\*\* \*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*\*.](#restricted)

\*\*\*\*\*\*\*, \*\*\*\*\*\*\*\*\*'\* \*\*\*\*\*\* \*\*\* \*\*\* \*\*\*\*\*\*\* by \*\*\*\*\* \*\*\*\*\*\*\*\*\*, \*\*\*\*\*\*\*, \*\*\*\* \*\*\*\*\*\*\*\*\* with \*\*\*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\* protesters \*\*\* \*\*\*\*\*\*\*\*\*\*\*\* ""\*\*\*\*\*\*\*\*\*\*\*"" (信访人), \* group \*\*\*\* \*\*[\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\* \*\* \*\*\* \*\*\*.](#restricted)

**Dahua \*\*\*\*\*\*\* \*\*\*\*\*\*\*\***

\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\* \*\*\*\*\*, the ""\*\*\*\*\*\* \*\*\*\*\*\*"" (社会治安) \*\*\*\*\*\*\*\* \*\*\*\*\*\* the \*\*\*\*\*\*\*\*\* \*\* ""\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*"" (拉横幅):



\*\*\*\*\*\*\*, \*\*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\* \*\*\* its \*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\* out \*\*\* \*\*\*\*\*\*\*[\*.\*., ""\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*"" (拉横幅) \*\* \*\*\*\*\*\* appears](#restricted). \*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\* \*\*\*[\*\*\*\*\*\*\*\* \*\*\* \*\*\*\*\* \*\*\*\*\*\*\*.](#restricted)

\*\*\*\* \*\* \*\*\*\*\*\*\* \*\*\* \*\*\* \*\*\*\*\* which \*\*\* \* \*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\* evidence \*\*\* \*\*\* \*\*\* \*\*\*\*\*\*\*, \*.\*.[\*\*\*\*\* \*\*\*\*\* \*\* \*\*\*\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*, Then \*\*\*\*\* (\*\*\*\*)](#restricted)\*\*\*[\*\*\*\*\*'\* \*\*\*\*\*\* \*\*\*\*\* \*\*\*\* \*\*\*, \*\*\*\*\*\*\* Evidence (\*\*\*\*).](#restricted)

**Dahua, \*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\***

\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\* \*\*\*\* article. \*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\* the \*\*\*\*\*\*\*\*\* \*\* \*\*\* \*\*\*\* (\*\*\*\*\*) and \*\*\* \*\*\*\* (\*\*\*\*\*\*\*\*), \* \*\* 5 \*\*\*\* \*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*\*.

\*\*\*\*\*\*: \*\*\*\*\*\*\*\*'\* \*\*\*\*\*\*\*\*\* \*\*\*\* \*\*\* \*\*\*\* she \*\* ""\*\*\* \*\*\*\*\*"" \*\* \*\*\*\*\*\*\* with \*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*\* \*\*\*\* \*\* police \*\*\* \*\*\*\*\*\*\*\*'\* \*\*\* \*\*\*\*\* \*\*\* only \*% \*\* \*\*\*\*\* \*\*\*\*\*\*\*. \*\*\*\*, Infinova \*\*\*\* \*\*\*\*\* $\*\*\*\* \*\*\* \*\* sales[\*\*\* \*\*\*\*](#restricted)\*\* \*\*\*\* \*\*\*\*\*\*\*\*\*\* \*\*\*\*\* $\*.\*\* \*\* sales.

\*\*\*\* \*\*\* \*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*:

> \* \*\* \*\*\* \*\*\*\*\* \*\* \*\*\*\*\* of \*\*\*\*\* \*\*\*\*\*\*\* \*\* \*\*\*\*\*\*. \*\* general, \*\*\*\*\*\*\*\*’\* \*\*\*\*\*\* \*\*\*\*\* \*\* \*\*\*\*\* are \*\*\*\* \*\*\* (\*\*\*\*\* \*% \*\* total \*\*\*\*\*\*\*).

\*\*\*\*\* \*\*\* \*\*\*\* \*\*\* \*\* ""\*\*\* aware"" \*\* \*\*\*\*\* \*\* \*\*\*\*\*\*, \*\*\*\*\*\* detection \*\*\* \*\* \*\*\* \*\*\*\*\* (\*\*\*\* IPVM \*\*\*\*\* \*\*\*\*) \*\*\*\*\*\*\* \*\* \*\*\* law \*\*\*\*\*\*\*\*\*\*\*. \*\*\* \*\*\*\*\* \*\*\*\*\*\*\*\*'\* \*\*\*\*\* sales \*\*\* \*\*\*, \*\*\*\* \*\*\*\* \*\*\* or \*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\* \*\*\*\* \*\*\*\*\*\*\*\*\*\* could \*\* \*\*\*\* \*\* \*\*\*\*\* \*\* protestors.","Dahua Selling Protestor / Banner Alarms, Deletes Evidence",https://ipvm.com/reports/dahua-protestor-alarms
"[""Smitha Milli"",""Micah Carroll"",""Sashrika Pandey"",""Yike Wang"",""Anca D. Dragan""]",2023-07-24,2023-07-24,2023-05-26,2023-07-24,https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,,en,,arxiv.org,"[""Khoa Lam""]","As social media continues to have a significant influence on public opinion, understanding the impact of the machine learning algorithms that filter and curate content is crucial. However, existing studies have yielded inconsistent results, potentially due to limitations such as reliance on observational methods, use of simulated rather than real users, restriction to specific types of content, or internal access requirements that may create conflicts of interest. To overcome these issues, we conducted a pre-registered controlled experiment on Twitter's algorithm without internal access. The key to our design was to, for a large group of active Twitter users, simultaneously collect (a) the tweets the personalized algorithm shows, and (b) the tweets the user would have seen if they were just shown the latest tweets from people they follow; we then surveyed users about both sets of tweets in a random order.  

Our results indicate that the algorithm amplifies emotional content, and especially those tweets that express anger and out-group animosity. Furthermore, political tweets from the algorithm lead readers to perceive their political in-group more positively and their political out-group more negatively. Interestingly, while readers generally say they prefer tweets curated by the algorithm, they are less likely to prefer algorithm-selected political tweets. Overall, our study provides important insights into the impact of social media ranking algorithms, with implications for shaping public discourse and democratic engagement.","Twitter's Algorithm: Amplifying Anger, Animosity, and Affective Polarization",https://arxiv.org/abs/2305.16941
"[""Gavin Abercrombie"",""Amanda Cercas Curry"",""Tanvi Dinkar"",""Zeerak Talat""]",2023-07-24,2023-07-24,2023-05-16,2023-07-24,https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,,en,,arxiv.org,"[""Khoa Lam""]","Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have begun to investigate factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be considered. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, arguing that it can reinforce stereotypes of gender roles and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.",Mirages: On Anthropomorphism in Dialogue Systems,https://arxiv.org/abs/2305.09800
"[""Avram Piltch""]",2023-07-24,2023-07-24,2023-06-11,2023-07-24,https://cdn.mos.cms.futurecdn.net/cE4DTLrJF5ueefP3LSooaY-1200-80.png,,,en,,tomshardware.com,"[""Khoa Lam""]","Search has always been the Internet’s most important utility. Before Google became dominant, there were many contenders for the search throne, from Altavista to Lycos, Excite, Zap, Yahoo (mainly as a directory) and even Ask Jeeves. The idea behind the World Wide Web is that there’s power in having a nearly infinite number of voices. But with millions of publications and billions of web pages, it would be impossible to find all the information you want without search. 

Google succeeded because it offered the best quality results, loaded quickly and had less cruft on the page than any of its competitors. Now, having taken over [91 percent of the search market](https://kinsta.com/search-engine-market-share/#:~:text=Google%20dominates%20the%20search%20engine,91.88%25%20as%20of%20June%202022.), the company is testing a major change to its interface that replaces the chorus of Internet voices with its own robotic lounge singer. Instead of highlighting links to content from expert humans, the “Search Generative Experience” (SGE) uses an AI plagiarism engine that grabs facts and snippets of text from a variety of sites, cobbles them together (often word-for-word) and passes off the work as its creation. If Google makes SGE the default mode for search, the company will seriously damage if not destroy the open web while providing a horrible user experience.

A couple of weeks ago, Google made SGE available to the public in a limited beta (you can [sign up here](https://www.tomshardware.com/how-to/access-google-ai-betas)). If you are in the beta program like I am, you will see what the company seems to have planned for the near future: a search results page where answers and advice from Google take up the entire first screen, and you have to scroll way below the fold to see the first organic search result.  

For example, when I searched “best bicycle,” Google’s SGE answer, combined with its shopping links and other cruft took up the first 1,360 vertical pixels of the display before I could see the first actual search result. 



(Image credit: Tom's Hardware)

For its part, Google says that it’s just “experimenting,” and may make some changes before rolling SGE out to everyone as a default experience. The company says that it wants to continue driving traffic offsite.

“We’re putting websites front and center in SGE, designing the experience to highlight and drive attention to content from across the web,” a Google spokesperson told me. “SGE is starting as an experiment in Search Labs, and getting feedback from people is helping us improve the experience and understand how generative AI can be helpful in information journeys. The experiences that ultimately come to Search will likely look different from the experiments you see in Search Labs. As we experiment with new LLM-powered capabilities in Search, we'll continue to prioritize approaches that will drive valuable traffic to a wide range of creators."" 

By “putting websites front-and-center,” Google is referring to the block of three related-link thumbnails that sometimes (but not always) appear to the right of its SGE answer. These are a fig leaf to publishers, but they’re not always the best resources (they don’t match the top organic results) and few people are going to click them, having gotten their “answer” in the SGE text.



(Image credit: Tom's Hardware)

For example, when I searched for “Best CPU,” the related links were from the sites [Maketecheasier.com](https://www.maketecheasier.com/buying-cpu-processor-guide/#:~:text=Clock%20speed%20explains%20how%20fast%20the%20individual,count%20and%20vice%20versa%20for%20productivity%20users.), [Nanoreview](https://nanoreview.net/en/cpu-compare/amd-ryzen-9-7900x3d-vs-amd-ryzen-7-7800x3d#:~:text=Pros:%20+%20Has%204%20more%20physical%20cores,v5%20test%20%2D%202192%20vs%201945%20points) and [MacPaw](https://macpaw.audw.net/c/221109/66209/1733?subId1=tomshardware-us-7035733169633876000&sharedId=tomshardware-us&u=https%3A%2F%2Fmacpaw.com%2Fhow-to%2Fchoose-best-processor-for-mac). None of these sites is even on the first page of organic results for “Best CPU” and for good reason. They aren’t leading authorities in the field and the linked articles don’t even provide lists of the best CPUs. The MacPaw article is about how to choose the best processor for your MacBook, a topic that does not match the intent of someone searching for “best CPU,” as those folks are almost certainly looking for a desktop PC processor.

A Plagiarism Stew
-----------------

Even worse, the answers in Google’s SGE boxes are frequently plagiarized, often word-for-word, from the related links. Depending on what you search for, you may find a paragraph taken from just one source or get a whole bunch of sentences and factoids from different articles mashed together into a plagiarism stew. 

When I searched “which is faster the Ryzen 7 7800X3D or the Core i9-13900K,” the Google SGE grabbed an exact phrase from our Tom’s Hardware [article comparing the two CPUs](https://www.tomshardware.com/news/amd-ryzen-7-7800x3d-vs-intel-core-i9-13900k-vs-intel-core-7-13700K#:~:text=In%20our%20test%20suite%2C%20the,and%209%25%20faster%20at%201440p.), writing “The Ryzen 7 7800X3D is 12% faster than the Core i9-13900K at 1080p gaming and 9% faster at 1440p.” It then rephrased two sentences from [this article on Hardware Times](https://www.hardwaretimes.com/amd-ryzen-7-7800x3d-vs-intel-core-i9-13900k-12-gaming-benchmarks-power-efficiency-temps/#:~:text=The%20Core%20i9%2D13900K%20snags,13900K%20in%20Ubisoft's%20latest%20title.). The original copy read as:

“The Core i9-13900K snags a win in “A Plague Tale” both with and without ray-tracing. It’s marginally faster than the Ryzen 7 7800X3D with similar lows. The tables get turned in Assassins’ Creed Valhalla as the 7800X3D edges past the 13900K in Ubisoft’s latest title.”

And Google’s AI wrote it as:

“The Core i9-13900K is marginally faster than the Ryzen 7 7800X3D in ‘A Plague Tale’. However, the Ryzen 7 7800X3D edges past the Core i9-13900K in Assassins' Creed Valhalla.”



(Image credit: Tom's Hardware)

You can even clearly see in our screenshot that our sentence is quoted word-for-word in Google’s “featured snippet” box but not in the SGE box (which will likely replace the featured snippets in the future since SGE does basically the same thing). Yes, both the Hardware Times article and the Tom’s Hardware article that Google’s bot copied data from are listed as related links on the right side of the box. 

When I asked Google about the fact that its SGE answers are frequently word-for-word copies drawn from the related links articles, the company said that it picks those links because they “corroborate” the responses.

“Generative responses are corroborated by sources from the web,” the spokesperson said. “And when a portion of a snapshot briefly includes content from a specific source, we will prominently highlight that source in the snapshot.”

It’s pretty easy to find sources that back up your claims when your claims are word-for-word copied from those sources. While the bot could do a better job of laundering its plagiarism, it’s inevitable that the response would come from some human’s work. No matter how advanced LLMs get, they will never be the primary source of facts or advice and can only repurpose what people have done. LLMs are relatively good at generating “creative” works that are designed to be a mashup of existing ideas (ex: “write me a haiku about farts”) but, until they are connected to robotic bodies that go out and gather information first-hand, they will never be a source of truth.

The company also said that “you can expand to see how the links apply to each part of the snapshot.” There’s an expand icon that sits inconspicuously in the upper right corner of the SGE box, above the third related link. And, if you decide to click it, you will see a clunky interface which puts the thumbnails for related links inline with the pilfered text.

Image 1 of 2



(Image credit: Tom's Hardware)



(Image credit: Tom's Hardware)

Whether you click the expand button or not, SGE’s related links are not presented as citations, but recommendations for further reading. If I start singing “Thriller” then tell you that it’s an original song I wrote, it doesn’t matter if I also say “you might want to listen to a guy named Micheal Jackson because he also makes some nice songs like this.” That’s still plagiarism and, even if it were not, we’d have a problem. 

Plagiarism is a moral and academic term, not a legal one, and simply giving credit is not a defense against copyright infringement. You can’t run a business selling pirated Blu-ray discs and then, when busted, say “it’s all good, because I listed George Lucas as the director of _Star Wars_ rather than substituting my own name in the credits.”

In answering my questions, Google’s spokesperson also compared the SGE box to [featured snippets](https://support.google.com/websearch/answer/9351707?hl=en), noting that publishers today usually want their articles to appear in featured snippets because those links drive traffic back. While both experiences use content directly from publishers, featured snippets are short quotes with direct attribution and a very prominent link directly to the source. They do not pretend to be generated by an all-knowing AI and they often give you just enough information to want to click-through for more.

From a reader’s perspective, we’re left without any authority to take responsibility for the claims in the bot’s answer. Who, exactly, says that the Ryzen 7 7800X3D is faster and on whose authority is it recommended? I know, from tracing back the text, that Tom’s Hardware and Hardware Times stand behind this information, but because there’s no citation, the reader has no way of knowing. Google is, in effect, saying that its bot is the authority you should believe. 

The fallacy underlying Google SGE is the false belief that a bot can have authority in the first place. Until the bot grows a pair of hands and opens its own lab space, it will never test CPUs. Until it opens a kitchen, it will never have its own family recipes. The only thing it can cook up is a plagiarism stew. 

Relying on an unsourced bot as the end-all, be-all authority stands in direct contradiction to Google’s stated [emphasis on E-E-A-T](https://developers.google.com/search/blog/2022/12/google-raters-guidelines-e-e-a-t) (Expertise, Experience, Authority and Trust), a standard it uses to decide which websites and authors should rank highly in organic search. 

It makes total sense that someone who has been reviewing CPUs for 15 years on a website that specializes in CPUs should have their AMD Ryzen review rank higher than someone with no authority on the topic. Unfortunately, when it comes to Google’s own AI author – a faceless entity that has no experience doing anything – the rules go out the window.

Mish-Mash Plagiarism Leads to Poor Answers
------------------------------------------

At least the result we got when asking which CPU was faster was an accurate one. However, by mashing up text from different sources and then not sharing what the source for each sentence or bullet point is, Google is offering incorrect information that often contradicts the source material it’s copied from, or contradicts itself.

For example, I searched for “ThinkPad X13 AMD Review,” because I was interested in seeing what reviewers thought of Lenovo’s ThinkPad X13 laptop with AMD processor inside. The Google bot wrote its own mini-review, complete with bulleted pros and cons for the ThinkPad X13, while grabbing sentences and bullet points from at least four different articles, including [a review](https://www.laptopmag.com/reviews/lenovo-thinkpad-x13-amd) from Laptop Mag, [a review](https://www.tomshardware.com/reviews/lenovo-thinkpad-X13-gen1-amd) from Tom’s Hardware, [another review](https://www.notebookcheck.net/Lenovo-ThinkPad-X13-Gen-2-review-AMD-Ryzen-Pro-makes-the-compact-business-laptop-fast.580644.0.html#:~:text=Pros:%20+%20Very%20good%20system%20performance%20Cons:,slow%20for%20competitive%20gamers%20%2D%20Not%20great) from Notebook Check and a [blog post](https://go.redirectingat.com/?id=92X1584492&xcust=tomshardware_us_8146957167845176000&xs=1&url=https%3A%2F%2Fwww.laptopoutlet.co.uk%2Fblog%2Fbest-laptops-for-civil-and-structural-engineers.html%23%3A~%3Atext%3DPacked%2520with%2520high%252Dperforming%2520AMD%2520Ryzen%25207%2520PRO%2Cthe%2520best%2520and%2520cheapest%2520laptops%2520out%2520there.&sref=https%3A%2F%2Fwww.tomshardware.com%2Fnews%2Fgoogle-sge-break-internet) from LaptopOutlet – which is a store that had about 100 words on the product. 

The image below shows the result, along with pointers to where the SGE took its content from.



(Image credit: Tom's Hardware)

Aside from it being plagiarism and a slap in the face to the writers who did the actual work of testing and using this laptop, Google’s answer has a lot of issues. First of all, the answer refers to the ThinkPad X13 Gen 3 (the latest version with AMD CPU) but the reviews it draws from are from the Gen 1 and Gen 2 versions of the product, which are not the same.

While Laptop Mag and Tom’s Hardware both praised the laptop’s keyboard and durable design, both sites described the battery life as “lackluster” or “subpar,” while Google lists “Long battery life” as a pro. The bot clearly got the battery life pro from another site, but by mixing advice from different sources, Google is presenting readers with a very inaccurate picture. 

Also, since the bot doesn’t cite sources, the reader has no way to know who thought it had long battery life, whether that came from a reputable outlet and how they tested. One of the sources, LaptopOutlet, is a store that sells laptops and doesn’t do any benchmark testing. Should its claims be given equal weight to those journalists who actually do test and aren’t actively trying to sell the product? Like most LLMs, Google’s SGE bot doesn’t seem to care whether it’s giving you the truth or just mashing sentences together in a way that seems convincing.

Giving Faulty Medical Advice
----------------------------

The Google SGE bot is so careless in its plagiarism mashups that it also gives incorrect medical advice that has been drawn from a variety of sources. For example, I asked: “do I need a colonoscopy?” and it gave me the following answer:



(Image credit: Tom's Hardware)

I highlighted the text in blue because it is dangerously wrong. Google’s bot says that “the American Cancer Society recommends that men and women should be screened for colorectal cancer starting at age 50.” However, the American Cancer Society’s own website says that [screenings should start at age 45](https://www.cancer.org/cancer/types/colon-rectal-cancer/detection-diagnosis-staging/acs-recommendations.html), so this misleading “fact” probably came from elsewhere.

There’s also a bulleted list of “reasons to have a colonoscopy” that don’t include “routine screening,” hence it’s implying that you should only get the procedure if you have symptoms. The bulleted list is copied word-for-word from [an article](https://www.betterhealth.vic.gov.au/health/conditionsandtreatments/colonoscopy) on an Australian Government health site called BetterHealth. The article actually lists “screening and surveillance for colorectal cancer” as a reason, but Google’s bot decided not to copy that fact. 

Even if all the facts in the colonoscopy answer were clear and correct, they are not attributed to anyone. So why on earth should you trust them and whom do you blame when you follow this advice – for example, delaying your screening to age 50 – and something bad happens? By claiming content as its own, Google is acting as a publisher, which likely opens it up to lawsuits.

Though Google is telling the public that it wants to drive traffic to publishers, the SGE experience looks purpose-built to keep readers from leaving and going off to external sites, unless those external sites are ecomm vendors or advertisers. In some queries – “screenshot in windows” for example – there is a detailed answer but no related links at all. Nevermind that there are tons of articles that give you a lot more detail about how to take a screen shot.



(Image credit: Tom's Hardware)

If Google were to roll its SGE experience out of beta and make it the default, it would be detonating a 50-megaton bomb on the free and open web. Many publishers, who rely on Google referrals for the majority of their visits, would fold within a few months. Others would cut resources and retreat behind paywalls. Small businesses that rely on organic search placement to sell their products and services would have to either pay for advertising or, if they cannot afford it, close up shop. 

Eventually, even hobbyists who either run not-for-profit websites or post advice on forums would likely stop doing it. Who wants to write, even for fun, if your words are going to be stolen and no one is going to read your copy? Would you answer someone’s programming question on Stack Overflow if your contribution would just be reworded and spat out by Google, without ever mentioning your name or the post itself?

Not an AI Issue: An Anti-Competitive Issue
------------------------------------------

This isn’t a case of artificial intelligence outsmarting human writers or providing a better experience. In fact, the method of publishing is incidental to the problem. If it rolls the current SGE experience out, Google would be leveraging its monopoly position to push its own content over and above everyone else’s. The company could hire an army of unskilled writers to copy and paste content from third-party websites, sometimes rewording it, instead of using an AI. The outcome would be the same.

There’s no doubt that Google’s AI will get better, but get better at what exactly? It will likely do a better job of rephrasing content so that it’s harder to find the original source it copied from. It will do a better job of offering information that’s up-to-date and logically consistent with itself. However, by just grabbing other peoples’ ideas and not citing the source, there’s no authority behind anything it says.

The end result of Google SGE going live as the default search experience would be a weaker, more siloed Internet, but likely a wealthier Google. The company would increase its time-on-site, ad revenue and ecommerce referrals. It would also please investors, who want to see it compete with OpenAI and Bing. Some readers may grouse about the quality of the information, which can be outdated, false or word-for-word plagiarized, but taking up the entire first screen of results will be enough for Google to grab a huge percentage – if not the majority – of its current outbound clicks.

Many people I have talked to about and shown Google SGE can’t believe that the company would roll such a dangerous, poor-quality and web-breaking experience out to everyone. We can hope that the final product won’t take up as much screen real estate as what we’re seeing today. But Google is already making this the daily search experience for anyone who, like me, signs up for the beta. And it has every economic incentive to make this the new default experience for 91 percent of the web’s searches.

What Publishers Can Do, What Users Can Do
-----------------------------------------

Anyone who publishes on the web and needs people to actually read their work is in a precarious position, because of Google’s SGE. Almost every publication desperately needs to keep getting referrals from Google, so they can’t opt out of being indexed and having their data scraped. But if Google makes SGE the default search experience, the amount of Google referrals may fall so sharply that they can’t keep the lights on. 

Bing took only a few months to go from having its AI Chat in a limited beta to it being available to everyone. If Google follows a similar timeline, it could go from being a search engine to a zero-click, plagiarism engine by this fall.

Publishers and publishing associations are still grappling with what AI plagiarism could do to their businesses. The News / Media Alliance, an industry group that represents magazines and newspapers, published a [set of AI principles](https://www.newsmediaalliance.org/ai-principles/) that states “The unlicensed use of content created by our companies and journalists by GAI systems is an intellectual property infringement: GAI systems are using proprietary content without permission.”

Getty Images is [suing Stability AI](https://www.reuters.com/legal/getty-images-lawsuit-says-stability-ai-misused-photos-train-ai-2023-02-06/) to prevent the company from using its copyrighted images in training data. The image library has even asked a UK court to [block sales of the AI system](http://reuters.com/technology/getty-asks-london-court-stop-uk-sales-stability-ai-system-2023-06-01/) in that country. IAC Media Chairman Barry Diller has advocated for media companies to [sue AI vendors](https://fortune.com/2023/04/12/barry-diller-media-news-companies-a-i-sue-litigation/) over the unauthorized use of training data.

Will publishers sue Google over what it’s doing with SGE? There’s an argument that the word-for-word copying of information from websites without permission is a form of copyright infringement, even if the source was cited. However, we haven’t seen this litigated in court yet. And many companies, needing whatever traffic they will still get from Google, would want to avoid getting on the company’s bad side.

Companies could band together, through trade associations, to demand that Google respect intellectual property and not take actions that would destroy the open web as we know it. Readers can help by either scrolling past the company’s SGE to click on organic results or switching to a different search engine. Bing has shown a better way to incorporate AI, making its chatbot the non-default option and citing every piece of information it uses with a specific link back (the links aren’t very prominent, however). 

In the end, if Google follows through with its current iteration of SGE, it will damage the quality of its own service. The content that the bot trains on would get worse and worse as more quality publishers left the open web. Eventually, users would start looking for a service that provides better answers. But by that time, the damage done to the entire web information ecosystem could be irreparable.

_Note: As with all of our op-eds, the opinions expressed here belong to the writer alone and not Tom's Hardware as a team. _",Plagiarism Engine: Google’s Content-Swiping AI Could Break the Internet,https://www.tomshardware.com/news/google-sge-break-internet
"[""Federal Trade Commission""]",2023-07-24,2023-07-24,2023-05-22,2023-07-24,https://www.ftc.gov/themes/custom/ftc_uswds/img/ftc_social_share_default_en.jpg,,,en,,ftc.gov,"[""Khoa Lam""]","The Federal Trade Commission has obtained an order against education technology provider Edmodo for collecting personal data from children without obtaining their parent’s consent and using that data for advertising, in violation of the Children’s Online Privacy Protection Act Rule (COPPA Rule), and for unlawfully outsourcing its COPPA compliance responsibilities to schools. 

Under the [proposed order](/system/files/ftc_gov/pdf/2023129edmodojointmotionorder.pdf), filed by the Department of Justice on behalf of the FTC, Edmodo, Inc. will be prohibited from requiring students to hand over more personal data than is necessary in order to participate in an online educational activity. This is a first for an FTC order and is in line with a [policy statement the FTC issued in May 2022](/news-events/news/press-releases/2022/05/ftc-crack-down-companies-illegally-surveil-children-learning-online) that warned education technology companies about forcing parents and schools to provide personal data about children in order to participate in online education. During the course of the FTC’s investigation, Edmodo suspended operations in the United States. The order, if approved by the court, will bind the company, including if it resumes U.S. operations.

“This order makes clear that ed tech providers cannot outsource compliance responsibilities to schools, or force students to choose between their privacy and education,” said Samuel Levine, Director of the FTC’s Bureau of Consumer Protection. “Other ed tech providers should carefully examine their practices to ensure they’re not compromising students’ privacy.”

In a [complaint](/system/files/ftc_gov/pdf/edmodocomplaintfiled.pdf), also filed by DOJ, the FTC says Edmodo violated the COPPA Rule by failing to provide information about the company’s data collection practices to schools and teachers, and failing to obtain verifiable parental consent. The COPPA Rule requires online services and websites directed to children under 13 to notify parents about the personal information they collect and obtain verifiable parental consent for the collection and use of that information.

Until approximately September 2022, California-based Edmodo offered an online platform and mobile app with virtual class spaces to host discussions, share materials and other online resources for teachers and schools in the United States via a free and subscription-based service. The company collected personal information about students including their name, email address, date of birth and phone number as well as persistent identifiers, which it used to provide ads.

Under the COPPA Rule, schools can authorize collection of children’s personal information on behalf of parents. But a website operator must provide notice to the school of the operator’s collection, use and disclosure practices, and the school can only authorize collection and use of personal information for an educational purpose.

Edmodo required schools and teachers to authorize data collection on behalf of parents or to notify parents about Edmodo’s data collection practices and obtain their consent to that collection. Edmodo, however, failed to provide schools and teachers with the information they would need to comply in either scenario as required by the COPPA Rule, according to the complaint. For example, during the signup process for Edmodo’s free service, Edmodo provided minimal information about the COPPA Rule to teachers—providing only a link to the company’s terms of service and privacy policy, which teachers were not required to review before signing up for the company’s service.

Those teachers and schools that did read Edmodo’s terms of service were falsely told that they were “solely” responsible for complying with the COPPA Rule. The terms of service also failed to adequately disclose what personal information the company actually collects or indicate how schools or teachers should go about obtaining parental consent. These failures led to the illegal collection of personal information from children, according to the complaint.

In addition, Edmodo could not rely on schools to authorize collection on behalf of parents because the company used the personal information it collected from children for a non-educational purpose—to serve advertising. For such commercial uses, the COPPA Rule required Edmodo to obtain consent directly from parents. 

Edmodo also violated the COPPA Rule by retaining personal information indefinitely until at least 2020 when it put in place a policy to delete the data after two years, according to the complaint. COPPA prohibits retaining personal information about children for longer than is reasonably necessary to fulfill the purpose for which it was collected.

In addition to violating the COPPA Rule, the FTC says Edmodo violated the FTC Act’s prohibition on unfair practices by relying on schools to obtain verifiable parental consent. Specifically, the FTC says that Edmodo outsourced its COPPA compliance responsibilities to schools and teachers while providing confusing and inaccurate information about obtaining consent. This is the first time the FTC has alleged an unfair trade practice in the context of an operator’s interaction with schools.

Proposed Order
--------------

The proposed order with Edmodo includes a $6 million monetary penalty, which will be suspended due to the company’s inability to pay. Other order provisions, which will provide protections for children’s data should Edmodo resume operations in the United States, include:

*   prohibiting Edmodo from conditioning a child’s participation in an activity on the child disclosing more information than is reasonably necessary to participate in such activity;
*   requiring the company to complete several requirements before obtaining school authorization to collect information from a child;
*   prohibiting the company from using children’s information for non-educational purposes such as advertising or building user profiles;
*   banning the company from using schools as intermediaries in the parental consent process;
*   requiring the company to implement and adhere to a retention schedule that details what information it collects, what the data is used for and a time frame for deleting it; and
*   requiring Edmodo to delete models or algorithms developed using personal information collected from children without verifiable parental consent or school authorization.

The Commission voted 3-0 to refer the civil penalty complaint and proposed federal order to the Department of Justice. The DOJ filed the complaint and stipulated order in the U.S. District Court for the Northern District of California.

**NOTE:** The Commission authorizes the filing of a complaint when it has “reason to believe” that the named defendant is violating or is about to violate the law and it appears to the Commission that a proceeding is in the public interest. Stipulated orders have the force of law when approved and signed by the District Court judge.

The lead FTC attorneys on this matter are Gorana Neskovic and Peder Magee from the FTC’s Bureau of Consumer protection.",FTC Says Ed Tech Provider Edmodo Unlawfully Used Children’s Personal Information for Advertising and Outsourced Compliance to School Districts,https://www.ftc.gov/news-events/news/press-releases/2023/05/ftc-says-ed-tech-provider-edmodo-unlawfully-used-childrens-personal-information-advertising
"[""Nico Grant"",""Kashmir Hill""]",2023-07-24,2023-07-24,2023-05-22,2023-07-24,https://static01.nyt.com/images/2023/05/16/multimedia/g-0-promo/g-0-promo-facebookJumbo.jpg,,,en,,nytimes.com,"[""Khoa Lam""]","Eight years after a controversy over Black people being mislabeled as gorillas by image analysis software — and despite big advances in computer vision — tech giants still fear repeating the mistake.

When Google released its stand-alone Photos app in May 2015, people were wowed by what it could do: analyze images to label the people, places and things in them, an astounding consumer offering at the time. But a couple of months after the release, a software developer, Jacky Alciné, discovered that Google had labeled photos of him and a friend, who are both Black, as “gorillas,” a term that is particularly offensive because it echoes centuries of racist tropes.

In the ensuing controversy, Google prevented its software from categorizing anything in Photos as gorillas, and it vowed to fix the problem. Eight years later, with significant advances in artificial intelligence, we tested whether Google had resolved the issue, and we looked at comparable tools from its competitors: Apple, Amazon and Microsoft.  
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

Photo apps made by Apple, Google, Amazon and Microsoft rely on artificial intelligence to allow us to search for particular items, and pinpoint specific memories, in our increasingly large photo collections. Want to find your day at the zoo out of 8,000 images? Ask the app. So to test the search function, we curated **44 images** featuring people, animals and everyday objects.

We started with Google Photos. When we searched our collection for **cats** and **kangaroos,** we got images that matched our queries. The app performed well in recognizing most other animals.

But when we looked for **gorillas,** Google Photos failed to find any images. We widened our search to **baboons, chimpanzees, orangutans** and **monkeys,** and it still failed even though there were images of all of these primates in our collection.

We then looked at Google’s competitors. We discovered Apple Photos had the same issue: It could accurately find photos of particular animals, except for most primates. We did get results for **gorilla,** but only when the text appeared in a photo, such as an image of Gorilla Tape.

The photo search in Microsoft OneDrive drew a blank for every animal we tried. Amazon Photos showed results for all searches, but it was over-inclusive. When we searched for **gorillas,** the app showed a menagerie of primates, and repeated that pattern for other animals.

undefined

There was one member of the primate family that Google and Apple were able to recognize — lemurs, the permanently startled-looking, long-tailed animals that share opposable thumbs with humans, but are more distantly related than are apes.

Google’s and Apple’s tools were clearly the most sophisticated when it came to image analysis.

Yet Google, whose Android software underpins most of the world’s smartphones, has made the decision to turn off the ability to visually search for primates for fear of making an offensive mistake and labeling a person as an animal. And Apple, with technology that performed similarly to Google’s in our test, appeared to disable the ability to look for monkeys and apes as well.

Consumers may not need to frequently perform such a search — though in 2019, an iPhone user complained on Apple’s customer support forum that the software “[can’t find monkeys in photos on my device](https://discussions.apple.com/thread/250713142).” But the issue raises larger questions about other unfixed, or unfixable, flaws lurking in services that rely on computer vision — a technology that interprets visual images — as well as other products powered by A.I.

Mr. Alciné was dismayed to learn that Google has still not fully solved the problem and said society puts too much trust in technology.

“I’m going to forever have no faith in this A.I.,” he said.

Computer vision products are now used for tasks as mundane as sending an alert when there is a package on the doorstep, and as weighty as navigating cars and finding perpetrators in law enforcement investigations.

Errors can reflect racist attitudes among those encoding the data. In the gorilla incident, two former Google employees who worked on this technology said the problem was that the company had not put enough photos of Black people in the image collection that it used to train its A.I. system. As a result, the technology was not familiar enough with darker-skinned people and confused them for gorillas.

As artificial intelligence becomes more embedded in our lives, it is eliciting fears of unintended consequences. Although computer vision products and A.I. chatbots like ChatGPT are different, both depend on underlying reams of data that train the software, and both can misfire because of flaws in the data or biases incorporated into their code.

Microsoft recently [limited users’ ability](https://www.nytimes.com/2023/02/16/technology/microsoft-bing-chatbot-limits.html) to interact with a chatbot built into its search engine, Bing, after it instigated [inappropriate conversations](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html).

Microsoft’s decision, like Google’s choice to prevent its algorithm from identifying gorillas altogether, illustrates a common industry approach — to wall off technology features that malfunction rather than fixing them.

“Solving these issues is important,” said Vicente Ordóñez, a professor at Rice University who studies computer vision. “How can we trust this software for other scenarios?”

Michael Marconi, a Google spokesman, said Google had prevented its photo app from labeling anything as a monkey or ape because it decided the benefit “does not outweigh the risk of harm.”

Apple declined to comment on users’ inability to search for most primates on its app.

Representatives from Amazon and Microsoft said the companies were always seeking to improve their products.

When Google was developing its photo app, which was released eight years ago, it collected a large amount of images to train the A.I. system to identify people, animals and objects.

Its significant oversight — that there were not enough photos of Black people in its training data — caused the app to later malfunction, two former Google employees said. The company failed to uncover the “gorilla” problem back then because it had not asked enough employees to test the feature before its public debut, the former employees said.

Google profusely apologized for the gorillas incident, but it was one of a number of episodes in the wider tech industry that have led to accusations of bias.

Other products that have been criticized include [HP’s facial-tracking webcams](https://www.reuters.com/article/urnidgns852573c40069388000257693007c9b22/hp-our-webcams-arent-racist-idUS97608933020091222), which could not detect some people with dark skin, and the [Apple Watch,](https://splinternews.com/will-the-apple-watchs-coolest-feature-work-for-people-o-1793846147) which, according [to a lawsuit](https://www.usatoday.com/story/tech/2022/12/27/apple-watch-blood-oxygen-oximeter-dark-skin-lawsuit/10955942002/), failed to accurately read blood oxygen levels across skin colors. The lapses suggested that tech products were not being designed for people with darker skin. (Apple pointed [to a paper](https://www.apple.com/healthcare/docs/site/Blood_Oxygen_app_on_Apple_Watch_October_2022.pdf) from 2022 that detailed its efforts to test its blood oxygen app on a “wide range of skin types and tones.”)

Years after the Google Photos error, the company encountered a similar problem with its Nest home-security camera during internal testing, according to a person familiar with the incident who worked at Google at the time. The Nest camera, which used A.I. to determine whether someone on a property was familiar or unfamiliar, mistook some Black people for animals. Google rushed to fix the problem before users had access to the product, the person said.

However, Nest customers continue to complain on the company’s forums about other flaws. In 2021, a customer received alerts that his mother was ringing the doorbell but found his mother-in-law instead on the other side of the door. When users complained that the system was mixing up faces they had marked as “familiar,” a customer support representative in the forum advised them to delete all of their labels and start over.

Mr. Marconi, the Google spokesman, said that “our goal is to prevent these types of mistakes from ever happening.” He added that the company had improved its technology “by partnering with experts and diversifying our image datasets.”

In 2019, Google tried to improve a facial-recognition feature for Android smartphones by increasing the number of people with dark skin in its data set. But the contractors whom Google had hired to collect facial scans [reportedly](https://www.nytimes.com/2019/10/04/technology/google-facial-recognition-atlanta-homeless.html) resorted to a troubling tactic to compensate for that dearth of diverse data: They targeted homeless people and students. Google executives called the incident “very disturbing” at the time.

While Google worked behind the scenes to improve the technology, it never allowed users to judge those efforts.

Margaret Mitchell, a researcher and co-founder of Google’s Ethical AI group, joined the company after the gorilla incident and collaborated with the Photos team. She said in a recent interview that she was a proponent of Google’s decision to remove “the gorillas label, at least for a while.”

“You have to think about how often someone needs to label a gorilla versus perpetuating harmful stereotypes,” Dr. Mitchell said. “The benefits don’t outweigh the potential harms of doing it wrong.”

Dr. Ordóñez, the professor, speculated that Google and Apple could now be capable of distinguishing primates from humans, but that they didn’t want to enable the feature given the possible reputational risk if it misfired again.

Google has since released a more powerful image analysis product, Google Lens, a tool to search the web with photos rather than text. [Wired](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/) discovered in 2018 that the tool was also unable to identify a gorilla.



When we showed Lens a photo of a dog, it was able to suggest its likely breed.

But when we showed it a gorilla, a chimpanzee, a baboon, and an orangutan, Lens seemed to be stumped, refusing to label what was in the image and surfacing only “visual matches” — photos it deemed similar to the original picture.

For gorillas, it showed photos of other gorillas, suggesting that the technology recognizes the animal but that the

company is afraid of labeling it.



When we showed Lens a photo of a dog, it was able to suggest its likely breed.

But when we showed it a gorilla, a chimpanzee, a baboon, and an orangutan, Lens seemed to be stumped, refusing to label what was in the image and surfacing only “visual matches” — photos it deemed similar to the original picture.

For gorillas, it showed photos of other gorillas, suggesting that the technology recognizes the animal but that the company is afraid of labeling it.

undefined

These systems are never foolproof, said Dr. Mitchell, who is no longer working at Google. Because billions of people use Google’s services, even rare glitches that happen to only one person out of a billion users will surface.

“It only takes one mistake to have massive social ramifications,” she said, referring to it as “the poisoned needle in a haystack.”",Google’s Photo App Still Can’t Find Gorillas. And Neither Can Apple’s,https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html
"[""Nino Bucci"",""Christopher Knaus""]",2023-07-24,2023-07-24,2023-05-12,2023-07-24,https://i.guim.co.uk/img/media/9b98427a63c5a7e43cd10797b05627bd25756889/0_89_3898_2340/master/3898.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=316b7d26ef5052da117e2ee21e6b39e8,,,en,,theguardian.com,"[""Khoa Lam""]","A tool designed to predict future crime in terrorist offenders considered them at greater risk of offending if they were autistic despite having no empirical basis to do so, an independent report has found.

The report into the Vera-2R tool, which was released to Guardian Australia and others under freedom of information laws, found a lack of evidence underpinning the instrument had “potentially serious implications for \[its\] validity and reliability” and found it was “extremely poor” at predicting risk.

It also found that autism spectrum disorders, as well as non-compliance with conditions or supervision, were included as risk factors in the tool, despite a lack of empirical evidence.

The most comprehensive systematic review that has been conducted on the drivers of radicalisation and terrorist behaviour and violence found not a single piece of empirical evidence that supported the inclusion of those two factors, the report found.

The release of the critical report came after a series of Guardian Australia stories which exposed the federal government’s continued use of the tool despite criticisms. The federal government did not disclose the report [to the lawyers of convicted offenders](https://www.theguardian.com/australia-news/2023/apr/28/lawyers-outraged-by-government-failure-to-disclose-terrorism-prediction-tools-serious-problems), its own experts, or the NSW government, which uses the tool to justify harsh post-sentence orders for offenders, including [ongoing detention](https://www.theguardian.com/law/2023/apr/04/law-council-joins-calls-to-abolish-australias-powers-to-detain-terrorist-offenders-to-prevent-future-crimes).

The report, titled “Testing the Reliability, Validity and Equity of Terrorism Risk Assessment Instruments”, was completed by Australian National University academics Dr Emily Corner and Dr Helen Taylor for the Department of Home Affairs and runs to more than 270 pages.

The government received the report, known as the Corner report, in May 2020.

It was released under FoI with minimal redactions, despite the federal government previously claiming it could not be released because of national security.

“The lack of evidence underpinning both instruments has potentially serious implications for their validity and reliability,” the report found.

“Without a strong theoretical and empirical basis for factor inclusion, it is not reasonable to anticipate that the instruments are able to predict their specified risk with anything other than chance.

“If an instrument with a weak evidence base is employed as a predictive instrument by practitioners, it is not possible to determine if individuals who pass through assessment processes would ever be suitable for the management plan as determined by the risk decision outcome made on the instrument.”

Guardian Australia has previously reported the tool was [used 14 times by the federal government](https://www.theguardian.com/australia-news/2023/apr/08/coalition-warned-of-problems-with-tool-to-predict-future-terrorist-but-continued-to-use-it-on-offenders) after they received the report, and the NSW government also continued to use it to assess offenders, including some without convictions for terrorism.

Corner and Taylor also assessed the Radar tool, a classified instrument which is used less frequently to justify post-sentence orders than Vera-2R, and said their report was the first piece of research to be performed on these instruments.

The report made four recommendations, including that another evaluation is conducted, that the authors of both instruments “enact far more thorough evaluations of the wider theoretical and empirical literature to help develop risk factors that accurately reflect behavioural trajectories towards radicalisation and terrorist violence”, and that the risk specification of each instrument should be refined.

[skip past newsletter promotion](https://www.theguardian.com/australia-news/2023/may/12/australian-terrorism-prediction-tool-considered-autism-a-sign-of-criminality-despite-lack-of-evidence#EmailSignup-skip-link-16)

Sign up to Guardian Australia's Afternoon Update

Our Australian afternoon update email breaks down the key national and international stories of the day and why they matter

**Privacy Notice:** Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our [Privacy Policy](https://www.theguardian.com/help/privacy-policy). We use Google reCaptcha to protect our website and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.

after newsletter promotion

It found that the correct instrument, implemented correctly, could allow risk assessment that ensured those at highest risk of offending received the most intensive interventions, while those presenting as lower-risk were protected from too much intervention and therefore avoided “the disruption of circumstances that were making them low risk in the first place”.

But it found there was little basis for the claims made by the developers of the Vera-2R of its strong reliability and validity.

It said it was “concerning” that almost 60% of the cited evidence base for factor development of the tool was not empirical, and less than half of the works cited in the Vera-2R accurately reflected what was in the recorded texts.

The test can only be used by people who complete training accredited to the author, with the report noting that “there is no information within any documentation as to the time or costs of this training program, but it is believed to be a multiple day program with substantial costs attached”.

The federal and NSW governments both say they are considering the Corner report findings, but this week’s federal budget shows the Labor government is committed to continuing to fund its regime of post-sentence orders.

The budget gave $130.1m over two years to the high-risk terrorist offenders scheme.

A NSW communities and justice department spokesperson said the Vera-2R tool was just one of a number of tools used to assess risk.

“The Department of Communities and Justice is currently considering the Corner report in collaboration with other stakeholders,” the spokesperson said.",Australian terrorism prediction tool considered autism a sign of criminality despite lack of evidence,https://www.theguardian.com/australia-news/2023/may/12/australian-terrorism-prediction-tool-considered-autism-a-sign-of-criminality-despite-lack-of-evidence
"[""Jess Weatherbed""]",2023-07-24,2023-07-24,2023-05-15,2023-07-24,https://cdn.vox-cdn.com/thumbor/8KYMU6i45uSgH2BOZjCAUQ6U-VE=/0x0:1920x1200/1200x628/filters:focal(953x629:954x630)/cdn.vox-cdn.com/uploads/chorus_asset/file/24658583/House_of_Earth_and_Blood_Adobe_Stock.jpg,,,en,,theverge.com,"[""Khoa Lam""]","A prominent fantasy novel features a cover apparently generated with artificial intelligence, [sparking complaints](https://www.reddit.com/r/SarahJMaas/comments/138w0i1/crescent_city_ai_generated_covers/) from artists and book enthusiasts. Earlier this month, readers noted that the back of the UK edition of Sarah J. Maas’ _House of Earth and Blood_ [credits Adobe Stock](https://blackwells.co.uk/bookshop/product/9781526663559) for the illustration of a wolf on its cover. The illustration matches an image created by user [Aperture Vintage](https://go.redirectingat.com/?xs=1&id=1025X1701640&url=https%3A%2F%2Fstock.adobe.com%2Fuk%2Fcontributor%2F203047531%2Faperture-vintage%3Fload_type%3Dauthor%26prev_url%3Ddetail%26asset_id%3D550401570) and marked as AI-generated on Adobe’s site. The move has led to criticism of both Maas and Bloomsbury Publishing, one of the world’s leading independent publishing houses.

While stock image services like Getty Images have prohibited AI-generated illustrations to avoid [copyright disputes](https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion), Adobe has [notably welcomed](https://helpx.adobe.com/uk/stock/contributor/help/generative-ai-content.html) AI onto its Adobe Stock platform under specific criteria. Such content must be clearly labeled as AI-generated, and contributors must review the terms of any generative AI tools they use to create the images to ensure they have “all the necessary rights” to license them for commercial use. The rules are meant to avoid legal headaches. But the field of AI copyright is muddy, untested territory, and Adobe’s rules don’t clearly address all the issues it raises.

Bloomsbury rose to fame in the UK after it began publishing J.K. Rowling’s _Harry Potter_ franchise in 1997. Maas is currently one of Bloomsbury’s top authors, best known for her young adult fantasy series like _Throne of Glass_ (2012), _A Court of Thorns and Roses_ (2015), and _Crescent City_ (2020) — the franchise that _House of Earth and Blood_ is part of. She has sold over 12 million copies of her books, many of which have made it to the [_New York Times_ bestsellers list](https://www.nytimes.com/books/best-sellers/2021/03/07/), and a televised adaptation of _A Court of Thorns and Roses_ is currently [in development at Hulu](https://deadline.com/2021/03/a-court-of-thorns-and-roses-series-fantasy-books-hulu-ron-moore-sarah-j-maas-1234722918/). The publisher, Maas, Adobe, and Aperture Vintage did not immediately respond to requests for comment from _The Verge_.

Bloomsbury’s cover has exacerbated human artists’ concerns that publishers could replace them with text-to-image generators like Midjourney and Stable Diffusion. “Bloomsbury is one of the major publishing houses,” said [freelance artist Kala Elizabeth](https://twitter.com/kalaelizabeth/status/1657065874711543808) on Twitter. “They CAN afford to hire real illustrators instead of purchasing Adobe stock, which is where this AI content is from.” Other publishers have attracted similar scrutiny for using AI-generated artwork on covers. [Tor Books issued an apology](https://gizmodo.com/tor-book-ai-art-cover-christopher-paolini-fractalverse-1849904058) in December last year, claiming it was unaware that the cover image selected for Christopher Paolini’s _Fractal Noise_ novel was created by AI. While Maas hasn’t acknowledged the image’s provenance, and it’s not clear what — if any — involvement she had in the process, she’s praised the design of the cover on her [Instagram page](https://www.instagram.com/p/Co7pZ39LHGF/?hl=en).

Adobe launched its own [Firefly AI image generator](https://www.theverge.com/2023/3/21/23648315/adobe-firefly-ai-image-generator-announced) that it says is trained only on content that’s licensed or out of copyright. But those assurances don’t apply to images found in Adobe Stock, raising questions about whether copyrighted work was used to train the image generator that produced it. Many AI systems are built around datasets containing artwork scraped from the internet without the consent of the original artist, sometimes even [displaying signatures](https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit) from the original artwork within a generated final image. This has led [some artists](https://www.theverge.com/2022/12/23/23523864/artstation-removing-anti-ai-protest-artwork-censorship) to believe that [all AI-generated art is unethical](https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart) as it can profit from the work of human artists. It isn’t clear whether Adobe is assessing images to ensure they comply with its rules about copyright or whether it’s placing the legal responsibilities on creators.",A bestselling fantasy novel is using AI-generated cover art,https://www.theverge.com/2023/5/15/23724102/sarah-j-maas-ai-generated-book-cover-bloomsbury-house-of-earth-and-blood
"[""James Vincent""]",2023-07-24,2023-07-24,2023-05-02,2023-07-24,https://cdn.vox-cdn.com/thumbor/WgCVKvKeU4UBnz8GJYvIkkYyYyc=/0x0:5000x3120/1200x628/filters:focal(2500x1560:2501x1561)/cdn.vox-cdn.com/uploads/chorus_asset/file/24626885/537799082.jpg,,,en,,theverge.com,"[""Khoa Lam""]","AI chatbots are being used to generate news stories and blog posts for online content farms in the hopes of attracting a trickle of ad revenue from the stray clicks of web users.

Experts have been warning for years that such AI-generated content farms will soon become commonplace, but the wider availability of tools like OpenAI’s ChatGPT has now made these warnings a reality. NewsGuard, a for-profit organization that rates the trustworthiness of news sites, highlighted the problem in a [recent report](https://www.newsguardtech.com/special-reports/newsbots-ai-generated-news-websites-proliferating/) identifying 49 sites “that appear to be almost entirely written by artificial intelligence software.”

> The websites, which often fail to disclose ownership or control, produce a high volume of content related to a variety of topics, including politics, health, entertainment, finance, and technology. Some publish hundreds of articles a day. Some of the content advances false narratives. Nearly all of the content features bland language and repetitive phrases, hallmarks of artificial intelligence.

The sites identified by the organization often have generic names (like _Biz Breaking News_ and _Market News Reports_) and are stuffed with programmatic advertising that’s bought and sold automatically. They attribute news stories to generic or fake authors, and much of the content appears to be summaries or re-writes of stories from established sites like _CNN_.

Most of the sites are not spreading misinformation, said NewsGuard, but some publish blatant falsehoods. For example, in early April, a content farm named CelebritiesDeaths.com posted a story [claiming that Joe Biden had died](https://web.archive.org/web/20230409093456/https://celebritiesdeaths.com/biden-dead-harris-acting-president-address-9am-et/).

This Biden story might briefly fool a reader, though is soon revealed to be a fake. The second paragraph contains an error message from the chatbot that was asked to create the text and was evidently copy and pasted into the website without any oversight. “I’m sorry, I cannot complete this prompt as it goes against OpenAI’s use case policy on generating misleading content,” says the story. “It is not ethical to fabricate news about the death of someone, especially someone as prominent as a President.”

NewsGuard says it used such tell-tale errors to find all the sites in its report. As _The Verge_ has previously reported, [searching for phrases like “As an AI language model”](https://www.theverge.com/2023/4/25/23697218/ai-generated-spam-fake-user-reviews-as-an-ai-language-model) often reveals where chatbots are being used to generate fake reviews and other cheap text content. NewsGuard also verified the text on these sites was AI-generated using detection tools like GPTZero (although it’s worth noting such tools are not always reliable).

Noah Giansiracusa, an associate professor of data science who’s written about fake news, [told _Bloomberg_](https://www.bloomberg.com/news/articles/2023-05-01/ai-chatbots-have-been-used-to-create-dozens-of-news-content-farms?sref=ExbtjcSG) that the creators of such sites were experimenting “to find what’s effective” and would continue to spin up content farms given the cheap costs of production. “Before, it was a low-paid scheme. But at least it wasn’t free,” Giansiracusa told the outlet.

At the same time, as Giansiracusa noted, many established news outlets are also experimenting with using AI to lower the production costs of content — sometimes with undesirable outcomes. When _CNET_ started using AI to help write posts, a review of the system’s output found [errors in more than half the published stories](https://www.theverge.com/2023/1/25/23571082/cnet-ai-written-stories-errors-corrections-red-ventures). The pressure to use AI is increasing at a time when online news is facing a wave of layoffs and shut-downs.

You can read the full report from NewsGuard [here](https://www.newsguardtech.com/special-reports/newsbots-ai-generated-news-websites-proliferating/).",AI is being used to generate whole spam sites,https://www.theverge.com/2023/5/2/23707788/ai-spam-content-farm-misinformation-reports-newsguard
"[""Will Oremus""]",2023-07-24,2023-07-24,2023-05-05,2023-07-24,,,,en,,washingtonpost.com,"[""Khoa Lam""]","Chris Cowell, a Portland, Ore.-based software developer, spent more than a year writing a technical how-to book. Three weeks before it was released, another book on the same topic, with the same title, appeared on Amazon.",He wrote a book on a rare subject. Then a ChatGPT replica appeared on Amazon.,https://www.washingtonpost.com/technology/2023/05/05/ai-spam-websites-books-chatgpt
"[""Daniel Johnson""]",2023-07-24,2023-07-24,2023-07-20,2023-07-24,https://a9p9n2x2.stackpathcdn.com/wp-content/blogs.dir/1/files/2023/07/GettyImages-1299513271-e1689787410868.jpg,,,en,,blackenterprise.com,"[""Khoa Lam""]","[Daniel Johnson](https://www.blackenterprise.com/author/danieljohnson/)July 20, 2023July 20, 2023611

[0](#)[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.blackenterprise.com%2Fkhan-academys-harriet-tubman-ai%2F)[](https://twitter.com/intent/tweet?text=Khan%20Academy%20Is%20On%20The%20Wrong%20Side%20Of%20History%20And%20Intelligence%20With%20Its%20Harriet%20Tubman%20AI%20Avatar%20-%20https://www.blackenterprise.com/khan-academys-harriet-tubman-ai/)[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.blackenterprise.com%2Fkhan-academys-harriet-tubman-ai%2F&title=Khan%20Academy%20Is%20On%20The%20Wrong%20Side%20Of%20History%20And%20Intelligence%20With%20Its%20Harriet%20Tubman%20AI%20Avatar)[](mailto:?subject=Khan%20Academy%20Is%20On%20The%20Wrong%20Side%20Of%20History%20And%20Intelligence%20With%20Its%20Harriet%20Tubman%20AI%20Avatar&BODY=https://www.blackenterprise.com/khan-academys-harriet-tubman-ai/)



Activist and abolitionist Harriet Tubman, in 1895. (Smith Collection/Gado/Getty Images)

[Khan Academy](https://www.khanacademy.org/khan-labs#whoIsKhanAcademy) bills its artificial intelligence tutors as “the future of learning” on its website, but the truth is a little more complicated. What the site does not state upfront is that its service allows learners to select different historical figures such as Genghis Khan, Montezuma, Abigail Adams, and Harriet Tubman. The service is currently not available to all; it is restricted to a few school districts as well as volunteer testers of the product.

Similar to ChatGPT, avatars pull from data available on the internet to create a repository of words in the “vocabulary” of the bot that a user is talking to.

_The_ _Washington Post_[tested the limits](https://www.washingtonpost.com/history/interactive/2023/harriet-tubman-articial-intelligence-khan-academy/) of this technology, specifically the avatar of Harriet Tubman, to see if the AI would mimic Tubman’s speech pattern and spirit or if came off as an offensive impression or a regurgitation of Wikipedia information. 

According to the article, the tool is designed to assist educators in fostering students’ curiosity of historical figures, but there are limits in how the bot is programmed, resulting in avatars that do not accurately portray the figures they are supposed to represent. 

These AI interviews immediately raised questions, not just of the ethics in the nascent field of artificial intelligence, but of the ethics in even conducting such an “interview” in the interest of journalism. Many Black users on Twitter were horrified at the thought of digitally exhuming a venerated icon and ancestor in Harriet Tubman. These concerns seem to be located in the working knowledge that the creators of these apps and bots are not interested in fidelity to the spirits of the dead, because they don’t seem to care much about the living Black people they continually fail to do right by.

Even _The_ _Washington Post_ acknowledges that the bot fails its basic fact-checks, and Khan Academy stresses that the bot is not intended to function as a historical record of events. Why introduce such a technology if it cannot be trusted to even impersonate an up-to-date “version” of historical figures?

> What is wrong with y’all? [pic.twitter.com/0RXNDKeVf0](https://t.co/0RXNDKeVf0)
> 
> — CiCi Adams (@CiCiAdams\_) [July 18, 2023](https://twitter.com/CiCiAdams_/status/1681310375319584771?ref_src=twsrc%5Etfw)

[UNESCO](https://www.unesco.org/en/artificial-intelligence/recommendation-ethics) sets out some basic tenets and recommendations for ethics in the field of artificial intelligence on its website. The organization created the first global standard for ethics in artificial intelligence, which was accepted by 193 countries around the world in 2021.

Their four pillars are Human Rights and Human Dignity, Living in Peace With an Emphasis on Creating a Just Society, Ensuring Diversity and Inclusion, and Environmentalism. Even a cursory glance at these pillars would find Khan Academy’s bot impersonating historical figures who can’t consent to have their likenesses and names used is in flagrant violation of ethics and, some would argue, moral guidelines.

If the dead have dignity, digging them up for what amounts to thought exercises represents a complete disregard for their wishes and a lack of thought about these tenets of ethics. In its discussion of fairness and nondiscrimination, UNESCO writes: “AI actors should promote social justice, fairness, and non-discrimination while taking an inclusive approach to ensure AI’s benefits are accessible to all.”

It sounds like Khan Academy needs to take these words to heart, because at present, it does not exactly seem like social justice, fairness, and accessibility are at the heart of this project. The reactions to this experiment on social media tell that story to the world.

**RELATED CONTENT**: [Redman Wants No Parts Of Artificial Intelligence Says, ‘Don’t Let Technology Ruin Hip-Hop’](https://www.blackenterprise.com/celebrity-news-redman-artificial-intelligence/)",Khan Academy Is On The Wrong Side Of History And Intelligence With Its Harriet Tubman AI Avatar,https://www.blackenterprise.com/khan-academys-harriet-tubman-ai/
"[""Shira Ovide""]",2023-07-24,2023-07-24,2023-07-11,2023-07-24,,,,en,,washingtonpost.com,"[""Khoa Lam""]","On his way to catch a flight, Sen. Jeff Merkley (D-Ore.) was asked to have his photo taken by a facial recognition machine at airport security.",You can say no to a TSA face scan. But even a senator had trouble.,https://www.washingtonpost.com/technology/2023/07/11/tsa-airport-security-facial-recognition/
